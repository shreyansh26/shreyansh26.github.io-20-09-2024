<!DOCTYPE html>
<html lang="en-us">

<head>

  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="generator" content="Source Themes Academic 4.6.2">

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Shreyansh Singh">

  
  
  
    
  
  <meta name="description" content="Technical Paper: Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Blog: Our next-generation model: Gemini 1.5
These are just short notes / excerpts from the technical paper for quick lookup. Actual implementational details are anyways missing in the technical paper.
 Gemini 1.5 Pro is a sparse MoE Transformer-based model (seems like a trend these days, after GPT-4&rsquo;s rumors and Mixtral).
It supports upto 10M context multimodal context length which is about a day of 22 hours of audio recordings, more than ten times the entirety of the 1440 page book &ldquo;War and Peace&rdquo;, the entire Flax codebase (41,070 lines of code), or three hours of video at 1 frame-per-second.">

  
  <link rel="alternate" hreflang="en-us" href="https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/">

  


  
  
  
  <meta name="theme-color" content="#3f51b5">
  

  
  
  
  
    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css" integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.min.css" integrity="sha256-+N4/V/SbAFiW1MPBCXnfnP9QSN3+Keu+NlB+0ev/YKQ=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css" integrity="sha256-Vzbj7sDDS/woiFS3uNKo8eIuni59rjyNGtXfstRzStA=" crossorigin="anonymous">

    
    
    
      
    
    
      
      
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-light">
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/styles/github.min.css" crossorigin="anonymous" title="hl-dark" disabled>
      
    

    
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.css" integrity="sha256-SHMGCYmST46SoyGgo4YR/9AlK1vf3ff84Aq9yK4hdqM=" crossorigin="anonymous">
    

    

  

  
  
  
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=B612+Mono:400,700%7COrbitron:400,700%7CSpace+Mono:400,700%7CLato%7CMontserrat%7CInconsolata%7CAnonymous+Pro&display=swap">
  

  
  
  
  
  <link rel="stylesheet" href="/css/academic.css">

  





<script async src="https://www.googletagmanager.com/gtag/js?id=G-ZEZ7673Y7G"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
      dataLayer.push(arguments);
  }

  function trackOutboundLink(url) {
    gtag('event', 'click', {
         'event_category': 'outbound',
         'event_label': url,
         'transport_type': 'beacon',
         'event_callback': function () {
           document.location = url;
         }
    });
    console.debug("Outbound link clicked: " + url);
  }

  function onClickCallback(event) {
    if ((event.target.tagName !== 'A') || (event.target.host === window.location.host)) {
      return;
    }
    trackOutboundLink(event.target);  
  }

  gtag('js', new Date());
  gtag('config', 'G-ZEZ7673Y7G', {});

  
  document.addEventListener('click', onClickCallback, false);
</script>


  


  

  <link rel="manifest" href="/index.webmanifest">
  <link rel="icon" type="image/png" href="/img/shreyansh-icon-32.png">
  <link rel="apple-touch-icon" type="image/png" href="/img/shreyansh-icon-192.png">

  <link rel="canonical" href="https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/">

  
  
  
  
    
  
  
  <meta property="twitter:card" content="summary_large_image">
  <meta property="twitter:image" content="https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/featured.png">
  
  <meta property="twitter:site" content="@shreyansh_26">
  <meta property="twitter:creator" content="@shreyansh_26">
  
  <meta property="og:site_name" content="Shreyansh Singh">
  <meta property="og:url" content="https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/">
  <meta property="og:title" content="Paper Summary #10 - Gemini 1.5 Pro | Shreyansh Singh">
  <meta property="og:description" content="Technical Paper: Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context
Blog: Our next-generation model: Gemini 1.5
These are just short notes / excerpts from the technical paper for quick lookup. Actual implementational details are anyways missing in the technical paper.
 Gemini 1.5 Pro is a sparse MoE Transformer-based model (seems like a trend these days, after GPT-4&rsquo;s rumors and Mixtral).
It supports upto 10M context multimodal context length which is about a day of 22 hours of audio recordings, more than ten times the entirety of the 1440 page book &ldquo;War and Peace&rdquo;, the entire Flax codebase (41,070 lines of code), or three hours of video at 1 frame-per-second."><meta property="og:image" content="https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/featured.png">
  <meta property="twitter:image" content="https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/featured.png"><meta property="og:locale" content="en-us">
  
    
      <meta property="article:published_time" content="2024-02-18T19:01:35&#43;05:30">
    
    <meta property="article:modified_time" content="2024-02-18T19:01:35&#43;05:30">
  

  


    






  






<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/"
  },
  "headline": "Paper Summary #10 - Gemini 1.5 Pro",
  
  "image": [
    "https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/featured.png"
  ],
  
  "datePublished": "2024-02-18T19:01:35+05:30",
  "dateModified": "2024-02-18T19:01:35+05:30",
  
  "author": {
    "@type": "Person",
    "name": "Shreyansh Singh"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Shreyansh Singh",
    "logo": {
      "@type": "ImageObject",
      "url": "https://shreyansh26.github.io/img/shreyansh-icon-512.png"
    }
  },
  "description": "Technical Paper: Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context\nBlog: Our next-generation model: Gemini 1.5\nThese are just short notes / excerpts from the technical paper for quick lookup. Actual implementational details are anyways missing in the technical paper.\n Gemini 1.5 Pro is a sparse MoE Transformer-based model (seems like a trend these days, after GPT-4\u0026rsquo;s rumors and Mixtral).\nIt supports upto 10M context multimodal context length which is about a day of 22 hours of audio recordings, more than ten times the entirety of the 1440 page book \u0026ldquo;War and Peace\u0026rdquo;, the entire Flax codebase (41,070 lines of code), or three hours of video at 1 frame-per-second."
}
</script>

  

  


  


  

<style>

    @font-face {
        font-family: monaco123;
        font-weight: 100%;
        font-style: normal;
        src: url("/fonts/MONACO.TTF");
    } 

    code {
        font-family: monaco123;
        font-size: 100%;
    }

</style>

<script data-ad-client="ca-pub-8708413956078710" async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>




  <title>Paper Summary #10 - Gemini 1.5 Pro | Shreyansh Singh</title>

</head>

<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" >

  <aside class="search-results" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search">
        
      </div>

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>


  
<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container">

    
    
    
      <a class="navbar-brand" href="/">Shreyansh Singh</a>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link  active" href="/post/"><span>Posts</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#projects"><span>Projects</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        
          
          
          
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#contact"><span>Contact</span></a>
        </li>

        
        

        

        
        
        
          
        

        
        
        
        
        
        

        <li class="nav-item">
          <a class="nav-link " href="/files/Shreyansh_Resume.pdf"><span>CV</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      <li class="nav-item">
        <a class="nav-link js-dark-toggle" href="#"><i class="fas fa-moon" aria-hidden="true"></i></a>
      </li>
      

      

    </ul>

  </div>
</nav>


  <article class="article">

  




















  
  
    
  


<div class="article-container pt-3">
  <h1>Paper Summary #10 - Gemini 1.5 Pro</h1>

  
  <p class="page-subtitle">Google DeepMind announced a multimodal LLM with support of up to 10M context length.</p>
  

  


<div class="article-metadata">

  
  
  
  
  <div>
    



  
  <span><a href="/authors/shreyansh-singh/">Shreyansh Singh</a></span>

  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    Feb 18, 2024
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    5 min read
  </span>
  

  
  
  
  <span class="middot-divider"></span>
  <a href="/post/2024-02-18_gemini_pro_google/#disqus_thread"></a>
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/categories/machine-learning/">Machine Learning</a></span>
  

</div>

  














</div>


<div class="article-header article-container featured-image-wrapper mt-4 mb-4" style="max-width: 720px; max-height: 300px;">
  <div style="position: relative">
    <img src="/post/2024-02-18_gemini_pro_google/featured_hu3c67cabad3b0edc58173e7db51926dac_256112_720x0_resize_lanczos_2.png" alt="" class="featured-image">
    
  </div>
</div>



  <div class="article-container">

    <div class="article-style">
      <p><strong>Technical Paper</strong>: <a href="https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf">Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context</a><br>
<strong>Blog</strong>: <a href="https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024">Our next-generation model: Gemini 1.5</a></p>
<p>These are just short notes / excerpts from the technical paper for quick lookup. Actual implementational details are anyways missing in the technical paper.</p>
<hr>
<p>Gemini 1.5 Pro is a sparse MoE Transformer-based model (seems like a trend these days, after GPT-4&rsquo;s rumors and Mixtral).</p>
<p>It supports upto 10M context multimodal context length which is about a day of 22 hours of audio recordings, more than ten times the entirety of the 1440 page book &ldquo;War and Peace&rdquo;, the entire Flax codebase (41,070 lines of code), or three hours of video at 1 frame-per-second.</p>
<p>Near-perfect retrieval across complete context length. In all modalities, i.e., text, video and audio, it achieves 100% accurate needle-in-a-haystack recall for 530k tokens, 99.7% accurate recall for 1M tokens and 99.2% for 10M contexts.</p>













<figure>


  <a data-fancybox="" href="/post/2024-02-18_gemini_pro_google/images/niah1.png" >
<img src="/post/2024-02-18_gemini_pro_google/images/niah1.png" alt="" ></a>



</figure>














<figure>


  <a data-fancybox="" href="/post/2024-02-18_gemini_pro_google/images/text_niah.png" data-caption="Text Haystack. This figure compares Gemini 1.5 Pro with GPT-4 Turbo for the text needle-in-a-haystack task. Green cells indicate the model successfully retrieved the secret number, gray cells indicate API errors, and red cells indicate that the model response did not contain the secret number. The top row shows results for Gemini 1.5 Pro, from 1k to 1M tokens (top left), and from 1M to 10M tokens (top right). The bottom row shows results on GPT-4 Turbo up to the maximum supported context length of 128k tokens. The results are color-coded to indicate: green for successful retrievals and red for unsuccessful ones.">
<img src="/post/2024-02-18_gemini_pro_google/images/text_niah.png" alt="" ></a>


  
  
  <figcaption>
    Text Haystack. This figure compares Gemini 1.5 Pro with GPT-4 Turbo for the text needle-in-a-haystack task. Green cells indicate the model successfully retrieved the secret number, gray cells indicate API errors, and red cells indicate that the model response did not contain the secret number. The top row shows results for Gemini 1.5 Pro, from 1k to 1M tokens (top left), and from 1M to 10M tokens (top right). The bottom row shows results on GPT-4 Turbo up to the maximum supported context length of 128k tokens. The results are color-coded to indicate: green for successful retrievals and red for unsuccessful ones.
  </figcaption>


</figure>














<figure>


  <a data-fancybox="" href="/post/2024-02-18_gemini_pro_google/images/video_niah.png" data-caption="Video Haystack. This figure compares Gemini 1.5 Pro with GPT-4V for the video needle-in-a-haystack task, where the models are given video clips of different lengths up to three hours of video and are asked to retrieve a secret word embedded as text at different points within the clip. All video clips are sampled at one frame-per-second (1 fps). The first pair of 10 × 50 haystack plots on the left compare Gemini 1.5 Pro with GPT-4V on the first hour of the documentary. The x-axis represents the video duration which ranges from 1.2 minutes to 1 hour, and the y-axis represents the depth, namely the relative offset of the needle (e.g., the top left cell represents providing the model with the first 1.2 minutes and randomly sampling a frame in the first ten percent of that trimmed video where the needle is inserted). A green cell indicates that the model successfully retrieved the needle, whereas a gray cell indicates an API error. Whereas the GPT-4V API supports video lengths only up to around the first 3 minutes of the full hour, Gemini 1.5 Pro successfully retrieves the secret word inserted at all depth percentages up to an hour of video, as shown by the all-green plot. Finally, the 10 × 10 grid on the right shows Gemini 1.5 Pro’s perfect retrieval capabilities across three full hours of video, constructed by concatenating two copies of the documentary back-to-back.">
<img src="/post/2024-02-18_gemini_pro_google/images/video_niah.png" alt="" ></a>


  
  
  <figcaption>
    Video Haystack. This figure compares Gemini 1.5 Pro with GPT-4V for the video needle-in-a-haystack task, where the models are given video clips of different lengths up to three hours of video and are asked to retrieve a secret word embedded as text at different points within the clip. All video clips are sampled at one frame-per-second (1 fps). The first pair of 10 × 50 haystack plots on the left compare Gemini 1.5 Pro with GPT-4V on the first hour of the documentary. The x-axis represents the video duration which ranges from 1.2 minutes to 1 hour, and the y-axis represents the depth, namely the relative offset of the needle (e.g., the top left cell represents providing the model with the first 1.2 minutes and randomly sampling a frame in the first ten percent of that trimmed video where the needle is inserted). A green cell indicates that the model successfully retrieved the needle, whereas a gray cell indicates an API error. Whereas the GPT-4V API supports video lengths only up to around the first 3 minutes of the full hour, Gemini 1.5 Pro successfully retrieves the secret word inserted at all depth percentages up to an hour of video, as shown by the all-green plot. Finally, the 10 × 10 grid on the right shows Gemini 1.5 Pro’s perfect retrieval capabilities across three full hours of video, constructed by concatenating two copies of the documentary back-to-back.
  </figcaption>


</figure>














<figure>


  <a data-fancybox="" href="/post/2024-02-18_gemini_pro_google/images/audio_niah.png" data-caption="Audio Haystack. This figure presents the audio version of the needle-in-a-haystack experiment comparing Gemini 1.5 Pro and a combination of Whisper and GPT-4 Turbo. In this setting, the needle is a short segment of audio that is inserted within a very large audio segment (of up to 22 hours) containing concatenated audio clips. The task is to retrieve the &lsquo;secret keyword&rsquo; which is revealed in the needle. Red indicates that the model did not identify the keyword, whereas green indicates that the model identified the keyword correctly">
<img src="/post/2024-02-18_gemini_pro_google/images/audio_niah.png" alt="" ></a>


  
  
  <figcaption>
    Audio Haystack. This figure presents the audio version of the needle-in-a-haystack experiment comparing Gemini 1.5 Pro and a combination of Whisper and GPT-4 Turbo. In this setting, the needle is a short segment of audio that is inserted within a very large audio segment (of up to 22 hours) containing concatenated audio clips. The task is to retrieve the &lsquo;secret keyword&rsquo; which is revealed in the needle. Red indicates that the model did not identify the keyword, whereas green indicates that the model identified the keyword correctly
  </figcaption>


</figure>

<p>However, with about 100 needles (with the model requiring to retrieve them all), the performance drops to 70% recall up to 128K tokens, and &gt;60% recall up to 1M tokens. GPT-4 is 50% recall at 128k tokens (its maximum context).</p>













<figure>


  <a data-fancybox="" href="/post/2024-02-18_gemini_pro_google/images/multiple_niah.png" >
<img src="/post/2024-02-18_gemini_pro_google/images/multiple_niah.png" alt="" ></a>



</figure>

<p>Gemini 1.5 Pro beats RAG-based systems for all modalities. Although this may not be cost efficient right now unless there are multiple queries and the long context can be prefix-cached.</p>
<p>Excellent use of context for learning new skills. It was able to learn to translate from/to English to/from Kalamang, an extremely low-resource language (spoken by less than 200 people in the world), with just a single set of linguistic documentation, a dictionary, and ≈ 400 parallel sentences. That&rsquo;s it!</p>













<figure>


  <a data-fancybox="" href="/post/2024-02-18_gemini_pro_google/images/kalamang.png" data-caption="Given a reference grammar book and a bilingual wordlist (dictionary), Gemini 1.5 Pro is able to translate from English to Kalamang with similar quality to a human who learned from the same materials.">
<img src="/post/2024-02-18_gemini_pro_google/images/kalamang.png" alt="" ></a>


  
  
  <figcaption>
    Given a reference grammar book and a bilingual wordlist (dictionary), Gemini 1.5 Pro is able to translate from English to Kalamang with similar quality to a human who learned from the same materials.
  </figcaption>


</figure>














<figure>


  <a data-fancybox="" href="/post/2024-02-18_gemini_pro_google/images/kalamang_results.png" >
<img src="/post/2024-02-18_gemini_pro_google/images/kalamang_results.png" alt="" ></a>



</figure>

<p>Gemini 1.5 Pro is exceeds Gemini 1.0 Ultra on text. However the latter is still better on vision and audio tasks, but the compute-efficiency and speed of shipping does suggest some ideas of distillation coming into the picture.</p>













<figure>


  <a data-fancybox="" href="/post/2024-02-18_gemini_pro_google/images/results_comparison.png" >
<img src="/post/2024-02-18_gemini_pro_google/images/results_comparison.png" alt="" ></a>



</figure>

<p>Figures 4 and 5 in the Technical report are super cool. In Figure 4, Gemini 1.5 is able to extract the exact scene with the page number from a textbook given a hand-drawn sketch as the prompt.</p>













<figure>


  <a data-fancybox="" href="/post/2024-02-18_gemini_pro_google/images/fig4_paper.png" >
<img src="/post/2024-02-18_gemini_pro_google/images/fig4_paper.png" alt="" ></a>



</figure>

<p>In Figure 5 they show that it is able to do the same in videos as well. The model returns a detailed description and timestamp of the scene.</p>













<figure>


  <a data-fancybox="" href="/post/2024-02-18_gemini_pro_google/images/fig5_paper.png" >
<img src="/post/2024-02-18_gemini_pro_google/images/fig5_paper.png" alt="" ></a>



</figure>

<p>The cumulative average negative log-likelihood (NLL) as a function of token position in long documents follows a power-law quite accurately up to 1M tokens for long-documents and about 2M tokens for code but deviates at the 10M scale.</p>













<figure>


  <a data-fancybox="" href="/post/2024-02-18_gemini_pro_google/images/pplx.png" >
<img src="/post/2024-02-18_gemini_pro_google/images/pplx.png" alt="" ></a>



</figure>

<p>Though it <a href="https://youtu.be/wa0MT8OwHuk">seems like the inference speeds are quite slow</a> for long contexts at the moment, but the capabilities are absolutely insane!</p>
<hr>
<p>An exciting future ahead!</p>
<p> </p>
<script type="text/javascript" src="//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js" data-dojo-config="usePlainJson: true, isDebug: false"></script>
<!-- <button style="background-color: #70ab17; color: #1770AB" id="openpopup">Subscribe to my posts!</button> -->
<div class="button_cont" align="center"><button id="openpopup" class="example_a">Subscribe to my posts!</button></div>
<style>
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
</style>
<script type="text/javascript">

function showMailingPopUp() {
    window.dojoRequire(["mojo/signup-forms/Loader"], function(L) { L.start({"baseUrl":"mc.us4.list-manage.com","uuid":"0b10ac14f50d7f4e7d11cf26a","lid":"667a1bb3da","uniqueMethods":true}) })

    document.cookie = "MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC";
}

document.getElementById("openpopup").onclick = function() {showMailingPopUp()};

</script>
<p> </p>
<script data-name="BMC-Widget" data-cfasync="false" src="https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js" data-id="shreyanshsingh" data-description="Support me on Buy me a coffee!" data-message="" data-color="#FF5F5F" data-position="Right" data-x_margin="18" data-y_margin="18"></script>
<p>Follow me on <a href="https://twitter.com/shreyansh_26">Twitter</a>, <a href="https://github.com/shreyansh26">Github</a> or connect on <a href="https://www.linkedin.com/in/shreyansh26/">LinkedIn</a>.</p>

    </div>

    





<div class="article-tags">
  
  <a class="badge badge-light" href="/tags/llm/">llm</a>
  
  <a class="badge badge-light" href="/tags/long-context/">long context</a>
  
  <a class="badge badge-light" href="/tags/multimodal/">multimodal</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/&amp;text=Paper%20Summary%20#10%20-%20Gemini%201.5%20Pro" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/&amp;t=Paper%20Summary%20#10%20-%20Gemini%201.5%20Pro" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Paper%20Summary%20#10%20-%20Gemini%201.5%20Pro&amp;body=https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/&amp;title=Paper%20Summary%20#10%20-%20Gemini%201.5%20Pro" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://web.whatsapp.com/send?text=Paper%20Summary%20#10%20-%20Gemini%201.5%20Pro%20https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
  </ul>
</div>












  
  
    
  
  






  
  
  
  
  <div class="media author-card content-widget-hr">
    

    <div class="media-body">
      <h5 class="card-title"><a href="/authors/shreyansh-singh/"></a></h5>
      
      
      <ul class="network-icon" aria-hidden="true">
  
</ul>

    </div>
  </div>




<section id="comments">
  
    
<div id="disqus_thread"></div>
<script>
  let disqus_config = function () {
    
    
    
  };
  (function() {
    if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
      document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
      return;
    }
    var d = document, s = d.createElement('script'); s.async = true;
    s.src = 'https://' + "shreyansh26-netlify-com" + '.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


  
</section>




<div class="article-widget">
  
<div class="post-nav">
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Next</div>
    <a href="/post/2024-02-18_sora_openai/" rel="next">Paper Summary #11 - Sora</a>
  </div>
  
  
  
  <div class="post-nav-item">
    <div class="meta-nav">Previous</div>
    <a href="/post/2023-07-22_solving_substitution_cipher_using_mcmc/" rel="prev">Solving Substitution Ciphers using Markov Chain Monte Carlo (MCMC)</a>
  </div>
  
</div>

</div>



  
  



  </div>
</article>

      

    
    
    
    <script src="/js/mathjax-config.js"></script>
    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js" integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      <script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js" integrity="sha256-yt2kYMy0w8AbtF89WXb2P1rfjcP/HTHLT7097U8Y5b8=" crossorigin="anonymous"></script>

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/highlight.min.js" integrity="sha256-1zu+3BnLYV9LdiY85uXMzii3bdrkelyp37e0ZyTAQh0=" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/r.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/python.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/cpp.min.js"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.10/languages/js.min.js"></script>
        
      

      
      
      <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    

    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.5.1/leaflet.js" integrity="sha256-EErZamuLefUnbMBQbsEqu1USa+btR2oIlCpBJbyD4/g=" crossorigin="anonymous"></script>
    

    
    
    <script>const code_highlighting = true;</script>
    

    
    
    
    
    
    
    <script>
      const search_config = {"indexURI":"/index.json","minLength":1,"threshold":0.3};
      const i18n = {"no_results":"No results found","placeholder":"Search...","results":"results found"};
      const content_type = {
        'post': "Posts",
        'project': "Projects",
        'publication' : "Publications",
        'talk' : "Talks"
        };
    </script>
    

    
    

    
    
    <script id="search-hit-fuse-template" type="text/x-template">
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script>
    

    
    
    <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
    

    
    

    
    
    <script id="dsq-count-scr" src="https://shreyansh26-netlify-com.disqus.com/count.js" async></script>
    

    
    
    
    
    
    
    
    
    
      
    
    
    
    
    <script src="/js/academic.min.bcfae8267aba63cc55af53a503896bd9.js"></script>

    






  
  
  <div class="container">
    <footer class="site-footer">
  

  <p class="powered-by">
    © Shreyansh Singh 2024 &middot; 

    Powered by 
    
    <a href="https://gohugo.io" target="_blank" rel="noopener">Hugo</a>.

    
    <span class="float-right" aria-hidden="true">
      <a href="#" class="back-to-top">
        <span class="button_icon">
          <i class="fas fa-chevron-up fa-2x"></i>
        </span>
      </a>
    </span>
    
  </p>
</footer>

  </div>
  

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

</body>
</html>
