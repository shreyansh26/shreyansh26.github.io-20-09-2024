<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>federated-learning | Shreyansh Singh</title>
    <link>https://shreyansh26.github.io/tags/federated-learning/</link>
      <atom:link href="https://shreyansh26.github.io/tags/federated-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>federated-learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Shreyansh Singh 2022</copyright><lastBuildDate>Mon, 27 Dec 2021 00:00:57 +0530</lastBuildDate>
    <image>
      <url>https://shreyansh26.github.io/img/Shreyansh.jpg</url>
      <title>federated-learning</title>
      <link>https://shreyansh26.github.io/tags/federated-learning/</link>
    </image>
    
    <item>
      <title>PPML Series #3 - Federated Learning for Mobile Keyboard Prediction</title>
      <link>https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/</link>
      <pubDate>Mon, 27 Dec 2021 00:00:57 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/</guid>
      <description>&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Gboard — the Google keyboard, is a virtual keyboard for smartphones with support for more than 900+ language varieties and over 1 billion installs. In addition to decoding noisy signals from input modalities including tap and word-gesture typing, Gboard provides auto-correction, word completion, and next-word prediction features.&lt;/p&gt;
&lt;p&gt;Next-word predictions provide a tool for facilitating text entry and is plays an important role in improving user experience. Based on a small amount of user-generated preceding text, language models (LMs) can predict the most probable
next word or phrase.&lt;/p&gt;
&lt;p&gt;The above figure shows an example: given the text, &amp;ldquo;I love you&amp;rdquo;, Gboard predicts the user is likely to type &amp;ldquo;and&amp;rdquo;, &amp;ldquo;too&amp;rdquo;, or &amp;ldquo;so much&amp;rdquo; next. The centre position
in the suggestion strip is reserved for the highest-probability candidate, while the second and third most likely candidates occupy the left and right positions, respectively.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt; -  The technical details shared in this post are based on the &lt;a href=&#34;https://arxiv.org/abs/1811.03604&#34;&gt;paper&lt;/a&gt; which was published by Google in 2019.  So, some details may be out-of-date, but the core idea behind the solution should still pretty much be the same. Checkout may annotated version of the paper &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/PPML/Federated%20Learning/Federated%20Learning%20for%20Mobile%20Keyboard%20Prediction.pdf&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The primary (static) language model for the English language in Gboard is a Katz smoothed Bayesian interpolated 5-gram LM containing 1.25 million n-grams, including 164,000 unigrams. You can read more about it in &lt;a href=&#34;https://research.google/pubs/pub37567/&#34;&gt;this paper&lt;/a&gt;. We won;t go into this much as the focus of the post is on the next-word prediction task.&lt;/p&gt;
&lt;p&gt;Another important point one should keep in mind is that mobile keyboard models are constrained in multiple ways - the models should be small, the inference time should be low. Users typically expect a visible keyboard response within 20 milliseconds of an input event. And, given the frequency with which mobile keyboard apps are used, client device batteries could be quickly depleted if CPU consumption were not constrained. As a result, language models are usually limited to tens of megabytes in size with vocabularies of hundreds of thousands of words.&lt;/p&gt;
&lt;p&gt;In the paper, the authors also discussed about how RNNs and more specifically LSTMs can be used for language modeling since they can utilize an arbitrary and dynamically-sized context window.&lt;/p&gt;
&lt;h2 id=&#34;where-does-federated-learning-come-in&#34;&gt;Where does Federated Learning come in?&lt;/h2&gt;
&lt;p&gt;For the task of next-word prediction, publicly available datasets could have been used. However, the training distribution of those datasets does not match the population distribution. Using sample user-generated text will require efforts such as logging, infrastructure, dedicated storage and security. And even still, some users might not be comfortable with the collection and remote storage of their personal data.&lt;/p&gt;
&lt;p&gt;For these reasons, the authors use federated learning. The federated learning environment gives users greater control over the use of their data and simplifies the task of incorporating privacy by default with distributed training and aggregation across a population of client devices. An RNN model is trained from scratch in the server and federated environments and achieves recall improvements with respect to the baseline, which is a &lt;a href=&#34;https://dl.acm.org/doi/10.5555/972695.972698&#34;&gt;n-gram finite state transducer (FST)&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;model-architecture-and-training&#34;&gt;Model Architecture and Training&lt;/h2&gt;
&lt;p&gt;A variant of LSTM called Coupled Input and Forget Gate (CIFG) is used. The coupled input and forget gate variant uses only one gate for modulating the input and the cell recurrent self-connections, i.e., &lt;i&gt;f&lt;/i&gt; = 1 - &lt;i&gt;i&lt;/i&gt;. Read more about CIFG in &lt;a href=&#34;https://arxiv.org/abs/1804.04849&#34;&gt;this paper&lt;/a&gt;. Since CIFG uses a single gate to control both the input and recurrent cell self-connections,  the number of parameters per cell is reduced by 25%.
For time step &lt;em&gt;t&lt;/em&gt;, input gate &lt;i&gt;i&lt;sub&gt;t&lt;/sub&gt;&lt;/i&gt; and forget gate &lt;i&gt;f&lt;sub&gt;t&lt;/sub&gt;&lt;/i&gt; have the relation&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/cifg.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/cifg.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The CIFG architecture is advantageous for the mobile device environment because the number of computations and the parameter set size are reduced with no impact on model performance. The model is trained using Tensorflow and on-device inference is supported by Tensorflow Lite. Client device requirements limit the dictionary size to 10,000 words. CIFG&amp;rsquo;s input and output embedding size is 96. A single layer of CIFG with 670 units is used. Overall, 1.4 million parameters comprise the network. After weight quantization, the model shipped to Gboard devices is 1.4 megabytes in size.&lt;/p&gt;
&lt;p&gt;The training of the model was done using the FederatedAveraging (FedAvg) algorithm, which I wrote about in my &lt;a href=&#34;https://shreyansh26.github.io/post/2021-12-11_intro_to_federated_learning/&#34;&gt;previous blog post&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;The paper shows the performance of the CIFG and the FST model on three datasets -  server-hosted logs data, client-held data and in live production experiments.&lt;/p&gt;
&lt;h4 id=&#34;server-based-logs&#34;&gt;Server-based logs&lt;/h4&gt;
&lt;p&gt;Server-based training of the CIFG next-word prediction model relies on data logged from Gboard users who have opted to share snippets of text while typing in &lt;em&gt;Google apps&lt;/em&gt;. The text is truncated to contain short phrases of a few words, and snippets are only sporadically logged from individual users. Prior to training, logs are anonymized and stripped of personally identifiable information. Additionally, snippets are only used for training if they begin with a start of sentence token.&lt;/p&gt;
&lt;p&gt;Asynchronous stochastic gradient descent with a learning rate equal to 10&lt;sup&gt;-3&lt;/sup&gt; and no weight decay or momentum is used to train the server CIFG. Adaptive gradient methods like Adam and AdaGrad do not improve convergence. The network converges after 150 million steps of SGD.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/server-sgd.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/server-sgd.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h4 id=&#34;federated-training-with-client-caches&#34;&gt;Federated training with client caches&lt;/h4&gt;
&lt;p&gt;As with the logs data, each client cache stores text belonging to the device owner, as well as prediction candidates generated by the decoder. Devices must have at least 2 gigabytes of memory available. Additionally, the clients are
only allowed to participate if they are charging, connected to an un-metered network, and idle.&lt;/p&gt;
&lt;p&gt;The FedAvg algorithm is used here. Between 100 and 500 client updates are required to close each round of federated training in Gboard. The server update is achieved via the Momentum optimizer, using Nesterov accelerated gradient, a momentum hyperparameter of 0.9, and a server learning rate of 1.0 .&lt;/p&gt;
&lt;p&gt;On average, each client processes approximately 400 example sentences during a single training epoch. The federated CIFG converges after 3000 training rounds, over the course of which 600 million sentences are processed by 1.5 million clients.&lt;/p&gt;
&lt;p&gt;N-gram model recall is measured by comparing the decoder candidates stored in the on-device training cache to the actual user-entered text.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/cache-sgd.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/cache-sgd.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Recall (metric) for the highest likelihood candidate is important for Gboard because users are more prone to read and utilize predictions in the centre suggestion spot. Both top-1 and top-3 recall are of interest here.&lt;/p&gt;
&lt;p&gt;Server-hosted logs data and client device-owned caches are used to measure prediction recall. Although each contain snippets of data from actual users, the client caches are believed to more accurately represent the true typing data distribution. Cache data, unlike logs, are not truncated in length and are not restricted to keyboard usage in Google-owned apps.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/tab3.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/tab3.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/tab4.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/tab4.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Prediction impression recall is measured by dividing the number of predictions that match the user-entered text by the number of times users are shown prediction candidates. The prediction impression recall metric is typically lower than the standard recall metric. Zero-state prediction events (in which users open the Gboard app but do not commit any text) increase the number of impressions but not matches.&lt;/p&gt;
&lt;p&gt;The prediction click-through rate (CTR), defined as the ratio of the number of clicks on prediction candidates to the number of proposed prediction candidates.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/tab5.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/tab5.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/tab6.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-27_federated_learning_mobile_keyboard/images/tab6.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;For both the server training and the federated training, the CIFG model improves the top-1 and top-3 recall with respect to the baseline n-gram FST model.&lt;/p&gt;
&lt;p&gt;These gains are impressive given that the n-gram model uses an order of magnitude larger vocabulary and includes personalized components such as user history and contacts language models.&lt;/p&gt;
&lt;p&gt;The results also demonstrate that the federated CIFG performs better on recall metrics than the server-trained CIFG. Comparisons on server-hosted logs data show the recall of the two models is comparable, though the logs are not as representative of the true typing distribution.&lt;/p&gt;
&lt;p&gt;Different flavors of SGD are used in each training context—the results show that federated learning provides a preferable alternative to server-based training of neural language models.&lt;/p&gt;
&lt;h2 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;CIFG language model trained from scratch using federated learning can outperform an identical server trained CIFG model and baseline n-gram model on the keyboard next-word prediction task.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/PPML/Federated%20Learning/Federated%20Learning%20for%20Mobile%20Keyboard%20Prediction.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;That&amp;rsquo;s all for now!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PPML Series #2 - Federated Optimization Algorithms - FedSGD and FedAvg</title>
      <link>https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/</link>
      <pubDate>Sat, 18 Dec 2021 00:16:23 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/</guid>
      <description>&lt;p&gt;In my last post, I covered a high-level overview of Federated Learning, its applications, advantages &amp;amp; challenges.&lt;/p&gt;
&lt;p&gt;We also went through a high-level overview of how Federated Optimization algorithms work. But from a mathematical sense, how is Federated Learning training actually performed? That&amp;rsquo;s what we will be looking at in this post.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;There was a &lt;a href=&#34;https://arxiv.org/abs/1602.05629&#34;&gt;paper&lt;/a&gt;, Communication-Efficient Learning of Deep Networks from Decentralized Data by Google (3637 citations!!!), in which the authors had proposed a federated optimization algorithm called FedAvg and compared it with a naive baseline, FedSGD.&lt;/p&gt;
&lt;h2 id=&#34;fedsgd&#34;&gt;FedSGD&lt;/h2&gt;
&lt;p&gt;Stochastic Gradient Descent (SGD) had shown great results in deep learning. So, as a baseline, the researchers decided to base the Federated Learning training algorithm on SGD as well. SGD can be applied naively to the federated optimization problem, where a single batch gradient calculation (say on a randomly selected client) is done per round of communication.&lt;/p&gt;
&lt;p&gt;The paper showed that this approach is computationally efficient, but requires very large numbers of rounds of training to produce good models.&lt;/p&gt;
&lt;p&gt;Before we get into the maths, I&amp;rsquo;ll define a few terms -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/Latex-FL.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/Latex-FL.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The baseline algorithm, was called FedSGD, short for Federated SGD.&lt;/p&gt;
&lt;p&gt;For FedSGD, the parameter &lt;em&gt;C&lt;/em&gt; (explained above) which controls the global batch size is set to 1. This corresponds to a full-batch (non-stochastic) gradient descent. For the current global model &lt;i&gt;w&lt;sup&gt;t&lt;/sup&gt;&lt;/i&gt;, the average gradient on its global model is calculated for each client &lt;em&gt;k&lt;/em&gt;.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedSGD-updates.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedSGD-updates.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The central server then aggregates these gradients and applies the update.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedSGD-updates2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedSGD-updates2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;fedavg&#34;&gt;FedAvg&lt;/h2&gt;
&lt;p&gt;We saw FedSGD. Now let&amp;rsquo;s make a small change to the update step above.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedAvg-updates.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedAvg-updates.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;What this does is that now each client locally takes one step of gradient descent
on the current model using its local data, and the server then takes a weighted average of the resulting models.&lt;/p&gt;
&lt;p&gt;This way we can add more computation to each client by iterating the local update multiple times before doing the averaging step. This small modification results in the FederatedAveraging (FedAvg) algorithm.&lt;/p&gt;
&lt;p&gt;But why make this change? The answer is in my last post -&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In practice, major speedups are obtained when computation on each client is improved, once a minimum level of parallelism over clients is achieved.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The amount of computation is controlled by three parameters -&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;C&lt;/strong&gt; - Fraction of clients participating in that round
&lt;strong&gt;E&lt;/strong&gt; - No. of training passes each client makes over its local dataset each round
&lt;strong&gt;B&lt;/strong&gt; - Local minibatch size used for client updates&lt;/p&gt;
&lt;p&gt;The pseudocode for the FedAvg algorithm is shown below.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedAvg-algo.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedAvg-algo.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;B = ꝏ (used in experiments) implies full local dataset is treated as the minibatch. So, setting B = ꝏ and E = 1 makes this the FedSGD algorithm.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Okay, now let&amp;rsquo;s look at some experimental results, although I would also suggest looking up the results from the original paper as well.
One experiment showed the number of rounds required to attain a target accuracy, in two tasks - MNIST and a character modelling task.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/res1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/res1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here, IID and non-IID here refer to the datasets that were artificially generated by the authors to represent two kinds of distributions - IID, in which there is in fact an IID distribution among the clients. And non-IID in which the data is not IID among the clients. For example, for the MNIST dataset the authors studied two ways of partitioning the MNIST data over clients: IID, where the data is shuffled, and then partitioned into 100 clients each receiving 600 examples, and Non-IID, where we first sort the data by digit label, divide it into 200 shards of size 300, and assign each of 100 clients 2 shards. For the language modeling task, the dataset was built from &lt;em&gt;The Complete Works of William Shakespeare&lt;/em&gt;. From the paper -&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We construct a client dataset for each speaking role in each play with at least two lines. This produced a dataset with 1146 clients. For each client, we split the data into a set of training lines (the first 80% of lines for the role), and test lines (the last 20%, rounded up to at least one line). The resulting dataset has 3,564,579 characters in the training set, and 870,014 characters in the test set. This data is substantially unbalanced, with many roles having only a few lines, and a few with a large number of lines. Further, observe the test set is not a random sample of lines, but is temporally separated by the chronology of each play. Using an identical train/test split, we also form a balanced and IID version of the dataset, also with 1146 clients.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For the MNIST dataset, a CNN with two 5x5 convolution layers (the first with 32 channels, the second with 64, each followed with 2x2 max pooling), a fully connected layer with 512 units and ReLu activation, and a final softmax output layer was used. And for the language modeling task, a stacked character-level LSTM language model, which after reading each character in a line, predicts the next character. The model takes a series of characters as input and embeds each of these into a learned 8 dimensional space. The embedded characters are then processed through 2 LSTM layers, each with 256 nodes. Finally the output of the second LSTM layer is sent to a softmax output layer with one node per character. The full model has 866,578 parameters, and we trained using an unroll length of 80 characters.&lt;/p&gt;
&lt;p&gt;From the results in the paper, it could be seen that in both the IID and non-IID settings, keeping a small mini-batch size and higher number of training passes on each client per round resulted in the model converging faster. For all model classes, FedAvg converges to a higher level of test accuracy than the baseline FedSGD models. For the CNN, the B = ꝏ; E = 1 FedSGD model reaches 99.22% accuracy in 1200 rounds, while the B = 10 ;E = 20 FedAvg model reaches an accuracy of 99.44% in 300 rounds.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/res2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/res2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The authors also hypothesise that in addition to lowering communication costs, model averaging produces a regularization benefit similar to that achieved by dropout.&lt;/p&gt;
&lt;p&gt;All in all, the experiments demonstrated that the FedAvg algorithm was robust to unbalanced and non-IID distributions, and also reduced the number of rounds of communication required for training, by orders of magnitude.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I wrote a Twitter thread on this topic as well - do give it a like/follow me if you liked the article.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;My last thread covered a high-level overview of Federated Learning, its applications, advantages &amp;amp; challenges.&lt;br&gt;&lt;br&gt;But from a mathematical sense, how is Federated Learning training actually performed? That&amp;#39;s what we will be looking at in this thread 🧵&lt;a href=&#34;https://t.co/anyvEluWoq&#34;&gt;https://t.co/anyvEluWoq&lt;/a&gt;&lt;/p&gt;&amp;mdash; Shreyansh Singh (@shreyansh_26) &lt;a href=&#34;https://twitter.com/shreyansh_26/status/1463454860460785670?ref_src=twsrc%5Etfw&#34;&gt;November 24, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;That&amp;rsquo;s the end for now!&lt;/p&gt;
&lt;p&gt;This post finishes my summary on the basics of Federated Learning and is also a concise version of the very famous paper &amp;ldquo;Communication-Efficient Learning of Deep Networks from Decentralized Data&amp;rdquo; by Google.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/PPML/Federated%20Learning/Communication-Efficient%20Learning%20of%20Deep%20Networks%20from%20Decentralized%20Data.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If the post helps you or you have any questions, do let me know!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PPML Series #1 - An introduction to Federated Learning</title>
      <link>https://shreyansh26.github.io/post/2021-12-11_intro_to_federated_learning/</link>
      <pubDate>Sat, 11 Dec 2021 16:17:16 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-12-11_intro_to_federated_learning/</guid>
      <description>&lt;hr&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Privacy-preserving Machine Learning had always been exciting for me. Since my B.Tech. thesis involving PPML (SMPC + Computer Vision), I didn&amp;rsquo;t get a chance to work on it after that. So, after about 2 years, I have started to read about it again, and sharing it with the community.&lt;/p&gt;
&lt;p&gt;Federated Learning is a domain that I had somewhat eluded during my thesis. I had some idea about the topic but didn&amp;rsquo;t get into it much. So, I decided to start with FL this time. There is a ton of literature out there and is a field of active interest right now.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Modern mobile devices have abundance of data, majorly textual data, image data. Applying machine learning to these can definitely help improve user experience. For example - your mobile keyboard uses language models can improve speech recognition and text entry, your photos apps (Google Photos, say) has image models that can automatically select good photos.&lt;/p&gt;
&lt;p&gt;However, we have two problems here. Firstly, if we consider millions of devices, then this data is large in quantity. Secondly, this dataset may have personal pictures, textual information written by the device owners,among other things.  Hence, this data is privacy sensitive in most cases. This creates problems to store this data in a database. It can be both infeasible as well as cause privacy violations.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Federated Learning&lt;/em&gt; is a decentralised machine learning approach which allows to leave the training data on the individual devices and learns a shared model by aggregating locally computed gradient updates. Federated Learning can significantly reduce privacy and security risks by limiting the attack surface to only the device, rather than the device and the cloud. Obviously, some level of trust on the server coordinating the training is still required.&lt;/p&gt;
&lt;h2 id=&#34;why-fl&#34;&gt;Why FL?&lt;/h2&gt;
&lt;p&gt;Federated Learning usually helps in three contexts -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Training on real-world data from mobile devices provides advantage over training on proxy data stored in data centres. The distributions from which these examples are drawn are likely to differ substantially from easily available proxy datasets: the use of language in chat and text messages is generally much different than standard language corpora, e.g., chat messages are not like Wikipedia articles. Images taken through the camera are also not like Flickr images.&lt;/li&gt;
&lt;li&gt;The  data is privacy sensitive or large in size (compared to the size of the model), so it is preferable not to log it to the data centre purely for the purpose of model training.&lt;/li&gt;
&lt;li&gt;Sometimes, labels for tasks can naturally be obtained from user interaction. Entered text is self-labeled for learning a language model, and photo labels can be defined by natural user interaction with their photo app (which photos are deleted, shared, or viewed).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Applications of FL could include image classification, predicting which images will be viewed multiple times in the future, language modelling, next word/phrase prediction.&lt;/p&gt;
&lt;h3 id=&#34;how-does-fl-provide-privacy-up-to-a-certain-extent&#34;&gt;How does FL provide privacy (up to a certain extent)?&lt;/h3&gt;
&lt;p&gt;Handling even anonymized data can lead to privacy concerns. What better way than to use the data itself to train the models but at the same time, not risk its privacy. In contrast, the information transmitted for federated learning is the minimal update necessary to improve a particular model. They will generally contain much less information about the raw data. Further, the source of the updates is not needed by the aggregation algorithm, so updates can be transmitted without identifying meta-data over a mix network such as Tor or trusted third-parties.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;h2 id=&#34;federated-optimization&#34;&gt;Federated Optimization&lt;/h2&gt;
&lt;p&gt;The optimization problem implicit in federated learning as federated optimization, drawing a connection (and contrast) to distributed optimization.&lt;/p&gt;
&lt;h3 id=&#34;how-is-fl-different-from-any-other-distributed-optimization-problem&#34;&gt;How is FL different from any other distributed optimization problem?&lt;/h3&gt;
&lt;p&gt;In federated optimization, there are a few key properties that differentiate from a typical distributed optimization problem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Non-IID&lt;/strong&gt; - Training data will vary from user to user and will not have properties that are similar to the population.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unbalanced&lt;/strong&gt; - Some users use a device more and generate more data, some less.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Massively distributed&lt;/strong&gt; - Number of users are much more than the average number of examples per client.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limited communication&lt;/strong&gt; - Devices are frequently offline and are on slow or expensive connections.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-perform-federated-optimization&#34;&gt;How to perform Federated Optimization?&lt;/h3&gt;
&lt;p&gt;In this blog post, I won&amp;rsquo;t go into the mathematical details regarding the optimization techniques used in Federated Learning. However, we will be discussing the high-level overview of how training is performed. The steps are as follows -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a fixed set of &lt;em&gt;K&lt;/em&gt; clients, each with a fixed local dataset.&lt;/li&gt;
&lt;li&gt;At the beginning of each round, a random fraction &lt;em&gt;C&lt;/em&gt; of clients is selected, and the server sends the current global algorithm state to each of these clients (e.g., the current model parameters).&lt;/li&gt;
&lt;li&gt;Only a fraction of clients is selected for efficiency, as experiments show diminishing returns for adding more clients beyond a certain point.&lt;/li&gt;
&lt;li&gt;Each selected client then performs local computation based on the global state and its local dataset, and sends an update to the server.&lt;/li&gt;
&lt;li&gt;The server then applies these updates to its global state, and the process repeats.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;challenges&#34;&gt;Challenges&lt;/h2&gt;
&lt;p&gt;In general, for ML tasks, and in data centres, the costs of compute is what is the most important - GPUs are used to lower the computation cost.
In Federated Learning, the communication costs somewhat dominate.&lt;/p&gt;
&lt;p&gt;But what do communication costs mean here?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Upload bandwidths in mobiles (globally) is limited to 1 MB/s or less.&lt;/li&gt;
&lt;li&gt;Clients volunteer only if the charged, plugged-in and on free/unmetered WiFi connections.&lt;/li&gt;
&lt;li&gt;Each client participates in only a small number of rounds per day.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Federated Learning, computation is not much of an issue because the dataset size on each device will be relatively much less and modern smartphones now have processors fast enough to do those computations locally. So, the goal becomes to use additional computation in order to decrease the number of rounds of communication needed to train a model.&lt;/p&gt;
&lt;p&gt;So, the goal becomes to use additional computation in order to lower the number of rounds of communication needed to train the model.&lt;/p&gt;
&lt;p&gt;There are two approaches which can be adopted for this -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Increased parallelism&lt;/em&gt; -  More clients are used which work independently between each communication round.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Increased computation on each client&lt;/em&gt; - Rather than performing a simple computation like gradient calculation, each client performs a more complex calculation between each communication round.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In practice, major speedups are obtained when computation on each client is improved, once a minimum level of parallelism over clients is achieved.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I wrote a Twitter thread on this topic as well - do give it a like/follow me if you liked the article.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;I recently started reading about Privacy-preserving ML, as this has been a topic that has always interested me.&lt;br&gt;I hope to share my learnings here on Twitter.&lt;br&gt;&lt;br&gt;I started with Federated Learning and here&amp;#39;s a detailed thread that will give you a high-level idea of FL🧵&lt;/p&gt;&amp;mdash; Shreyansh Singh (@shreyansh_26) &lt;a href=&#34;https://twitter.com/shreyansh_26/status/1462262151209381888?ref_src=twsrc%5Etfw&#34;&gt;November 21, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;This is all for now. Thanks for reading!&lt;/p&gt;
&lt;p&gt;In my next post, I&amp;rsquo;ll share a mathematical explanation as to how optimization (learning) is done in a Federated Learning setting. I will also explain some experimental results that have been published.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
