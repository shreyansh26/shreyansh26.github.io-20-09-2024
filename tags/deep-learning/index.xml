<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>deep learning | Shreyansh Singh</title>
    <link>https://shreyansh26.github.io/tags/deep-learning/</link>
      <atom:link href="https://shreyansh26.github.io/tags/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>deep learning</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Shreyansh Singh 2023</copyright><lastBuildDate>Sun, 26 Mar 2023 15:47:48 +0530</lastBuildDate>
    <image>
      <url>https://shreyansh26.github.io/img/Shreyansh.jpg</url>
      <title>deep learning</title>
      <link>https://shreyansh26.github.io/tags/deep-learning/</link>
    </image>
    
    <item>
      <title>Paper Summary #8 - FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness</title>
      <link>https://shreyansh26.github.io/post/2023-03-26_flash-attention/</link>
      <pubDate>Sun, 26 Mar 2023 15:47:48 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2023-03-26_flash-attention/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2205.14135&#34;&gt;https://arxiv.org/abs/2205.14135&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré&lt;br&gt;
&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/HazyResearch/flash-attention&#34;&gt;https://github.com/HazyResearch/flash-attention&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/General-DL/FlashAttention%20-%20Fast%20and%20Memory-Efficient%20Exact%20Attention.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I finished reading the FlashAttention paper recently and thought that it would be good to have a technical write-up of the paper, so that understand the concept well. I decided to make it public and hopefully it can help anyone reading this.&lt;/p&gt;
&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;Attention as we know, in its standard implementation is an $O(N^2)$ operation, where N is the sequence length. There are many approximate attention methods out there like Reformer, Smyrf, Reformer, Performer and others (&lt;a href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/&#34;&gt;you can find more details on a few of these in my previous blog&lt;/a&gt;) which aim to reduce the compute requirements to linear or near-linear in sequence length, but many of them do not display wall-clock speedup against standard attention. They focus on FLOP reduction (which doesn&amp;rsquo;t always correlate with wall-clock speed) and tend to ignore overheads from memory access (IO).FlashAttention aims to incorporate IO-awareness i.e. dividing operations between faster and slower levels of GPU memory to make the whole computation faster. The algorithm uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. FlashAttention can also be extended to block-spare attention and this results in the fastest approximate (or not) attention algorithm out there.&lt;/p&gt;
&lt;p&gt;All this helps to improve the training time of Transformer models - a 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3× speedup on GPT-2 (seq. length 1K). This memory-efficient approach also helps to incorporate a longer context (up to 16k/64k tokens) which also results in better models (0.7 better perplexity on GPT-2).&lt;/p&gt;
&lt;p&gt;I&amp;rsquo;ll describe more details in the future sections.&lt;/p&gt;
&lt;h2 id=&#34;background---hardware-performance&#34;&gt;Background - Hardware Performance&lt;/h2&gt;
&lt;p&gt;Since FlashAttention computes exact attention, and the major crux of their work is the efficient hardware usage, it is important to know a bit about GPU memory and the performance characteristics of various kinds of operations on it.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/gpu_mem.png&#34; data-caption=&#34;A100 GPU Memory Hierarchy. Source - https://arxiv.org/abs/2205.14135&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/gpu_mem.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    A100 GPU Memory Hierarchy. Source - &lt;a href=&#34;https://arxiv.org/abs/2205.14135&#34;&gt;https://arxiv.org/abs/2205.14135&lt;/a&gt;
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;gpu-memory-hierarchy&#34;&gt;GPU Memory Hierarchy&lt;/h3&gt;
&lt;p&gt;For a A100 GPU with 40GB of High Memory Bandwidth (HBM), a rough diagram of the memory hierarchy is shown above. The SRAM memory us spread across 108 streaming multiprocessors (SMs), 192KB for each. As one can see, the on-chip SRAM is much faster the HBM but is much smaller than size. In terms of compute, the theoretical peak throughput for BFLOAT16 using Tensor Core is 312 TFLOPS. With time, compute has gotten much faster relative to memory speed, hence processes (operations) are increasingly bottlenecked by memory (HBM) access. Thus, the goal of the FlashAttention paper was to use the SRAM as well as efficiently as possible to speed up the computation.&lt;/p&gt;
&lt;h3 id=&#34;execution-model&#34;&gt;Execution Model&lt;/h3&gt;
&lt;p&gt;he typical way in which GPUs operate are that they use a large number of threads to perform an operation, which is called a kernel. The input is loaded from the HBM to the registers and SRAM, and written back to the HBM after computation.&lt;/p&gt;
&lt;h3 id=&#34;performance-characteristics&#34;&gt;Performance Characteristics&lt;/h3&gt;
&lt;p&gt;There is a term called &lt;strong&gt;arithmetic intensity&lt;/strong&gt; which is given by the number of arithmetic operations per byte of memory access. It helps to understand the bottleneck of an operation. An operation can be characterized as compute-bound (also called math-bound) or memory-bound.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Compute-bound&lt;/strong&gt; - When the bottleneck is the compute i.e., the time taken by the operation is determined by how many arithmetic operations there are since the time taken due to HBM accesses is relative lower. E.g. of such operations are matrix multiplication with large inner dimension, and convolution with large number of channels.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Memory-bound&lt;/strong&gt; - When the bottleneck is the memory i.e., the time taken by the operation is determined by the number of memory accesses there are since the time spent in computation is relative lower. E.g. of such processes are most other operation like elementwise operations - activation, dropout and reduction operations - sum, softmax, batch normalization, layer normalization.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To understand this better, let&amp;rsquo;s analyze it mathematically. Let $N_{op}$ be the number of arithmetic/floating point operations, $N_{byte}$ be the number of memory accesses, ${BW}_{compute}$ and ${BW}_{memory}$ be the compute and memory bandwidth respectively, the time taken for compute operations and memory accesses can be determined as -&lt;/p&gt;
&lt;p&gt;$$t_{compute} = \frac{N_{op}}{{BW}_{compute}}$$
$$t_{memory} = \frac{N_{byte}}{{BW}_{memory}}$$&lt;/p&gt;
&lt;p&gt;The operation is compute-bound if $t_{compute}$ is less than $t_{memory}$ and vice-versa for memory bound. Which mathematically becomes -&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For compute-bound&lt;/strong&gt;
$$\frac{N_{op}}{N_{byte}} \gt \frac{{BW}_{compute}}{{BW}_{memory}}$$&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;For memory-bound&lt;/strong&gt;
$$\frac{N_{op}}{N_{byte}} \lt \frac{{BW}_{compute}}{{BW}_{memory}}$$&lt;/p&gt;
&lt;p&gt;As mentioned above as well, matrix multiplication for large inner dimensions is compute bound but below that it is memory bound. If using FP32 and plugging in numbers for A100 40GB, then for $N \lt 74$, the $N \times N$ multiplication is memory bound, but compute bound when $N$ is greater than that. A great and detailed resource to understand this theory is this &lt;a href=&#34;https://leimao.github.io/blog/Math-Bound-VS-Memory-Bound-Operations/&#34;&gt;blog post by Lei Mao&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;kernel-fusion&#34;&gt;Kernel Fusion&lt;/h3&gt;
&lt;p&gt;Kernel Fusion is often down by compilers to fuse together multiple elementwise operations. It is used to accelerate memory-bound operations. The basic ideas is that instead of loading the input from the HBM, performing the operation and writing back to the HBM and repeating that for each operation applied to the same input, the operation can be fused so that all of the operations are performed at once when the input is loaded from the HBM.&lt;/p&gt;
&lt;p&gt;However, one must note that when performing model training, the effectiveness of kernel fusion is reduced as the intermediate values still have to be written to the HBM to save for the backward pass.&lt;/p&gt;
&lt;h2 id=&#34;background---standard-attention&#34;&gt;Background - Standard Attention&lt;/h2&gt;
&lt;p&gt;For anyone familiar with transformers, this equation is well-known -&lt;/p&gt;
&lt;p&gt;$$Attention(Q, K, V) = softmax(\frac{QK^\mathsf{T}}{\sqrt{d_k}})V$$&lt;/p&gt;
&lt;p&gt;Here, the sequences $Q, K, V  \in \mathbb{R}^{N \times d}$ where $N$ is the sequence length and $d$ is the head dimension. The attention output, above, can be denoted by $O \in \mathbb{R}^{N \times d}$. The equation can be broken down as -&lt;/p&gt;
&lt;p&gt;$$\mathbf{S} = \mathbf{QK^\mathsf{T}} \in \mathbb{R}^{N \times N},\quad \mathbf{P} = softmax(\mathbf{S}) \in \mathbb{R}^{N \times N},\quad \mathbf{O} = \mathbf{PV} \in \mathbb{R}^{N \times d}$$&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/att2.png&#34; data-caption=&#34;Scaled Dot Product Attention&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/att2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Scaled Dot Product Attention
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In standard attention implementations, the $\mathbf{S}$ and $\mathbf{P}$ matrices are materialized in the HBM, which takes $O(N^2)$ memory. Also, most operations are memory-bound/elementwise operations, e.g. softmax applied on $\mathbf{P}$, masking applied to $\mathbf{S}$, dropout applied to $\mathbf{P}$. This leads to slow wall-clock time.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/standard-att-algo.png&#34; data-caption=&#34;Standard Attention Implementation&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/standard-att-algo.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Standard Attention Implementation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;flashattention---algorithm-details&#34;&gt;FlashAttention - Algorithm details&lt;/h2&gt;
&lt;p&gt;As one may understand, the materialization of the $N \times N$ attention matrix on the HBM and its repeated reading and writing is a major bottleneck. To solve this, two main things need to be done -&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Computing the softmax reduction without access to the whole input&lt;/li&gt;
&lt;li&gt;Not storing the large intermediate attention matrix for the backward pass&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Two established techniques, namely &lt;strong&gt;tiling&lt;/strong&gt; and &lt;strong&gt;recomputation&lt;/strong&gt; are used to solve this.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Tiling - The attention computation is restructured to split the input into blocks and performing the softmax operation incrementally by making several passes over the input blocks.&lt;/li&gt;
&lt;li&gt;Recomputation - The softmax normalization factor from the forward pass is stored to quickly recompute attention on-chip in the backward pass, which is faster than the standard attention approach of reading the intermediate matrix from HBM.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This does lead to increased FLOPs due to recomputation, however FlashAttention runs both faster (up to 7.6x on GPT-2) and uses less memory — linear in sequence length, due to the massively reduced amount of HBM access.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/gpt2-att-speedup.png&#34; data-caption=&#34;Speedup over the PyTorch implementation of attention on GPT-2&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/gpt2-att-speedup.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Speedup over the PyTorch implementation of attention on GPT-2
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;understanding-the-algorithm&#34;&gt;Understanding the algorithm&lt;/h3&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/flash-attention-schematic.png&#34; data-caption=&#34;FlashAttention Forward Pass Algorithm&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/flash-attention-schematic.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    FlashAttention Forward Pass Algorithm
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The main idea behind the algorithm is to split the inputs $\mathbf{Q, K, V}$ into blocks, loading them from slow HBM to fast SRAM and then computing the attention output w.r.t those blocks. The output of each block is scaled by the right normalization factor before adding them up, which gives the correct result.&lt;/p&gt;
&lt;p&gt;$$\mathbf{S} = \mathbf{\tau QK^\mathsf{T}} \in \mathbb{R}^{N \times N},\quad \mathbf{S}^\mathrm{masked} = \mathrm{MASK}(S) \in \mathbb{R}^{N \times N},\quad \mathbf{P} = softmax(\mathbf{S^\mathrm{masked}}) \in \mathbb{R}^{N \times N},$$&lt;/p&gt;
&lt;p&gt;$$\mathbf{P}^\mathrm{dropped} = \mathrm{dropout}(\mathbf{P}, p_\mathrm{drop}), \quad \mathbf{O} = \mathbf{P^\mathrm{dropped}V} \in \mathbb{R}^{N \times d},$$&lt;/p&gt;
&lt;p&gt;where $\tau \in \mathbb{R}$ is some softmax scaling factor (typically $\frac{1}{\sqrt{d}}$), $\mathrm{MASK}$ is some masking function that sets some entries of
the input to $-\infty$ and keep other entries the same, and $\mathrm{dropout}(x, p)$ applies dropout to 𝑥 elementwise (i.e., output $\frac{x}{1-p}$ with probability $1 − p$ and output $0$ with probability $p$ for each element $x$)&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/flash-attention-forward-algo.png&#34; data-caption=&#34;FlashAttention Forward Pass Algorithm&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/flash-attention-forward-algo.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    FlashAttention Forward Pass Algorithm
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;tiling&#34;&gt;Tiling&lt;/h4&gt;
&lt;p&gt;The key part in understanding the block-wise computation of attention in the algorithm above is the block-wise computation of the softmax. The paper explains it well though. The softmax of a vector $x \in \mathbb{R}^B$ can be computed as -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/softmax-1.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/softmax-1.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;And for vectors $x^\mathrm{(1)}, x^\mathrm{(2)} \in \mathbb{R}^B$, the softmax of the concatenated $x = [x^\mathrm{(1)}, x^\mathrm{(2)}] \in \mathbb{R}^{2B}$ is given by -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/softmax-2.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/softmax-2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Let&amp;rsquo;s understand this better. In the above equations, $m(x)$ holds the maximum between $m(x^\mathrm{(1)})$ and $m(x^\mathrm{(2)})$. Now, $m(x^\mathrm{(1)})$ is the maximum element of $x^\mathrm{(1)}$ and $m(x^\mathrm{(2)})$ is the maximum element of $x^\mathrm{(2)}$ which means that $m(x)$ is basically the maximum of the whole concatenated vector. The beauty is that this was done blockwise.&lt;/p&gt;
&lt;p&gt;So, if statistics $(m(x), l(x))$ are tracked then softmax can be computed one block at a time. In line 12 of the algorithm, $\tilde{m_{ij}}$ has the maximum element of each row of $S_{ij}^\mathrm{masked}$, and next in line 13, $m_i^\mathrm{new}$ holds the row-wise maximum of the $m_i$ till now and the new one i.e., $\tilde{m_{ij}}$. Hence $m_i$ is updated every column from the outer loop and eventually stores the row-wise max of the matrix $\mathbf{S}$. The same logic goes for $l_i$ and the matrix $\mathbf{P}$. The results are combined to get the output attention matrix in line 15.&lt;/p&gt;
&lt;h4 id=&#34;recomputation&#34;&gt;Recomputation&lt;/h4&gt;
&lt;p&gt;The backward pass of FlashAttention requires the $\mathbf{S}$ and $\mathbf{P}$ matrices to compute the gradients w.r.t $\mathbf{Q}$,$\mathbf{K}$,$\mathbf{V}$. However, they are $N \times N$ matrices and as it can be seen in the algorithm above, they aren&amp;rsquo;t stored explicitly. The trick is to use the output $\mathbf{O}$ and the softmax normalization statistics $(m, l)$, we can recompute the attention matrix $\mathbf{S}$ and $\mathbf{P}$ easily in the backward pass from blocks of $\mathbf{Q}$,$\mathbf{K}$,$\mathbf{V}$ in SRAM. even with more FLOPs, the recomputation step speeds up the backward pass due to reduced HBM accesses. The backward pass is very interesting too but slightly more complicated hence I&amp;rsquo;ll probably cover it in a separate post. One can cover the Appendix B of the paper to learn more.&lt;/p&gt;
&lt;p&gt;Kernel Fusion is also used to implement the algorithm in one CUDA kernel, loading input from HBM, performing all the computation steps (matrix multiply, softmax, optionally masking and dropout, matrix multiply), then writing the result back to HBM. This avoids repeatedly reading and writing of inputs and outputs from and to HBM.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important Information&lt;/strong&gt; - &lt;em&gt;The FlashAttention algorithm computed $\mathbf{O} = softmax(QK^\mathsf{T})V$ with $O(N^2d)$ FLOPs and requires $O(N)$ additional memory beyond inputs and output (for the $(l, m)$ statistics).&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;The proof for the FLOPs calculation is given in Appendix C of the paper, which should be checked out by the curious reader.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Important Information&lt;/strong&gt; - &lt;em&gt;Let $N$ be the sequence length, $d$ be the head dimension, and $M$ be the size of SRAM with $d \leq M \leq Nd$. Standard attention requires $\Theta(Nd + N^2)$ HBM accesses while FlashAttention requires $\Theta(N^2d^2M^{-1})$ HBM accesses.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For typical values of $d$ (64-128) and $M$ (around 100KB), $d^2$ is many times smaller than $M$, and thus FlashAttention requires many times fewer HBM accesses than standard implementation. This leads to both faster execution and a lower memory footprint.&lt;/p&gt;
&lt;p&gt;The authors also go on to show that the number of HBM accesses by FlashAttention is a lower-bound. There can be no implementation which can asymptotically improve on the number of HBM accesses for all values of $M$ when doing exact attention calculation.&lt;/p&gt;
&lt;p&gt;As the block size increases, the number of HBM accesses decreases as there are less passes over the input, and the runtime also decreases. However, beyond 256, the runtime starts getting bottlenecked by factors like arithmetic operations. And there is also a limit on how large we can choose the block size to be, as we want it to be able to fit in the SRAM.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-1.png&#34; data-caption=&#34;Left - Comparison of standard attention and FlashAttention for GPT-2 medium on A100. Despite the higher FLOPs (due to the recomputation step in backward pass), the lesser number of HBM access leads to a much faster runtime. Right - The effect of block size on the forward runtime and HBM accesses.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-1.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;Left&lt;/strong&gt; - Comparison of standard attention and FlashAttention for GPT-2 medium on A100. Despite the higher FLOPs (due to the recomputation step in backward pass), the lesser number of HBM access leads to a much faster runtime. &lt;strong&gt;Right&lt;/strong&gt; - The effect of block size on the forward runtime and HBM accesses.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;block-sparse-flashattention&#34;&gt;Block-Sparse FlashAttention&lt;/h3&gt;
&lt;p&gt;As mentioned in the overview, FlashAttention can be used to make a approximate attention algorithm as well. The authors call it Block-Sparse FlashAttention and it is the fastest approximate attention algorithm. The memory complexity is smaller than FlashAttention by a factor proportional to the sparsity.&lt;/p&gt;
&lt;p&gt;For inputs $\mathbf{Q, K, V} \in \mathbb{R}^{N \times d}$ and a mask $\tilde{\mathbf{M}} \in { 0,1 }^{N \times N}$, we want to calculate -&lt;/p&gt;
&lt;p&gt;$$\mathbf{S} = \mathbf{QK^\mathsf{T}} \in \mathbb{R}^{N \times N},\quad \mathbf{P} = softmax(\mathbf{S} \odot \mathbb{1}_{\tilde{\mathbf{\mathrm{M}}}}) \in \mathbb{R}^{N \times N},\quad \mathbf{O} = \mathbf{PV} \in \mathbb{R}^{N \times d}$$&lt;/p&gt;
&lt;p&gt;Given a pre-defined block sparsity mask $\mathbf{M} \in { 0,1 }^{N/B_r \times N/B_c}$, Algorithm 2 above can be adapted to only compute the nonzero blocks of the attention matrix. We can just skip the zero blocks. The Algorithm shown below describes the forward pass of Block-sparse FlashAttention.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/blocksparse-flash-attention-forward-algo.png&#34; data-caption=&#34;Blcok-Sparse FlashAttention Forward Pass Algorithm&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/blocksparse-flash-attention-forward-algo.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Blcok-Sparse FlashAttention Forward Pass Algorithm
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;Important Information&lt;/strong&gt; - &lt;em&gt;Let $N$ be the sequence length, $d$ be the head dimension, and $M$ be the size of SRAM with $d \leq M \leq Nd$. Block-sparse FlashAttention requires $\Theta(Nd + N^2d^2M^{-1}s)$ HBM accesses where $s$ is the fraction of nonzero blocks in the block-sparsity mask.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;For large sequence lengths, $s$ is set to $N^{-1/2}$ or $N^{-1} \log N$ resulting in $\Theta(N \sqrt{N})$ or $\Theta(N \log N)$ IO complexity. As the sparsity increases, the runtime of block-sparse FlashAttention improves proportionally.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-2.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;experiments&#34;&gt;Experiments&lt;/h2&gt;
&lt;p&gt;There are tons of results in the paper. But the TL;DR is that FlashAttention beats all other exact attention algorithms in both training speed and quality of the models/down stream models especially when pushed to the limits of sequence length. I&amp;rsquo;ll add the plots and graphs for their various results here. Additional results are present in the paper.&lt;/p&gt;
&lt;h3 id=&#34;training-speed&#34;&gt;Training Speed&lt;/h3&gt;
&lt;h4 id=&#34;bert&#34;&gt;BERT&lt;/h4&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-3.png&#34; data-caption=&#34;Training time of BERT-large. starting from the same initialization provided by the MLPerf benchmark, to reach the target accuracy of 72.0% on masked language modeling. Averaged over 10 runs on 8×A100 GPUs.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-3.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Training time of BERT-large. starting from the same initialization provided by the MLPerf benchmark, to reach the target accuracy of 72.0% on masked language modeling. Averaged over 10 runs on 8×A100 GPUs.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;gpt-2&#34;&gt;GPT-2&lt;/h4&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-4.png&#34; data-caption=&#34;GPT-2 small and medium using FlashAttention achieve up to 3× speed up compared to Huggingface implementation and up to 1.7× compared to Megatron-LM. Training time reported on 8×A100s GPUs.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-4.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    GPT-2 small and medium using FlashAttention achieve up to 3× speed up compared to Huggingface implementation and up to 1.7× compared to Megatron-LM. Training time reported on 8×A100s GPUs.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;long-range-arena&#34;&gt;Long-range Arena&lt;/h4&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-5.png&#34; data-caption=&#34;The performance of standard attention, FlashAttention, block-sparse FlashAttention, and approximate attention baselines on the Long-Range-Arena benchmarks. Each task has a different sequence length varying between 1024 and 4096.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-5.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    The performance of standard attention, FlashAttention, block-sparse FlashAttention, and approximate attention baselines on the Long-Range-Arena benchmarks. Each task has a different sequence length varying between 1024 and 4096.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Block-sparse FlashAttention is faster than all of the approximate attention methods that were tested.&lt;/p&gt;
&lt;h3 id=&#34;model-quality&#34;&gt;Model Quality&lt;/h3&gt;
&lt;h4 id=&#34;language-modeling-with-long-context&#34;&gt;Language Modeling with Long Context&lt;/h4&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-6.png&#34; data-caption=&#34;GPT-2 small with FlashAttention, with 4× larger context length compared to Megatron-LM, is still 30% faster while achieving 0.7 better perplexity. Training time on 8×A100 GPUs is reported.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-6.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    GPT-2 small with FlashAttention, with 4× larger context length compared to Megatron-LM, is still 30% faster while achieving 0.7 better perplexity. Training time on 8×A100 GPUs is reported.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;long-document-classification&#34;&gt;Long Document Classification&lt;/h4&gt;
&lt;p&gt;Since FlashAttention allows training on longer sequences, it improves performance on such datasets. MIMIC-III contains intensive care unit patient discharge summaries, each annotated with multiple labels. ECtHR contains legal cases from the European Court of Human Rights, each of which is mapped to articles of the Convention of Human Rights
that were allegedly violated. Both of these datasets contain very long text documents. The average number of tokens in MIMIC-III is 2395 tokens and the longest document contains 14562 tokens.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-7.png&#34; data-caption=&#34;Sequence length 16K outperforms length 512 by 4.3 points on MIMIC, and that length 8K outperforms length 512 by 8.5 points on ECtHR. The discrepancies may be due to subtle distribution shifts: MIMIC-III contains specialized medical text and thus may be more susceptible to a distribution shift in the document length, whereas ECtHR contains general language.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-7.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Sequence length 16K outperforms length 512 by 4.3 points on MIMIC, and that length 8K outperforms length 512 by 8.5 points on ECtHR. The discrepancies may be due to subtle distribution shifts: MIMIC-III contains specialized medical text and thus may be more susceptible to a distribution shift in the document length, whereas ECtHR contains general language.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;path-x-and-path-256&#34;&gt;Path-X and Path-256&lt;/h4&gt;
&lt;p&gt;These are challenging tasks from the long range arena benchmark where the task is to classify whether two points in a black and white 128×128 (or 256×256) image have a path connecting them, and the images are fed to the transformer one pixel at a time. No transformer model in the past has been able to model these tasks effectively. They have either ran out of memory or achieved random performance. FlashAttention yields the first Transformer that can achieve better-than-random performance on the challenging Path-X task (sequence length 16K), and block-sparse FlashAttention yields the first sequence model that can achieve better-than-random performance on Path-256 (sequence length 64K).&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-8.png&#34; data-caption=&#34;First Transformer model that can achieve non-random performance on Path-X and Path-256. Path-256 requires longer sequences but has relatively shorter paths than Path-X, so it is easier to obtain a higher accuracy.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-8.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    First Transformer model that can achieve non-random performance on Path-X and Path-256. Path-256 requires longer sequences but has relatively shorter paths than Path-X, so it is easier to obtain a higher accuracy.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;benchmarking-attention&#34;&gt;Benchmarking Attention&lt;/h3&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-9.png&#34; data-caption=&#34;Left - runtime of forward pass &amp;#43; backward pass. Right - attention memory usage&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2023-03-26_flash-attention/images/res-9.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    &lt;strong&gt;Left&lt;/strong&gt; - runtime of forward pass + backward pass. &lt;strong&gt;Right&lt;/strong&gt; - attention memory usage
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h4 id=&#34;runtime&#34;&gt;Runtime&lt;/h4&gt;
&lt;p&gt;FlashAttention beats all exact attention baselines and is about 3× faster than the PyTorch implementation. The runtimes of many approximate/sparse attention mechanisms grow linearly with sequence length, but FlashAttention still runs faster than approximate and sparse attention for short sequences due to fewer memory accesses. The approximate attention runtimes begin to cross over with FlashAttention at sequences between 512 and 1024. On the other hand, block-sparse FlashAttention is faster than all implementations of exact, sparse, and approximate attention that are available, across all sequence lengths.&lt;/p&gt;
&lt;h4 id=&#34;memory-footprint&#34;&gt;Memory Footprint&lt;/h4&gt;
&lt;p&gt;FlashAttention and block-sparse FlashAttention have the same memory footprint, which grows linearly with sequence length. FlashAttention is up to 20× more memory efficient than exact attention baselines, and is more memory-efficient than the approximate attention baselines.  All other algorithms except for Linformer run out of memory on an A100 GPU before 64K, and FlashAttention is still 2× more efficient than Linformer.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;A great paper overall, tremendous impact and personally, I had loads to learn from it!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Academic Log | October-December 2022</title>
      <link>https://shreyansh26.github.io/post/2023-01-03-academic_log_october_december_22/</link>
      <pubDate>Tue, 03 Jan 2023 22:16:12 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2023-01-03-academic_log_october_december_22/</guid>
      <description>&lt;p&gt;A collection of academic papers/blogs/talks/projects that I read/watched/explored during the month. I also include any small (or large) personal projects that I did and any such related ML/non-ML work.&lt;/p&gt;
&lt;h2 id=&#34;personal-projects&#34;&gt;Personal Projects&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Paper re-implementation&lt;/strong&gt; - Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability by Cohen et al., 2021 - &lt;a href=&#34;https://github.com/shreyansh26/Gradient-Descent-on-Neural-Networks-Typically-Occurs-at-the-Edge-of-Stability&#34;&gt;[Github]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Paper re-implementation&lt;/strong&gt; - The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks by Frankle et al., 2018 - &lt;a href=&#34;https://github.com/shreyansh26/Lottery-Ticket-Hypothesis&#34;&gt;[Github]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Paper re-implementation&lt;/strong&gt; -An Empirical Model of Large-Batch Training by OpenAI, 2018 - &lt;a href=&#34;https://github.com/shreyansh26/An-Empirical-Model-of-Large-Batch-Training&#34;&gt;[Github]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;annotated-papers&#34;&gt;Annotated Papers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/ML%20Theory/The%20Lottery%20Ticket%20Hypothesis%20-%20Finding%20Sparse%2C%20Trainable%20Neural%20Networks%20.pdf&#34;&gt;The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/ML%20Theory/Gradient%20Descent%20on%20Neural%20Networks%20Typically%20Occurs%20at%20the%20Edge%20of%20Stability.pdf&#34;&gt;Gradient Descent on Neural Networks Typically Occurs at the Edge of Stability&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/General-DL/Modeling%20Language%20Usage%20and%20Listener%20Engagement%20in%20Podcasts.pdf&#34;&gt;Modeling Language Usage and Listener Engagement in Podcasts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/ML%20Theory/Which%20Algorithmic%20Choices%20Matter%20at%20Which%20Batch%20Sizes_%20Insights%20From%20a%20Noisy%20Quadratic%20Model.pdf&#34;&gt;Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/ML%20Theory/An%20Empirical%20Model%20of%20Large-Batch%20Training.pdf&#34;&gt;An Empirical Model of Large-Batch Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/ML%20Theory/An%20Empirical%20Model%20of%20Large-Batch%20Training.pdf&#34;&gt;Fine-Tuning Language Models from Human Preferences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/LLMs/RLHF/Training%20language%20models%20to%20follow%20instructions%20with%20human%20feedback.pdf&#34;&gt;Training language models to follow instructions with human feedback&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/ML%20Theory/Adam%20-%20A%20Method%20for%20Stochastic%20Optimization.pdf&#34;&gt;Adam: A Method for Stochastic Optimization&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/RecSys/Monolith%20-%20Real%20Time%20Recommendation%20System%20With%20Collisionless%20Embedding%20Table.pdf&#34;&gt;Monolith: Real Time Recommendation System With Collisionless Embedding Table&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/ML%20Theory/Limitations%20of%20the%20NTK%20for%20Understanding%20Generalization%20in%20Deep%20Learning.pdf&#34;&gt;Limitations of the NTK for Understanding Generalization in Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/ML%20Theory/What%20can%20linearized%20neural%20networks%20actually%20say%20about%20generalization.pdf&#34;&gt;What can linearized neural networks actually say about generalization?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/LLMs/GLM-130B%20-%20An%20Open%20Bilingual%20Pre-trained%20Model.pdf&#34;&gt;GLM-130B: An Open Bilingual Pre-trained Model&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/RecSys/Augmenting%20Netflix%20Search%20with%20In-Session%20Adapted%20Recommendations.pdf&#34;&gt;Augmenting Netflix Search with In-Session Adapted Recommendations&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/RecSys/Adversary%20or%20Friend%20-%20An%20adversarial%20Approach%20to%20Improving%20Recommender%20Systems.pdf&#34;&gt;Adversary or Friend? An adversarial Approach to Improving Recommender Systems&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;papers-i-read-in-addition-to-above&#34;&gt;Papers I read (in addition to above)&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.notion.so/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1&#34;&gt;How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.10877&#34;&gt;Artificial Interrogation for Attributing Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2107.07075&#34;&gt;Deep Learning on a Data Diet: Finding Important Examples Early in Training&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.05610&#34;&gt;BERT on a Data Diet: Finding Important Examples by Gradient-Based Pruning&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2110.13048&#34;&gt;Nonuniform Negative Sampling and Log Odds Correction with Rare Events Data&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.03128&#34;&gt;Confidence-Ranked Reconstruction of Census Microdata from Published Statistics&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2108.05857&#34;&gt;How Optimal is Greedy Decoding for Extractive Question Answering?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.12574&#34;&gt;The Curious Case of Absolute Position Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.11413v1&#34;&gt;Finding the smallest or largest element of a tensor from its low-rank factors&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2201.08860&#34;&gt;GreaseLM: Graph REASoning Enhanced Language Models for Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.06378&#34;&gt;QA-GNN: Reasoning with Language Models and Knowledge Graphs for Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.04610&#34;&gt;Red-Teaming the Stable Diffusion Safety Filter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1812.00417&#34;&gt;Snorkel DryBell: A Case Study in Deploying Weak Supervision at Industrial Scale&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;blogs-i-read&#34;&gt;Blogs I read&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/@AnalyticsAtMeta/notifications-why-less-is-more-how-facebook-has-been-increasing-both-user-satisfaction-and-app-9463f7325e7d&#34;&gt;Notifications: why less is more — how Facebook has been increasing both user satisfaction and app usage by sending only a few notifications | by Analytics at Meta | Dec, 2022 | Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://fennel.ai/blog/feature-engineering-for-personalized-search/&#34;&gt;Feature Engineering for Personalized Search (fennel.ai)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=BYxzrFyES6I&amp;amp;ab_channel=TheEconomist&#34;&gt;Are brain implants the future of computing? - YouTube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=_bDXXWQxK38&#34;&gt;A New Way to Achieve Nuclear Fusion: Helion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.ft.com/content/741772c0-ee76-4d3d-bfcd-4fabc1fb405d&#34;&gt;The secret lives of MI6’s top female spies&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/jfG6vdJZCwTQmG7kb/re-examining-layernorm&#34;&gt;Re-examining LayerNorm&lt;/a&gt; + &lt;a href=&#34;https://colab.research.google.com/drive/1S39-w4vzX3VzZx_27X_BtrLs442pOJnJ?usp=sharing&#34;&gt;Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/t9svvNPNmFf5Qa3TA/mysteries-of-mode-collapse-due-to-rlhf&#34;&gt;Mysteries of Mode Collapse due to RLHF&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/Couhhp4pPHbbhJ2Mg/will-we-run-out-of-ml-data-evidence-from-projecting-dataset&#34;&gt;Will we run out of ML data evidence from projecting dataset&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://huggingface.co/blog/introducing-csearch&#34;&gt;Generating Human-level Text with Contrastive Search in Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://vickiboykis.com/2022/11/10/how-i-learn-machine-learning/&#34;&gt;How I learn machine learning | ★❤✰ Vicki Boykis ★❤✰&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=r7UfYlFj2xw&amp;amp;ab_channel=InnovationEndeavors&#34;&gt;Emerging Research &amp;amp; Applications of Large Language Models (w/ Google Brain, Replit, &amp;amp; HuggingFace) - YouTube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/srush/GPU-Puzzles&#34;&gt;https://github.com/srush/GPU-Puzzles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/pinterest-engineering/how-pinterest-leverages-realtime-user-actions-in-recommendation-to-boost-homefeed-engagement-volume-165ae2e8cde8&#34;&gt;How Pinterest Leverages Realtime User Actions in Recommendation to Boost Homefeed Engagement Volume | by Pinterest Engineering | Pinterest Engineering Blog | Nov, 2022 | Medium&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=yo4QvDn-vsU&amp;amp;ab_channel=NeelNanda&#34;&gt;(1) Real-Time Research Recording: Can a Transformer Re-Derive Positional Info? - YouTube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/QzpKq92nXqp8NHM34/neural-tangent-kernel-distillation&#34;&gt;Neural Tangent Kernel Distillation - LessWrong&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://twitter.com/__nmca__/status/1588575691284807682?s=20&amp;amp;t=Yea0IQkI3v8VjiEAx1l8ow&#34;&gt;Generating Human-level Text with Contrastive Search in Transformers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=SV87S38M1J4&amp;amp;ab_channel=TheInsideView&#34;&gt;Ethan Caballero–Broken Neural Scaling Laws - YouTube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://engineering.fb.com/2022/11/04/video-engineering/instagram-video-processing-encoding-reduction/&#34;&gt;Reducing Instagram’s basic video compute time by 94 percent (fb.com)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=yo4QvDn-vsU&amp;amp;feature=youtu.be&amp;amp;ab_channel=NeelNanda&#34;&gt;Real-Time Research Recording: Can a Transformer Re-Derive Positional Info? - YouTube&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;NTK
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://lilianweng.github.io/posts/2022-09-08-ntk/&#34;&gt;Some Math behind Neural Tangent Kernel | Lil&amp;rsquo;Log (lilianweng.github.io)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://rajatvd.github.io/NTK/&#34;&gt;Understanding the Neural Tangent Kernel – Rajat&amp;rsquo;s Blog – A blog about machine learning and math. (rajatvd.github.io)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Gaussian Processes
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://distill.pub/2019/visual-exploration-gaussian-processes/&#34;&gt;A Visual Exploration of Gaussian Processes&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.dominodatalab.com/blog/fitting-gaussian-process-models-python&#34;&gt;Fitting Gaussian Process Models in Python&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;courses&#34;&gt;Courses&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Revisited some of the &lt;a href=&#34;https://youtube.com/playlist?list=PL8PYTP1V4I8D0UkqW2fEhgLrnlDW9QK7z&#34;&gt;lectures of Advanced NLP (Fall&amp;rsquo;22)&lt;/a&gt; having completed the Fall&amp;rsquo;21 set of lectures in early 2022.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Academic Log | August/September 2022</title>
      <link>https://shreyansh26.github.io/post/2022-10-13-academic_log_august_septemeber_22/</link>
      <pubDate>Thu, 13 Oct 2022 13:11:04 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2022-10-13-academic_log_august_septemeber_22/</guid>
      <description>&lt;p&gt;A collection of academic papers/blogs/talks/projects that I read/watched/explored during the month. I also include any small (or large) personal projects that I did and any such related ML/non-ML work.&lt;/p&gt;
&lt;h2 id=&#34;personal-projects&#34;&gt;Personal Projects&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;VAE-Implementation&lt;/strong&gt; - A simple implementation of Autoencoder and Variational Autoencoder - &lt;a href=&#34;https://github.com/shreyansh26/VAE-Implementation&#34;&gt;[Github]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;MinHash-Implemenation&lt;/strong&gt; - A simple MinHash implementation based on the explanation in the Mining of Massive Datasets course by Stanford - &lt;a href=&#34;https://github.com/shreyansh26/MinHash-Implemenation&#34;&gt;[Github]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Paper re-implementation&lt;/strong&gt; - Sentence VAE paper, &amp;ldquo;Generating Sentences from a Continuous Space&amp;rdquo; by Bowman et al., 2016 - &lt;a href=&#34;https://github.com/shreyansh26/Sentence-VAE&#34;&gt;[Github]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Protohackers&lt;/strong&gt; - Started the Protohackers set of challenges to create servers for network protocols &lt;a href=&#34;https://protohackers.com&#34;&gt;[Website]&lt;/a&gt; &lt;a href=&#34;https://github.com/shreyansh26/Protohackers-Solutions&#34;&gt;[Solutions]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;annotated-papers&#34;&gt;Annotated Papers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/General-DL/Large-Scale%20High-Precision%20Topic%20Modeling%20on%20Twitter.pdf&#34;&gt;Large-Scale High-Precision Topic Modeling on Twitter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/Trustworthy%20ML/Neural%20Trojans/BadNets%20-%20Identifying%20Vulnerabilities%20in%20the%20Machine%20Learning%20Model%20Supply%20Chain.pdf&#34;&gt;BadNets - Identifying Vulnerabilities in the Machine Learning Model Supply Chain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/Trustworthy%20ML/Neural%20Trojans/Detecting%20AI%20Trojans%20Using%20Meta%20Neural%20Analysis.pdf&#34;&gt;Detecting AI Trojans Using Meta Neural Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/Trustworthy%20ML/Neural%20Trojans/Trojaning%20Attack%20on%20Neural%20Networks.pdf&#34;&gt;Trojaning Attack on Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;papers-i-read&#34;&gt;Papers I read&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://dl.acm.org/doi/pdf/10.1145/3523227.3547394&#34;&gt;Rethinking personalized ranking at Pinterest&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.05310&#34;&gt;On the Factory Floor: ML Engineering for Industrial-Scale Ads Recommendation Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.07137&#34;&gt;Prioritized Training on Points that are Learnable, Worth Learning, and Not Yet Learnt&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://cobweb.cs.uga.edu/~squinn/mmd_s15/papers/p1907-yang.pdf&#34;&gt;Large-Scale High-Precision Topic Modeling on Twitter&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1910.03137&#34;&gt;Detecting AI Trojans Using Meta Neural Analysis&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.lib.purdue.edu/cgi/viewcontent.cgi?article=2782&amp;amp;context=cstech&#34;&gt;Trojaning Attack on Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/1708.06733&#34;&gt;BadNets: Identifying Vulnerabilities in the Machine Learning Model Supply Chain&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://people.cs.uchicago.edu/~ravenben/publications/pdf/backdoor-sp19.pdf&#34;&gt;Neural Cleanse: Identifying and Mitigating Backdoor Attacks in Neural Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;blogs-i-read&#34;&gt;Blogs I read&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://aslvrstn.com/posts/transformer_precision_loss/&#34;&gt;Transformer Precision Loss&lt;/a&gt; + &lt;a href=&#34;https://twitter.com/NeelNanda5/status/1570217238799720454&#34;&gt;Neel Nanda&amp;rsquo; Twitter thread on the same&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/kyvCNgx9oAwJCuevo/deep-q-networks-explained&#34;&gt;Deep Q-Networks Explained - LessWrong&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/qLqyPMfc8epav72JF/learning-how-to-learn&#34;&gt;Learning How to Learn - LessWrong&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.neelnanda.io/blog/43-making-friends&#34;&gt;Making Friends - Neel Nanda&amp;rsquo;s Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.neelnanda.io/blog/47-inside-views&#34;&gt;Inside Views - Neel Nanda&amp;rsquo;s Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.neelnanda.io/blog/34-learning&#34;&gt;Learning - Neel Nanda&amp;rsquo;s Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://mccormickml.com/2015/06/12/minhash-tutorial-with-python-code/&#34;&gt;MinHash Tutorial&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/7cHgjJR2H5e4w4rxT/alignment-papers-roundup-week-1&#34;&gt;Alignment Papers Roundup (week 1) - LessWrong&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kipp.ly/blog/jits-intro/&#34;&gt;Intro to JIT - Kipply&amp;rsquo;s Blog&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.lesswrong.com/posts/6Fpvch8RR29qLEWNH/chinchilla-s-wild-implications&#34;&gt;Chinchilla&amp;rsquo;s Wild Implications - LessWrong&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ppwwyyxx.com/blog/2022/TorchScript-Tracing-vs-Scripting/&#34;&gt;TorchScript - Tracing vs Scripting&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://paulbridger.com/posts/mastering-torchscript/&#34;&gt;Mastering TorchScript&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/advanced/cpp_export.html&#34;&gt;Loading a TorchScript Model in C++&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://shough.me/neural-trojans/&#34;&gt;Neural Trojans&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;podcasts-i-listened-to&#34;&gt;Podcasts I listened to&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://open.spotify.com/episode/3fpxxuxexS4redIfAbwHEE?si=548c6f88b1374f9a&#34;&gt;Engineering an ML-Powered Developer First Search Engine with Richard Socher&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://open.spotify.com/episode/7C4Zoy90rGq8VvRVglk9Dj?si=57c21856617544cb&#34;&gt;Spotify&amp;rsquo;s Gustav Söderström on machine learning to personalize user experiences&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://open.spotify.com/episode/0pphUkLfK6A5ZynmsaznRm?si=78b7c40abdfb4b5e&#34;&gt;George Netscher of SafelyYou on the role of AI for fall detection&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://open.spotify.com/episode/5X67hz2ssR2FcZ6jxh7jMP?si=af12a52c7c7d4e7a&#34;&gt;Andrew Song of Whisper AI on solving hearing loss with AI&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://open.spotify.com/episode/3gFoeQ7hXLQ3xOmFDLKsPo?si=e2a49a7956ec4d17&#34;&gt;How Wayve is teaching cars to drive&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://open.spotify.com/episode/1m7V84DdplFczUD7mLgRdW?si=c7e52c1d3dbf4266&#34;&gt;David Rolnick on how machine learning can help tackle climate change&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://open.spotify.com/episode/3sr5TEzdhXT0pq3uk84U4Y?si=1249169b3ddc4948&#34;&gt;Mike Fisher of Etsy talks AI and E-commerce&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://open.spotify.com/episode/4tj1krRWAiw7SSrB43PDcj?si=800b1dfa6ebf4589&#34;&gt;It&amp;rsquo;s All About the Data - NerdOut@Spotify&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;talksvideos-i-watched&#34;&gt;Talks/Videos I watched&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://youtu.be/ABmRCdnVq3E&#34;&gt;How We&amp;rsquo;re Reverse Engineering the Human Brain in the Lab | Sergiu P. Pasca | TED&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=W7O1xLlsSVs&amp;amp;ab_channel=ScaleAI&#34;&gt;Creating Personalized Listening Experiences with Spotify&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=spUNpyF58BY&amp;amp;ab_channel=3Blue1Brown&#34;&gt;But what is the Fourier Transform? A visual introduction&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=cUqoHQDinCM&amp;amp;t=0s&amp;amp;ab_channel=Mathemaniac&#34;&gt;The weirdest paradox in statistics (and machine learning)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=nra0Tt3a-Oc&#34;&gt;Continual Learning - Full Stack Deep Learning&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summary #7 - Efficient Transformers: A Survey</title>
      <link>https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/</link>
      <pubDate>Mon, 10 Oct 2022 14:57:33 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: Efficient Transformers: A Survey&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2009.06732&#34;&gt;https://arxiv.org/abs/2009.06732&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yi Tay, Mostafa Dehghani, Dara Bahri, Donald Metzler&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I wanted to summarize this paper for a long time now because of the immense amount of information in this paper. Thanks to the &lt;a href=&#34;https://cohere.for.ai/&#34;&gt;Cohere For AI&lt;/a&gt; community for having a session on this paper which made me revisit this.&lt;/p&gt;
&lt;h1 id=&#34;what&#34;&gt;What?&lt;/h1&gt;
&lt;p&gt;This is a survey paper on the various memory-efficiency based improvements on the original Transformers architecture by Vaswani et al. But wait, for those unaware, how is the Transformers architecture inefficient?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The attention operation has a quadratic complexity over the sequence length L, also sometimes represented using N (since each token attends to other set of tokens in the sequence)&lt;/li&gt;
&lt;li&gt;The Attention operation of Q*K&lt;sup&gt;T&lt;/sup&gt; uses N&lt;sup&gt;2&lt;/sup&gt; time and memory. Here (in no-batching case) Q, K, V (query, key and value matrices) have dimensions &lt;i&gt;N x d &lt;/i&gt; where d is the dimension of query, key and value vectors.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/attention.PNG&#34; data-caption=&#34;Attention calculation&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/attention.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Attention calculation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h1 id=&#34;glossary&#34;&gt;Glossary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#low-rank-methods&#34;&gt;Low-rank Methods&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#linformer---httpsarxivorgabs200604768httpsarxivorgabs200604768&#34;&gt;Linformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#performer---httpsarxivorgabs200914794httpsarxivorgabs200914794&#34;&gt;Perfomer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#learnable-patterns&#34;&gt;Learnable Patterns based methods&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#clustered-attention---httpsarxivorgabs200704825httpsarxivorgabs200704825--httpsclustered-transformersgithubiobloghttpsclustered-transformersgithubioblog&#34;&gt;Clustered Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reformer---httpsarxivorgabs200104451httpsarxivorgabs200104451&#34;&gt;Reformer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#memory-based&#34;&gt;Memory-based methods&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#big-bird-httpsarxivorgabs200714062httpsarxivorgabs200714062--httpshuggingfacecoblogbig-birdhttpshuggingfacecoblogbig-bird&#34;&gt;Big Bird&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#complexity-summary-of-various-models&#34;&gt;Complexity summary of various models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;memory-efficient-transformers&#34;&gt;Memory-Efficient Transformers&lt;/h1&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/arch-vaswani.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/arch-vaswani.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/summary.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/summary.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;low-rank-methods&#34;&gt;Low-Rank methods&lt;/h2&gt;
&lt;h3 id=&#34;linformer---httpsarxivorgabs200604768httpsarxivorgabs200604768&#34;&gt;Linformer - &lt;a href=&#34;https://arxiv.org/abs/2006.04768&#34;&gt;https://arxiv.org/abs/2006.04768&lt;/a&gt;&lt;/h3&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/linformer.png&#34; data-caption=&#34;Left and bottom-right show architecture and example of our proposed multihead linear self-attention. Top right shows inference time vs. sequence length for various Linformer models.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/linformer.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Left and bottom-right show architecture and example of our proposed multihead linear self-attention. Top right shows inference time vs. sequence length for various Linformer models.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In Linformer, the original Key and Value matrices are projected from &lt;i&gt;(N x d)&lt;/i&gt; to a reduced &lt;i&gt;(k x d)&lt;/i&gt;.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/linformer-dets.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/linformer-dets.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The above operations only require &lt;code&gt;O(n*k)&lt;/code&gt; time and space complexity. Thus, if we can choose a very small projected dimension k, such that k &amp;lt; &amp;lt; N, then we can significantly reduce the memory and space consumption.&lt;/p&gt;
&lt;h3 id=&#34;performer---httpsarxivorgabs200914794httpsarxivorgabs200914794&#34;&gt;Performer - &lt;a href=&#34;https://arxiv.org/abs/2009.14794&#34;&gt;https://arxiv.org/abs/2009.14794&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The goal in the Performer paper was to reduce the complexity of attention calculation (Q * K&lt;sup&gt;T&lt;/sup&gt;) * V of O(L&lt;sup&gt;2&lt;/sup&gt; * d) to O (L * d&lt;sup&gt;2&lt;/sup&gt;) by transforming the order of operations and using a kernel operation to approximate the softmax operation so that the order of operations can be changed.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/performer.png&#34; data-caption=&#34;An overview from the paper&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/performer.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    An overview from the paper
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;From, a &lt;a href=&#34;https://chiaracampagnola.io/2020/10/29/from-transformers-to-performers/&#34;&gt;great blog on the Performer paper&lt;/a&gt; -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/performer-dets.png&#34; data-caption=&#34;Change of operation order&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/performer-dets.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Change of operation order
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/performer-dets2.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/performer-dets2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/performer-dets3.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/performer-dets3.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;learnable-patterns&#34;&gt;Learnable Patterns&lt;/h2&gt;
&lt;h3 id=&#34;clustered-attention---httpsarxivorgabs200704825httpsarxivorgabs200704825--httpsclustered-transformersgithubiobloghttpsclustered-transformersgithubioblog&#34;&gt;Clustered Attention - &lt;a href=&#34;https://arxiv.org/abs/2007.04825&#34;&gt;https://arxiv.org/abs/2007.04825&lt;/a&gt; + &lt;a href=&#34;https://clustered-transformers.github.io/blog/&#34;&gt;https://clustered-transformers.github.io/blog/&lt;/a&gt;&lt;/h3&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/clusteredatt.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/clusteredatt.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;First cluster the queries into  non-overlapping clusters.&lt;/li&gt;
&lt;li&gt;Attention weights A&lt;sup&gt;c&lt;/sup&gt; are computed using the centroids instead of computing them for every query&lt;/li&gt;
&lt;li&gt;Use clustered attention weights A&lt;sup&gt;c&lt;/sup&gt; to compute new Values V&lt;sup&gt;c&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;Use the same attention weights and new values for queries that belong to same cluster.&lt;/li&gt;
&lt;li&gt;Computational complexity becomes &lt;code&gt;O(N * C * max (D&lt;sub&gt;k&lt;/sub&gt; * D&lt;sub&gt;v&lt;/sub&gt;))&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They also propose an Improved Clustered Attention in their blog. The complexity comaprisons are here -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/clusteredatt-dets.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/clusteredatt-dets.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;reformer---httpsarxivorgabs200104451httpsarxivorgabs200104451&#34;&gt;Reformer - &lt;a href=&#34;https://arxiv.org/abs/2001.04451&#34;&gt;https://arxiv.org/abs/2001.04451&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Uses the concept of Locality sensitive hashing (LSH) attention, where the goal is to not store the entire Q * K&lt;sup&gt;T&lt;/sup&gt; matrix but only the softmax(Q * K&lt;sup&gt;T&lt;/sup&gt;), which is dominated by the largest elements in a typically sparse matrix. For each query q we only need to pay attention to the keys k that are closest to q. For example, if K is of length 64K, for each q we could only consider a small subset of the 32 or 64 closest keys. So the attention mechanism finds the nearest neighbor keys of a query but in an inefficient manner.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/reformer-lsh.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/reformer-lsh.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;Calcluate LSH hashes of Queries and Keys (Q and K)&lt;/li&gt;
&lt;li&gt;Make chunks and compute attention only for vectors in the same bucket&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The paper also introduces the concept of Reversible residual networks (RevNets). In the residual connections in Transformers, one needs to store the activations in each layer in memory in order to calculate gradients during backpropagation. RevNets are composed of a series of reversible blocks. In RevNet, each layer’s activations can be reconstructed exactly from the subsequent layer’s activations, which enables us to perform backpropagation without storing the activations in memory.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/reformer-revnet.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/reformer-revnet.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Reformer applies the RevNet idea to the Transformer by combining the attention and feed-forward layers inside the RevNet block. Now F becomes an attention layer and G becomes the feed-forward layer:&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/reformer-revnet2.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/reformer-revnet2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The reversible residual layers allows storing activations only once during the training process instead of N times.&lt;/p&gt;
&lt;p&gt;The memory complexity of Reformer is &lt;code&gt;O(N log N)&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;memory-based&#34;&gt;Memory-based&lt;/h2&gt;
&lt;h3 id=&#34;big-bird-httpsarxivorgabs200714062httpsarxivorgabs200714062--httpshuggingfacecoblogbig-birdhttpshuggingfacecoblogbig-bird&#34;&gt;Big Bird &lt;a href=&#34;https://arxiv.org/abs/2007.14062&#34;&gt;https://arxiv.org/abs/2007.14062&lt;/a&gt; + &lt;a href=&#34;https://huggingface.co/blog/big-bird&#34;&gt;https://huggingface.co/blog/big-bird&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;BigBird relies on block sparse attention and can handle sequences up to a length of 4096 at a much lower computational cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts.&lt;/p&gt;
&lt;p&gt;BigBird proposes three ways of allowing long-term attention dependencies while staying computationally efficient -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Global attention&lt;/strong&gt; - Introduce some tokens which will attend to every token and which are attended by every token. The authors call this the &amp;lsquo;internal transformer construction (ITC)&amp;rsquo; in which a subset of indices is selected as global tokens. This can be interpreted as a model-memory-based approach.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sliding attention&lt;/strong&gt; - Tokens close to each other, attend together. In BigBird, each query attends to w/2 tokens to the left and w/2 tokens to the right. This corresponds to a fixed pattern (FP) approach.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Random attention&lt;/strong&gt; - Select some tokens randomly which will transfer information by transferring to other tokens which in turn can transfer to other tokens. This may reduce the cost of information travel from one token to other. Each query attends to r random keys. This pattern is fixed&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/bigbird-graph.gif&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/bigbird-graph.gif&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;BigBird block sparse attention is a combination of sliding, global &amp;amp; random connections (total 10 connections) as shown in gif above. While a graph of normal attention (bottom) will have all 15 connections (note: total 6 nodes are present). One can simply think of normal attention as all the tokens attending globally.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/bigbird-full.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/bigbird-full.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The attention calculation in BigBird is slightly complex and I would refer to the &lt;a href=&#34;https://huggingface.co/blog/big-bird#bigbird-block-sparse-attention&#34;&gt;Huggingface blog&lt;/a&gt; for it -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/bigbird-attention-gif.gif&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/bigbird-attention-gif.gif&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;&lt;i&gt;blue -&amp;gt; global blocks, red -&amp;gt; random blocks, orange -&amp;gt; sliding blocks&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;The memory complexity of the self-attention is linear, i.e., &lt;code&gt;O(n)&lt;/code&gt;. The BigBird model does not introduce new parameters beyond the Transformer model.&lt;/p&gt;
&lt;h2 id=&#34;complexity-summary-of-various-models&#34;&gt;Complexity Summary of various models&lt;/h2&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/complexity-summary.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/complexity-summary.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;hr&gt;
&lt;p&gt;There are many more papers discussed in the survey. I will add their summaries here as I go through them.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Academic Log | June/July 2022</title>
      <link>https://shreyansh26.github.io/post/2022-08-04-academic_log_june_july_22/</link>
      <pubDate>Thu, 04 Aug 2022 00:27:33 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2022-08-04-academic_log_june_july_22/</guid>
      <description>&lt;p&gt;A collection of academic papers/blogs/talks/projects that I read/watched/explored during the month. I also include any small (or large) personal projects that I did and any such related ML/non-ML work.&lt;/p&gt;
&lt;h2 id=&#34;personal-projects&#34;&gt;Personal Projects&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Paper re-implementation&lt;/strong&gt; - &amp;ldquo;Extracting Training Data from Large Language Models&amp;rdquo; by Carlini et al., 2021. - &lt;a href=&#34;https://github.com/shreyansh26/Extracting-Training-Data-from-Large-Langauge-Models&#34;&gt;[Github]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;annotated-papers&#34;&gt;Annotated Papers&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/General-DL/Learning%20Backward%20Compatible%20Embeddings.pdf&#34;&gt;Learning Backward Compatible Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/LLMs/Memorization%20Without%20Overfitting%20-%20Analyzing%20the%20Training%20Dynamics%20of%20Large%20Language%20Models.pdf&#34;&gt;Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/LLMs/Tracing%20Knowledge%20in%20Language%20Models%20Back%20to%20the%20Training%20Data.pdf&#34;&gt;Tracing Knowledge in Language Models Back to the Training Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;papers-i-read&#34;&gt;Papers I read&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2111.12128&#34;&gt;On the Unreasonable Effectiveness of Feature propagation in Learning on Graphs with Missing Node Features&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.02311&#34;&gt;PaLM: Scaling Language Modeling with Pathways&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://cdn.openai.com/papers/dall-e-2.pdf&#34;&gt;Hierarchical Text-Conditional Image Generation with CLIP Latents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.00598&#34;&gt;Socratic Models: Composing Zero-Shot Multimodal Reasoning with Language&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.03610v1&#34;&gt;Unified Contrastive Learning in Image-Text-Label Space&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.07496&#34;&gt;Improving Passage Retrieval with Zero-Shot Question Generation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2204.07120&#34;&gt;Exploring Dual Encoder Architectures for Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.01541&#34;&gt;Efficient Fine-Tuning of BERT Models on the Edge&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2112.14569&#34;&gt;Fine-Tuning Transformers: Vocabulary Transfer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2104.09667&#34;&gt;Manipulating SGD with Data Ordering Attacks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openreview.net/forum?id=Q42f0dfjECO&#34;&gt;Differentially Private Fine-tuning of Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2012.07805&#34;&gt;Extracting Training Data from Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.03040&#34;&gt;Learning Backward Compatible Embeddings&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2106.04647&#34;&gt;Compacter: Efficient Low-Rank Hypercomplex Adapter Layers&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2206.13089&#34;&gt;Agreement-on-the-Line: Predicting the Performance of Neural Networks under Distribution Shift&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.10770&#34;&gt;Memorization Without Overfitting: Analyzing the Training Dynamics of Large Language Models&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://arxiv.org/abs/2205.11482&#34;&gt;Tracing Knowledge in Language Models Back to the Training Data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;blogs-i-read&#34;&gt;Blogs I read&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.pinecone.io/learn/gpl/&#34;&gt;Domain Adaptation with Generative Pseudo-Labeling (GPL)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://horace.io/brrr_intro.html&#34;&gt;Making Deep Learning Go Brrrr From First Principles&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html&#34;&gt;Introduction to TorchScript&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://openai.com/blog/nonlinear-computation-in-linear-networks/&#34;&gt;Nonlinear Computation in Deep Linear Networks&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;talks-i-watched&#34;&gt;Talks I watched&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31151/&#34;&gt;How GPU Computing Works&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summary #6 - Language Models are Unsupervised Multitask Learners</title>
      <link>https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/</link>
      <pubDate>Sun, 23 May 2021 16:44:32 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: Language Models are Unsupervised Multitask Learners&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;https://bit.ly/3vgaVJc&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever  &lt;br&gt;
&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;https://github.com/openai/gpt-2&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I also made an annotated version of the paper which you can find &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;The paper demonstrates that language models begin to learn NLP tasks like question answering, machine translation, reading comprehension and summarization without any explicit supervision. The results shown are obtained after training the model on a new dataset of millions of web pages called WebText. GPT-2 is a 1.5 billion parameter model that achieves SOTA on 7 out of 8 LM tasks in a zero-shot setting. The paper proves that it is possible to build NLP systems that learn to perform tasks from the naturally occurring demonstrations of the tasks in text.&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;The main motivation arises from the fact that current systems are narrow experts rather than competent generalists. There should be a shift to more general systems which can perform many tasks without the need to manually create and label a training dataset for each one. There have been examples of erratic behaviour of captioning models, reading comprehension systems and image classifiers due to the large variety of possible inputs which can&amp;rsquo;t be modeled using supervised approaches. There is a lack of generalization in current systems.&lt;/p&gt;
&lt;p&gt;Additionally, multitask NLP systems are still in a very early stage mostly due to the fact that it would require a large amount of very specific labeled data for the model to learn from. Although some models use a combination of unsupervised pretraining followed by supervised fine-tuning. GPT-2 wants to do away with any supervised training and show how language models perform in a zero-shot setting on a wide range of tasks.&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;
&lt;p&gt;Language modeling is usually framed as a unsupervised distribution estimation. It is modeled as a joint probability over the symbols. Due to the sequential order of natural text, this can be written as a product of the conditional probabilities.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/lm.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/lm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Instead of a simple conditional probability distribution for a single task &lt;em&gt;p(output&lt;/em&gt;|&lt;em&gt;input)&lt;/em&gt;. To make a system that can perform multiple tasks, the distribution should be conditioned on the task as well i.e., &lt;em&gt;p(output&lt;/em&gt;|&lt;em&gt;input, task)&lt;/em&gt;. This is usually implemented at the architecture level for example, by using task-specific encoders and decoders. It can also be performed by the language directly.  A translation task can be represented as &lt;tt&gt;(translate to french, english text, french text). &lt;/tt&gt; A reading comprehension can be written as &lt;tt&gt; (answer the question, document, question, answer) &lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;In principle, language modeling will be able to learn these tasks without any supervision of which symbols are to be predicted. Since the global minimum of the unsupervised objective is also the global minimum of the supervised objective (which is based on a subset of the sequence), hence the model can focus only on optimizing for the unsupervised objective. For this, very large models are required, however the learning is slower as compared to the explicitly supervised approaches.&lt;/p&gt;
&lt;p&gt;The authors believe that a large enough language model will begin to learn the tasks embedded within the natural language itself and won&amp;rsquo;t require any additional supervision. For example, given enough text, the model will learn what question answering is, without having to train on question-answering data specifically.&lt;/p&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;A new dataset (self-curated), WebText was used by the authors to train the model. The dataset contained page contents of all the scraped outbound links from Reddit, from posts that received at least 3 karma. They performed HTML cleaning, de-duplication. Also, Wikipedia pages were removed as the test datasets of many of the downstream tasks had information from Wikipedia.&lt;/p&gt;
&lt;h3 id=&#34;input-representation&#34;&gt;Input Representation&lt;/h3&gt;
&lt;p&gt;BytePair encoding, which effectively interpolates between word level inputs for frequent symbol sequences and character level inputs for infrequent symbol sequences was used for tokenizing the corpus. The encoding in the paper was not performed on bytes but Unicode points. This increases the base vocabulary from 256 (in byte mode) to 130,000 (with Unicode).&lt;/p&gt;
&lt;p&gt;The BPE encoding allowed the authors to combine the benefits of word-level LMs with the generality of byte-level approaches. Also, since now the model can assign a probability to any Unicode string, so the LM will be able to be evaluated on any dataset regardless of the pre-processing, tokenization or vocabulary size.&lt;/p&gt;
&lt;hr&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;hr&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;p&gt;The model is a Transformer decoder architecture, very similar to GPT-1. You can find details &lt;a href=&#34;http://localhost:1313/post/2021-05-02_language_understanding_generative_pretraining/#task-specific-input-transformations&#34;&gt;here&lt;/a&gt;. Some modifications include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Moving the layer norm to the input of each sub-block and adding an additional layer norm after the final self-attention block.&lt;/li&gt;
&lt;li&gt;A modified initialization which accounts for the accumulation on the residual path with model depth is also used.&lt;/li&gt;
&lt;li&gt;The weights of the residual layers are scaled by 1/sqrt(&lt;em&gt;N&lt;/em&gt;) where &lt;em&gt;N&lt;/em&gt; is the number of residual layers.&lt;/li&gt;
&lt;li&gt;The vocabulary is expanded to 50,257.&lt;/li&gt;
&lt;li&gt;The context size from 512 to 1024 tokens and a larger batch size of 512 is used.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;h3 id=&#34;language-modeling&#34;&gt;Language Modeling&lt;/h3&gt;
&lt;p&gt;This is the primary task the model was trained for. In this category, the model is evaluated on its perplexity score. Some invertible de-tokenizers had to be used on the test set as not all types of text are seen during training for example, standardized text, having tokenization artifacts like shuffled sentences and &amp;lt;UNK&amp;gt; string. A de-tokenizer that removes those artifacts improves the score by 2.5 - 5 perplexity points.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Large improvements were seen on small datasets like Penn Treebank and WikiText-2.&lt;/li&gt;
&lt;li&gt;Large improvements were also seen on LAMBADA and Children&amp;rsquo;s Book Test where long-term dependency had to be measured.&lt;/li&gt;
&lt;li&gt;It failed to perform better than existing approaches for the One Billion Word Benchmark, probably because of it being the largest dataset and having the most destructive pre-processing - the sentence level shuffling which removes all range structure. this makes it difficult for GPT-2 to perform well on it.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;childrens-book-test-cbt&#34;&gt;Children’s Book Test (CBT)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CBT task was to predict which of 10 possible choices for an omitted word is correct.&lt;/li&gt;
&lt;li&gt;Data overlap analysis showed one of the CBT test set books, The Jungle Book by Rudyard Kipling, is in WebText, so the authors report results on the validation set which has no significant overlap.&lt;/li&gt;
&lt;li&gt;GPT-2 achieved SOTA results on both prediction of common nouns (CBT-CN) and the prediction of named entities (CBT-NE).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;lambada&#34;&gt;LAMBADA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In LAMBADA, the task is to predict the final word of sentences which require at least 50 tokens of context for a human to successfully predict.&lt;/li&gt;
&lt;li&gt;GPT-2 improved the perplexity from 99.8 (existing SOTA) to 8.6 and the accuracy from 19% to 52.66%.&lt;/li&gt;
&lt;li&gt;Choosing a stopping filter was difficult as many times GPT-2 predicted valid continuations of the sentence but not valid final words. A stop-word filter for this helped improve the accuracy a bit.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/reslm.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/reslm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;winograd-schema-challenge&#34;&gt;Winograd Schema Challenge&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Winograd Schema challenge aims to measure the capability of a system to perform commonsense reasoning by measuring its ability to resolve ambiguities in the text.&lt;/li&gt;
&lt;li&gt;GPT-2 improves the state of the art accuracy by 7%, achieving 70.70%.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/reswino.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/reswino.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;reading-comprehension&#34;&gt;Reading Comprehension&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The CoQA dataset consists of documents from different domains with natural dialogues in the form of questions and answers. The task tests the reading comprehension capabilities and also the ability to answer questions based on conversation history.&lt;/li&gt;
&lt;li&gt;GPT-2 was evaluated on this task by conditioning on the document, the conversation history and the final token.&lt;/li&gt;
&lt;li&gt;This matched or exceeded the results from 3 of 4 baselines. Also, these models were trained on the 127,000+ question-answer pairs of the training data, which GPT-2 didn&amp;rsquo;t look at.&lt;/li&gt;
&lt;li&gt;GPT-2 didn&amp;rsquo;t perform as well as the BERT SOTA but the score is still impressive since it is a completely unsupervised model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;summarization&#34;&gt;Summarization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;GPT-2 didn&amp;rsquo;t perform well on the summarization task on the CNN and Daily Mail dataset.&lt;/li&gt;
&lt;li&gt;It just barely outperforms selecting 3 random sentences from the article.&lt;/li&gt;
&lt;li&gt;Removing the hint reduced the score by 6.4 points indicating that task-specific behaviour was being invoked by natural language.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/ressumm.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/ressumm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;translation&#34;&gt;Translation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The model was conditioned in the following manner - &lt;tt&gt; english sentence = french sentence &lt;/tt&gt; and then after the prompt of &lt;tt&gt; english sentence = &lt;/tt&gt;, the greedy decoding and the first generated sentence was used as the translation.&lt;/li&gt;
&lt;li&gt;However, the performance of the model was very poor, even worse than a word-by-word substitution of the words with their translation.&lt;/li&gt;
&lt;li&gt;However, the French to English task was a bit better, surpassing some unsupervised methods but very far from the best unsupervised method.&lt;/li&gt;
&lt;li&gt;The result is interesting as almost all non-English text was removed from WebText during preprocessing. And the best unsupervised method used 500x more French data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;question-answering&#34;&gt;Question Answering&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The context of the language model is seeded with example question-answer pairs which helps the model infer the short answer style of the dataset.&lt;/li&gt;
&lt;li&gt;However the performance is very poor and the model only answers 4.1% of the questions correctly.&lt;/li&gt;
&lt;li&gt;It was seen that the smaller models could answer around 1% of the questions, indicating that a larger model size helped.&lt;/li&gt;
&lt;li&gt;GPT-2 has an accuracy of 63.1% on the 1% of questions it is most confident in.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resqa.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resqa.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;generalization-vs-memorization&#34;&gt;Generalization vs Memorization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In general, it was seen that all the major datasets have some amount of overlap between the train and the test datasets.&lt;/li&gt;
&lt;li&gt;Even CIFAR-10 has a 3.3% overlap of train and test images.&lt;/li&gt;
&lt;li&gt;To test this, Bloom filters containing 8-grams of WebText training set tokens were created.&lt;/li&gt;
&lt;li&gt;These Bloom filters helped to calculate, given a dataset, the percentage of 8-grams from that dataset that are also found in the WebText training set.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resoverlap.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resoverlap.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;The overlap between the datasets&amp;rsquo; train and test set was also very high in some.&lt;/li&gt;
&lt;li&gt;And on analysis, it was seen that removing these overlaps from the train set, resulted in a slight drop in performance across tasks.&lt;/li&gt;
&lt;li&gt;The authors suggest fuzzy string matching or n-gram overlap based de-duplication as important sanity checks when splitting NLP datasets to create the train and test set.&lt;/li&gt;
&lt;li&gt;The performance on both the training and test sets of WebText are similar and improve together as model size is increased&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resown.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resown.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The authors conclude by saying that since finetuning had helped GPT, the same could be tried for GPT-2 as well for benchmarks such as decaNLP and GLUE. Also, it may be helpful because as BERT pointed out, the inefficiency of unidirectional representations can not absolutely be eliminated by more training data and model size, as could be seen in tasks like summarization, translation and question answering.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is all for now!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summary #5 - XLNet: Generalized Autoregressive Pretraining for Language Understanding</title>
      <link>https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/</link>
      <pubDate>Sun, 16 May 2021 14:25:04 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: XLNet: Generalized Autoregressive Pretraining for Language Understanding&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/pdf/1906.08237.pdf&#34;&gt;https://arxiv.org/pdf/1906.08237.pdf&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, Quoc V. Le&lt;br&gt;
&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/zihangdai/xlnet&#34;&gt;https://github.com/zihangdai/xlnet&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;The paper proposes XLNet, a generalized autoregressive pretraining method that enables learning bidirectional contexts over all permutations of the factorization order and overcomes the limitations of BERT due to the autoregressive formulation of XLNet. XLNet incorporates Transformer-XL as the underlying model. It outperforms BERT in 20 NLP tasks like question answering, natural language inference, sentiment analysis and document ranking.&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;The existing unsupervised representation learning approaches can be divided into two types - autoregressive language modeling and autoencoding approaches. The autoregressive methods like ELMo and GPT tried to estimate the probability distribution of a text corpus with an autoregressive model. They had a limitation in that they only captured the unidirectional context. BERT aimed to solve this problem by aiming to reconstruct the original data from the corrupted input. So BERT could capture the bidirectional context, but by converting this into a prediction problem, BERT assumed that the predicted tokens are independent of each other. However, that is not the case in natural language where long term dependency is prevalent. Moreover, the use of the [MASK] tokens also created a pretrain-finetune discrepancy as there are no [MASK] tokens available during finetuning.&lt;/p&gt;
&lt;p&gt;XLNet tries to leverage the best of both worlds. The qualities of XLNet are -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;XLNet computes the maximum likelihood of a sequence w.r.t. all possible permutations of the factorization order. So when calculating the expectation, each position learns to capture the context from all positions, hence capturing bidirectional context.&lt;/li&gt;
&lt;li&gt;XLNet does not rely on data corruption as in BERT and hence does not suffer from the pretrain-finetune discrepancy.&lt;/li&gt;
&lt;li&gt;XLNet integrates the novelties from Transformer-XL like recurrence mechanism and relative encoding scheme (explained later as well). This improves the performance of tasks that utilise a longer text sequence.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;
&lt;p&gt;Autoregressive language modeling performs pretraining by maximizing the likelihood under the forward autoregressive factorization -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/arobjective.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/arobjective.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here &lt;em&gt;x&lt;/em&gt; is the given text sequence. h&lt;sub&gt;Θ&lt;/sub&gt;(x&lt;sub&gt;1:t-1&lt;/sub&gt;) is the context representation produced by the model and &lt;em&gt;e&lt;/em&gt;(x) is the embedding of &lt;em&gt;x&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Denoising autoencoding approach like BERT first constructs a corrupt version &lt;em&gt;x&lt;/em&gt;(cap) by randomly masking a fraction (15%) of tokens of &lt;em&gt;x&lt;/em&gt; to a special symbol [MASK]. The masked tokens are denoted by &lt;em&gt;x&lt;/em&gt;(bar). So, the training objective in the case of BERT becomes -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/aeobjective.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/aeobjective.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here &lt;em&gt;m&lt;/em&gt;&lt;sub&gt;t&lt;/sub&gt; is 1 when &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;t&lt;/sub&gt; is masked. Here H&lt;sub&gt;Θ&lt;/sub&gt; is a Transformer that maps each token to a sequence of length &lt;em&gt;T&lt;/em&gt; to hidden vectors [H&lt;sub&gt;Θ&lt;/sub&gt;(x)&lt;sub&gt;1&lt;/sub&gt;, H&lt;sub&gt;Θ&lt;/sub&gt;(x)&lt;sub&gt;2&lt;/sub&gt;, &amp;hellip;, H&lt;sub&gt;Θ&lt;/sub&gt;(x)&lt;sub&gt;T&lt;/sub&gt;].&lt;/p&gt;
&lt;p&gt;In BERT, the conditional probability is taken when the input is masked, denoted using &lt;em&gt;m&lt;/em&gt;&lt;sub&gt;t&lt;/sub&gt;. Hence denoting the independence assumption among the targets.&lt;/p&gt;
&lt;h3 id=&#34;objective-permutation-language-modeling&#34;&gt;Objective: Permutation Language Modeling&lt;/h3&gt;
&lt;p&gt;Both autoregressive and autoencoding approaches have their benefits over each other. XLNet tries to bring both their advantages into the picture while avoiding their weaknesses.&lt;/p&gt;
&lt;p&gt;XLNet proposes the use of permutation language modeling objective that looks like the general autoregressive language modeling approach but it allows the model to capture bidirectional context as well. Here, the training is performed for each valid autoregressive factorization order (permutations) of the sequence. The model parameters are shared across all the factorization orders, and hence the model learns to capture information from all positions on both sides.&lt;/p&gt;
&lt;p&gt;The proposed permutation language modeling approach is -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/xlnetobjective.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/xlnetobjective.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here &lt;em&gt;Z&lt;/em&gt;&lt;sub&gt;T&lt;/sub&gt; is the set of all possible permutations of the length-&lt;em&gt;T&lt;/em&gt; index sequence [1, 2, 3&amp;hellip;, T]. &lt;em&gt;z&lt;/em&gt;&lt;sub&gt;t&lt;/sub&gt; and z&lt;sub&gt;&amp;lt;t&lt;/sub&gt; denote the t-th element and the first &lt;em&gt;t-1&lt;/em&gt; elements of the permutation. So, basically the autoregressive formulation is applied for each factorization order z.&lt;/p&gt;
&lt;p&gt;Since this is based on the autoregressive framework, the independence assumption of BERT is no longer present in this case and the pretrain-finetune discrepancy is also not present.&lt;/p&gt;
&lt;p&gt;* One must note that here the objective does not permute the sequence order. The sequence order remains as it is and the positional encodings correspond to the original sequence itself. Here, the attention mask in Transformers is used to achieve the permutation of the factorization order. This is done because permuting the sequence itself can cause problems as during finetuning, the natural order will always be preserved. The authors do not want to include any other pretrain-finetune discrepancy.&lt;/p&gt;
&lt;h3 id=&#34;architecture-two-stream-self-attention-for-target-aware-representations&#34;&gt;Architecture: Two-Stream Self-Attention for Target-Aware Representations&lt;/h3&gt;
&lt;p&gt;Using the Transformer(-XL) directly with the permutation language modeling objective will not work. This is because, say we have two sequences, in which z&lt;sub&gt;&amp;lt;t&lt;/sub&gt; sequence is same but the z&lt;sub&gt;t&lt;/sub&gt; token is different. And in the current formulation using transformers(-XL) the z&lt;sub&gt;&amp;lt;t&lt;/sub&gt; sequence determines z&lt;sub&gt;t&lt;/sub&gt; but that would not be correct if we predict the same distribution for both the sequences while they have two different tokens as the target.&lt;/p&gt;
&lt;p&gt;To solve this, a re-parameterization of the next-token distribution with the target-position (z&lt;sub&gt;t&lt;/sub&gt;) is required.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/rexlnetobjective.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/rexlnetobjective.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here, g&lt;sub&gt;Θ&lt;/sub&gt;(x&lt;sub&gt;z&lt;sub&gt;&amp;lt;t&lt;/sub&gt;&lt;/sub&gt;, &lt;i&gt;z&lt;/i&gt;&lt;sub&gt;&lt;/sub&gt;) is a new type of representation that takes in the z&lt;sub&gt;t&lt;/sub&gt; as input as well.&lt;/p&gt;
&lt;h4 id=&#34;two-stream-self-attention&#34;&gt;Two-Stream Self-Attention&lt;/h4&gt;
&lt;p&gt;Now, there is a contradiction here. If we want to predict x&lt;sub&gt;z&lt;sub&gt;t&lt;/sub&gt;&lt;/sub&gt;, g&lt;sub&gt;Θ&lt;/sub&gt;(x&lt;sub&gt;z&lt;sub&gt;&amp;lt;t&lt;/sub&gt;&lt;/sub&gt;, &lt;i&gt;z&lt;/i&gt;&lt;sub&gt;t&lt;/sub&gt;) should only use position z&lt;sub&gt;t&lt;/sub&gt; and not x&lt;sub&gt;z&lt;sub&gt;t&lt;/sub&gt;&lt;/sub&gt; itself. Also, to predict the future tokens x&lt;sub&gt;z&lt;sub&gt;j&lt;/sub&gt;&lt;/sub&gt; with j &amp;gt; t, we need  x&lt;sub&gt;z&lt;sub&gt;t&lt;/sub&gt;&lt;/sub&gt; to provide the full context information.&lt;/p&gt;
&lt;p&gt;So, to resolve this, XLNet uses two hidden representations -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Content representation h&lt;sub&gt;Θ&lt;/sub&gt;(x&lt;sub&gt;z&lt;sub&gt;&amp;lt;=t&lt;/sub&gt;&lt;/sub&gt;) abbreviated as h&lt;sub&gt;z&lt;sub&gt;t&lt;/sub&gt;&lt;/sub&gt;, which is similar to the general Transformer hidden state. This encodes both the context and the token x&lt;sub&gt;z&lt;sub&gt;t&lt;/sub&gt;&lt;/sub&gt;.&lt;/li&gt;
&lt;li&gt;Query representation g&lt;sub&gt;Θ&lt;/sub&gt;(x&lt;sub&gt;z&lt;sub&gt;&amp;lt;t&lt;/sub&gt;&lt;/sub&gt;, &lt;i&gt;z&lt;/i&gt;&lt;sub&gt;t&lt;/sub&gt;) , abbreviated as g&lt;sub&gt;z&lt;sub&gt;t&lt;/sub&gt;&lt;/sub&gt;, which only has access to the contextual information x&lt;sub&gt;z&lt;sub&gt;&amp;lt;t&lt;/sub&gt;&lt;/sub&gt; and the position z&lt;sub&gt;t&lt;/sub&gt; but not the contents x&lt;sub&gt;z&lt;sub&gt;t&lt;/sub&gt;&lt;/sub&gt;.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/arch.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/arch.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The above diagram shows the flow of the two streams. The two streams are updated with a set of shared parameters as follows -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/update.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/update.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The update rule of the content representations is exactly the same as the standard self-attention. During finetuning, the query stream can be dropped and the content stream can be used as a normal Transformer(-XL). And in the end, the last-layer query representation g&lt;sub&gt;z&lt;sub&gt;t&lt;/sub&gt;&lt;/sub&gt; is used to compute the likelihood.&lt;/p&gt;
&lt;hr&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;hr&gt;
&lt;h4 id=&#34;partial-prediction&#34;&gt;Partial Prediction&lt;/h4&gt;
&lt;p&gt;For a given factorization order &lt;em&gt;&lt;strong&gt;z&lt;/strong&gt;&lt;/em&gt;,  cutting point &lt;em&gt;c&lt;/em&gt; is chosen which splits the sequence into two subsequences. &lt;em&gt;&lt;strong&gt;z&lt;/strong&gt;&lt;/em&gt;&lt;sub&gt;&amp;lt;=c&lt;/sub&gt; is the non-target subsequence and &lt;em&gt;&lt;strong&gt;z&lt;/strong&gt;&lt;/em&gt;&lt;sub&gt;&amp;gt;c&lt;/sub&gt; is the target sequence. The objective to maximize the log-likelihood of the target subsequence conditioned on the non-target subsequence is written as -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/partial.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/partial.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;A hyperparameter &lt;em&gt;K&lt;/em&gt; is chosen to determine what fraction of the sequence length will be the target sequence. This is done so that sufficient sequence length is present for the model to learn the context.&lt;/p&gt;
&lt;p&gt;Here again, XLNet differs from BERT. Let us consider an example [New, York, is, a city]. If both BERT and XLNet take two tokens [New, York] as the prediction task and so they have to maximize p(New York | is a city). Here BERT and XLNet get the following objectives -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/bertxlnet.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/bertxlnet.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;XLNet considers the dependency in the target subsequence as well i.e., how &amp;ldquo;York&amp;rdquo; depends on &amp;ldquo;New&amp;rdquo; as well. XLNet always learns more dependency pairs given the same target and contains “denser” effective training signals.&lt;/p&gt;
&lt;h3 id=&#34;ideas-from-transformer-xl&#34;&gt;Ideas from Transformer-XL&lt;/h3&gt;
&lt;p&gt;Transformer-XL introduced the segment recurrence mechanism for caching and reuse of the previous segment knowledge. For a long sequence &lt;strong&gt;s&lt;/strong&gt;, if we take two segments &lt;em&gt;z&lt;/em&gt;(bar) and &lt;em&gt;z&lt;/em&gt; which are permutations of the segment, then the obtained representations from the first segment h(bar)&lt;sup&gt;(m)&lt;/sup&gt; for each layer &lt;em&gt;m&lt;/em&gt; can be cached and reused for the next segment. This can be written as -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/recur.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/recur.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Also since the positional embeddings depend on the actual positions in the original sequence, the above attention update is independent of the previous segment once the hidden representations have been calculated. So the factorization order of the previous segment need not be known. Subsequently, the model learns to utilize the memory over all factorization orders of the last segment. The same is done for the query stream as well.&lt;/p&gt;
&lt;h3 id=&#34;modeling-multiple-segments&#34;&gt;Modeling Multiple Segments&lt;/h3&gt;
&lt;p&gt;Like BERT, XLNet randomly samples two segments (either from the same context or not) and treats the concatenation of two segments as one sequence to perform permutation language modeling.&lt;/p&gt;
&lt;p&gt;XLNET introduces Relative Segment Encodings. Unlike BERT which had absolute segment embeddings that were added to the word embedding at each position, here, rather than giving the entire segment an encoding, relative encoding is used between positions to denote whether they belong to the same segment or not.
The segment encoding of the positions is used to compute the attention weight. So, when position &lt;em&gt;i&lt;/em&gt; attends to &lt;em&gt;j&lt;/em&gt;, the segment encoding s&lt;sub&gt;&lt;i&gt;ij&lt;/i&gt;&lt;/sub&gt; is used to compute an attention weight a&lt;sub&gt;ij&lt;/sub&gt; = (q&lt;sub&gt;i&lt;/sub&gt; + b)&lt;sup&gt;T&lt;/sup&gt;s&lt;sub&gt;ij&lt;/sub&gt; , where q&lt;sub&gt;i&lt;/sub&gt; is the query vector as in a standard attention operation and &lt;em&gt;b&lt;/em&gt; is a learnable head-specific bias vector.&lt;/p&gt;
&lt;p&gt;Relative segment encodings help because -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Inductive bias of the relative encodings improves the generalization.&lt;/li&gt;
&lt;li&gt;Opens up the possibility of finetuning on tasks that have more than two input segments, which is not possible when using absolute segment encodings.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Two datasets were the same as the ones BERT used i.e., BooksCorpus and English Wikipedia. Furthermore, Giga5, ClueWeb 2012-B and CommonCrawl datasets were also used. SentencePiece tokenization was used.&lt;/p&gt;
&lt;p&gt;XLNet had the same architecture hyperparameters as BERT-Base and XLNet-Large had the same hyperparameters as BERT-Large. this resulted in similar model size and hence a fair comparison.&lt;/p&gt;
&lt;p&gt;XLNet was trained on 512 TPU v3 chips for 500K steps with an Adam weight decay
optimizer, linear learning rate decay, and a batch size of 8192, which took about 5.5 days. And even after using so much compute and time, the model still underfitted on the data at the end of the training.&lt;/p&gt;
&lt;p&gt;Since the recurrence mechanism is introduced, XLNet uses a bidirectional data input pipeline where each of the forward and backward directions takes half of the batch size. The idea of span-based prediction, where first, a sample length &lt;em&gt;L&lt;/em&gt; from [1, &amp;hellip;, 5] is chosen and then a consecutive span of &lt;em&gt;L&lt;/em&gt; tokens is randomly selected as prediction targets within a context of (&lt;em&gt;KL&lt;/em&gt;) tokens.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comparisonwithbert.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comparisonwithbert.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;As seen above, trained on the same data with an almost identical training recipe,
XLNet outperforms BERT by a sizable margin on all the considered datasets.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comp1.PNG&#34; data-caption=&#34;Performance on reading comprehension and document ranking tasks. Comparison with GPT, BERT, RoBERTa and a BERT ensemble&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comp1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Performance on reading comprehension and document ranking tasks. Comparison with GPT, BERT, RoBERTa and a BERT ensemble
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comp2.PNG&#34; data-caption=&#34;Performance on question answering tasks - SQuADv1.1 and SQuADv2.0&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comp2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Performance on question answering tasks - SQuADv1.1 and SQuADv2.0
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comp3.PNG&#34; data-caption=&#34;Performance on text classification task.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comp3.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Performance on text classification task.
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comp4.PNG&#34; data-caption=&#34;Performance on natural language understanding tasks - the GLUE benchmark.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/comp4.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Performance on natural language understanding tasks - the GLUE benchmark.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;For explicit reasoning tasks like SQuAD and RACE that involve longer context, the performance gain of XLNet is larger. The use of Transformer-XL could be the main reason behind this.&lt;/li&gt;
&lt;li&gt;For classification tasks that already have abundant supervised examples such as MNLI (&amp;gt;390K), Yelp (&amp;gt;560K) and Amazon (&amp;gt;3M), XLNet still lead to substantial gains.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Ablation study&lt;/strong&gt; was also performed to understand the importance and effect of introducing each component. The points of the study were -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The effectiveness of the permutation language modeling objective alone, especially compared to the denoising auto-encoding objective used by BERT.&lt;/li&gt;
&lt;li&gt;The importance of using Transformer-XL as the backbone neural architecture. For this, a DAE + Transformer-XL model was used.&lt;/li&gt;
&lt;li&gt;The necessity of some implementation details including span-based prediction, the bidirectional input pipeline, and next-sentence prediction.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a fair comparison, all models were based on a 12-layer architecture with the same model hyper-parameters as BERT-Base and were trained on only Wikipedia and the BooksCorpus. All results reported are the median of 5 runs.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/ablation.PNG&#34; data-caption=&#34;Performance on natural language understanding tasks - the GLUE benchmark.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-16_generalized_autoregressive_pretraining_xlnet/images/ablation.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Performance on natural language understanding tasks - the GLUE benchmark.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;From the table -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Transformer-XL and the permutation LM (the basis of XLNet) are big factors in the superior performance of XLNet over BERT.&lt;/li&gt;
&lt;li&gt;On removing the memory caching mechanism, the performance drops especially for RACE where long context understanding is needed.&lt;/li&gt;
&lt;li&gt;Span-based prediction and bidirectional input pipeline also help in the performance of XLNet.&lt;/li&gt;
&lt;li&gt;The next-sentence prediction objective does not lead to an improvement. Hence the next-sentence prediction objective is excluded from XLNet.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/XLNet.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is all for now!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summary #4 - BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</title>
      <link>https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/</link>
      <pubDate>Sun, 09 May 2021 17:01:02 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: BERT - Pre-training of Deep Bidirectional Transformers for Language Understanding&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://www.aclweb.org/anthology/N19-1423.pdf&#34;&gt;https://bit.ly/3bdTUra&lt;/a&gt;    &lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova&lt;br&gt;
&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/google-research/bert&#34;&gt;https://bit.ly/3vRXlM7&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;The paper proposes BERT which stands for Bidirectional Encoder Representations from Transformers. BERT is designed to pre-train deep bidirectional representations from unlabeled text. It performs a joint conditioning on both left and right context in all the layers. The pre-trained BERT model can be fine-tuned with one additional layer to create the final task-specific models i.e., without substantial task-specific architecture modifications. BERT achieves SOTA results on eleven NLP tasks such as natural language inference, question answering textual similarity, text classification, etc.&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;The existing strategies for the pre-trained language representations are mostly based on unidirectional language models and hence are not very effective in capturing the entire context for sentence-level tasks. These are also harmful when applying fine-tuning based approaches to token-level tasks such as question answering, where it is crucial to capture context from both directions.
BERT aims to generate deep bidirectional representations by using maked language models.&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;
&lt;p&gt;Two main steps in the BERT framework are - pre-training and fine-tuning. Pre-training involves training the model on unlabeled data over different pretraining tasks. During fine-tuning, all the BERT parameters are fine-tuned using the labelled data from the downstream tasks. The fine-tuned model is different for each task, however, they share the same pre-trained parameters.&lt;/p&gt;
&lt;h3 id=&#34;model-architecture&#34;&gt;Model Architecture&lt;/h3&gt;
&lt;p&gt;The underlying architecture of BERT is a multi-layer Transformer encoder, which is inherently bidirectional in nature. Two models are proposed in the paper.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;BERT&lt;sub&gt;BASE&lt;/sub&gt; - 12 Transformer blocks, 12 self-attention heads, 768 is the hidden size&lt;/li&gt;
&lt;li&gt;BERT&lt;sub&gt;LARGE&lt;/sub&gt; - 24 transformer blocks, 16 self-attention heads, 1024 is the hidden size&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;i&gt;The model size of BERT&lt;sub&gt;BASE&lt;/sub&gt; and Open AI&amp;rsquo;s GPT was chosen to be the same.&lt;/i&gt;&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/model.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/model.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;input-output-representations&#34;&gt;Input-Output Representations&lt;/h3&gt;
&lt;p&gt;BERT uses WordPiece embeddings with a 30,000 token vocabulary. The first token of every sequence is ([CLS]). The final hidden state corresponding to the [CLS] token is used as the aggregate sequence representation.&lt;br&gt;
To deal with sentence pairs, BERT uses a special token [SEP] to separate the two sentences. A learned embedding is added to every token indicating whether it is the first or the second sentence. The input embedding for each token is obtained by adding the corresponding token embedding (WordPiece embedding), segment embedding (first / second sentence) and position embedding (as in Transformers).&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/inputembeds.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/inputembeds.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;bert-pre-training&#34;&gt;BERT pre-training&lt;/h3&gt;
&lt;p&gt;BERT is pre-trained using two unsupervised tasks.&lt;/p&gt;
&lt;h4 id=&#34;masked-lm&#34;&gt;Masked LM&lt;/h4&gt;
&lt;p&gt;The bidirectional model is more powerful than either a left-to-right model or the shallow concatenation of a left-to-right and right-to-left model.&lt;br&gt;
In order to train a deep bidirectional representation, some percentage (15% in the paper) of the input tokens are masked at random, and those masked tokens are predicted using an output softmax over the vocabulary. This is called a masked LM. The masking is performed by replacing the token with a [MASK] token. Now since the [MASK] token does not appear during fine-tuning, the [MASK] token is used 80% of the time. For 10% of the selected tokens (from the 15%) a random token is used to replace it and the token is kept unchanged for the rest 10%. The token is then predicted using cross-entropy loss.&lt;/p&gt;
&lt;h4 id=&#34;next-sentence-prediction-nsp&#34;&gt;Next Sentence Prediction (NSP)&lt;/h4&gt;
&lt;p&gt;To understand the relationship between two sentences (which is not captured by language modelling), a binarized NSP task is formulated. Here, when choosing the sentences A and B (refer to the model pre-training figure above) for each pre-training example, 50% of the time B is the actual next sentence and the rest 50% of the time, a random sentence from the corpus is used. The vector C (without fine-tuning) is used for NSP. This is helpful for tasks like Question Answering and Natural Language Inference.&lt;/p&gt;
&lt;h4 id=&#34;pre-training-data&#34;&gt;Pre-training data&lt;/h4&gt;
&lt;p&gt;It is useful for BERT to use a document-level corpus rather than a shuffled sentence-level corpus. BERT 9as in the paper) uses the BookCorpus (800M words) and English Wikipedia (2500M words).&lt;/p&gt;
&lt;h3 id=&#34;fine-tuning-bert&#34;&gt;Fine-tuning BERT&lt;/h3&gt;
&lt;p&gt;Instead of independently encoding text (sentence) pairs and then applying bidirectional cross attention, BERT uses the Transformer model architecture&amp;rsquo;s self-attention mechanism. Encoding the concatenated text (sentence) pair with self-attention effectively incorporates bidirectional cross attention between the two sentences.&lt;/p&gt;
&lt;p&gt;The fine-tuning is performed for all the parameters and the task-specific inputs and outputs of the downstream task are plugged for fine-tuning.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A and B are the sentence pairs in case of paraphrasing&lt;/li&gt;
&lt;li&gt;A and B are hypothesis-premise pairs in the entailment task&lt;/li&gt;
&lt;li&gt;A and B are question-passage pairs in question answering&lt;/li&gt;
&lt;li&gt;A and B are the text and Φ in text classification or sequence tagging task&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;At the output, for the token-level tasks (sequence tagging, question answering), the token representations are fed into the output layer. For the sentence-level tasks, the representation of the [CLS] token is fed to the output layer for classification.&lt;/p&gt;
&lt;hr&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;hr&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GLUE&lt;/strong&gt; - The General Language Understanding Evaluation benchmark is a collection of a number of diverse NLP tasks. The 8 datasets the paper evaluates on, are shown below. For these tasks, the [CLS] representation (hidden vector associated with it) is used. The classification layer (a single layer is used) and its weights are the only new parameters introduced. Standard log softmax loss is used.
The model used a batch size of 32 and was fine-tuned for 3 epochs. The learning rate was chosen from a list based on performance on the validation set.
BERT&lt;sub&gt;LARGE&lt;/sub&gt; was unstable on small datasets so random restarts were done with data shuffling and classification layer initialization. It was found that BERT&lt;sub&gt;LARGE&lt;/sub&gt; significantly outperforms BERT&lt;sub&gt;BASE&lt;/sub&gt; (and all other models) across all tasks, especially those with very little training data.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/glue.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/glue.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;SQuAD v1.1&lt;/strong&gt; - A collection of 100k question-answer pairs. Given a question and a passage, the task is to predict the answer span in the text. The question and the passage are represented using A and B embedding respectively. A start vector S and end vector E is introduced in the output. The probability of token &lt;em&gt;i&lt;/em&gt; being the start of the answer is given as&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/start.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/start.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;and similarly for the end token. The score of a candidate span from position &lt;em&gt;i&lt;/em&gt; to position &lt;em&gt;j&lt;/em&gt; is decided to be -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/etend.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/etend.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;This objective is maximised to get the answer range. 
Batch size of 32, learning rate of 5e-5 was used and the model was fine-tuned for 3 epochs. 
Also, for enhanced performance, a prior fine-tuning on the Trivia-QA dataset was done before the fine-tuning on SQuAD.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/squad1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/squad1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;SQuAD v2.0&lt;/strong&gt; - This task allows for the possibility of no short answer existing in the passage for the question, to present a more realistic situation. So, in this case, for the questions which don&amp;rsquo;t have an answer, the start and end is set to be the [CLS] token. So, now there is also a s&lt;sub&gt;null&lt;/sub&gt; = S•C + E•C as the no-answer span score. 
For a non-null answer, a s&lt;sub&gt;i,j&lt;/sub&gt; = S•T&lt;sub&gt;i&lt;/sub&gt; + E•T&lt;sub&gt;j&lt;/sub&gt; is defined. A non-null answer is predicted when s&lt;sub&gt;i,j&lt;/sub&gt; &amp;gt; s&lt;sub&gt;null&lt;/sub&gt; + τ. τ is decided on the basis of the performance of the model on the validation set. TriviaQA data was not used for this model. The model was fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/squad2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/squad2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;SWAG&lt;/strong&gt; - The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference. Given a sentence, the task is to choose the most correct continuation of the sentence among four choices. Scoring is performed for the four sentence pairs, the given sentence A and the possible continuation B. Here a vector is introduced whose dot product with the [CLS] token representation C denotes the score for each of the four choices and a softmax layer is used to get the probability distribution. The model was fine-tuned for 3 epochs with a learning rate of 2e-5 and a batch size of 16.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/swag.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/swag.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Key points from the analysis/ablation studies section -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Two additional modes of pre-training were performed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;No NSP&lt;/strong&gt; - The model is pre-trained with mask LM but not with the NSP task.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LTR and No NSP&lt;/strong&gt; - Instead of a masked LM, a standard left-to-right LM is used and the NSP task is again not performed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An LTR model performs poorly on token predictions and hence doesn&amp;rsquo;t perform well on SQuAD.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For strengthening the LTR models, a randomly initialized BiLSTM model is added on the top. This improves the results on SQuAD but does not perform well on the GLUE tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Separately training LTR (left-to-right) and RTL (right-to-left) models and concatenating them for the token representations is an approach similar to ELMo. But the authors mention that this is twice as expensive as a single bidirectional model. Also, this is unintuitive for tasks like Question Answering since the RTL model would not be able to condition the answer on the question. Furthermore, it is less powerful than a deep bidirectional model, since it can use both left and right context at every layer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;BERT&lt;sub&gt;BASE&lt;/sub&gt; contains 110M parameters and BERT&lt;sub&gt;LARGE&lt;/sub&gt; contains 340M parameters.&lt;/li&gt;
&lt;li&gt;Larger models lead to a strict accuracy improvement across all four datasets, even for MRPC (paraphrasing) which only has 3,600 labelled training examples.&lt;/li&gt;
&lt;li&gt;BERT claims to be the first model to demonstrate convincingly
that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained.&lt;/li&gt;
&lt;li&gt;When the model is fine-tuned directly on the downstream task and uses only a very small number of randomly initialized additional parameters, the task-specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;The feature-based model, in which fixed features are obtained from the model, has some advantages. Firstly, not all tasks can be modelled using a Transformer encoder and require task-specific model architecture to be added.&lt;/li&gt;
&lt;li&gt;Secondly, pre-computing the expensive representations and using them for multiple experiments with cheaper models is a computational benefit.&lt;/li&gt;
&lt;li&gt;The authors compare the feature-based approach for the BERT inference and the normal BERT for the NER task. In the inference part of the feature-based approach, the activations from one or more layers are taken &lt;em&gt;without&lt;/em&gt; any fine-tuning of the BERT parameters for the NER task. These contextual embeddings are used as input to a randomly initialized two-layer 768-dimensional BiLSTM before the classification layer.&lt;/li&gt;
&lt;li&gt;Although this does not perform better than the fine-tuned approach, the best performing method used the concatenation of the last four hidden layers&amp;rsquo; representation of the pre-trained Transformer as the token representation is only 0.3 F1 behind the fine-tuning approach. So, the authors conclude that BERT is effective for both fine-tuning and feature-based approaches.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation3.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation3.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;hr&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;hr&gt;
&lt;h2 id=&#34;results-1&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;GLUE&lt;/strong&gt; - The General Language Understanding Evaluation benchamrk is a collection of a number of diverse NLP tasks. The 8 datasets the paper evaluates on, are shown below. For these tasks, the [CLS] representation (hidden vector associated with it) is used. The classification layer (a single layer is used) and its weights are the only new parameters introduced. Standard log softmax loss is used.
Model used batch size of 32 and was fine tuned for 3 epochs. Learning rate was chosen from a list based on performance on validation set.
BERT&lt;sub&gt;LARGE&lt;/sub&gt; was unstable on small datasets so random restarts were done with data shuffling and classification layer initialization. It was found that BERT&lt;sub&gt;LARGE&lt;/sub&gt; significantly outperforms BERT&lt;sub&gt;BASE&lt;/sub&gt; (and all other models) across all tasks, especially those with very little training data.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/glue.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/glue.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;SQuAD v1.1&lt;/strong&gt; - A collection of 100k question-answer pairs. Given a question and a passage, the task is to predict the answer span in the text. The question and the passage are represneted using A and B embedding respectively. A start vector S and end vector E is introduced in the output. The probability of token &lt;em&gt;i&lt;/em&gt; being the start of the answer is given as&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/start.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/start.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;and similarly for the end token. The score of a candidate span form position &lt;em&gt;i&lt;/em&gt; to position &lt;em&gt;j&lt;/em&gt; is decided to be -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/etend.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/etend.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;This objective is maximised to get the answer range. 
Batch size of 32, learning rate of 5e-5 was used and the model was fine-tuned for 3 epochs. 
Also, for enhanced performance, a prior fine-tunig on the Trivia-QA dataset was done before the fine-tuning on SQuAD.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/squad1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/squad1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;SQuAD v2.0&lt;/strong&gt; - Thsi task allows for the possibilty of no short answer existing in the passage for the question, to present a more realistic situation. So, in this case, for the questions which don&amp;rsquo;t have a answer, the start and end is set to be the [CLS] token. So, now there is also a s&lt;sub&gt;null&lt;/sub&gt; = S•C + E•C as the no-answer span score. 
For a non-null answer, a s&lt;sub&gt;i,j&lt;/sub&gt; = S•T&lt;sub&gt;i&lt;/sub&gt; + E•T&lt;sub&gt;j&lt;/sub&gt; is defined. A non-null answer is predicted when s&lt;sub&gt;i,j&lt;/sub&gt; &amp;gt; s&lt;sub&gt;null&lt;/sub&gt; + τ. τ is decided on the basis of the performance of the model on the validation set. TriviaQA data was not used for this model. The model was fine-tuned for 2 epochs with a learning rate of 5e-5 and a batch size of 48.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/squad2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/squad2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;&lt;strong&gt;SWAG&lt;/strong&gt; - The Situations With Adversarial Generations (SWAG) dataset contains 113k sentence-pair completion examples that evaluate grounded commonsense inference. Given a sentence, the task is to choose the most correct continuation of the sentence among four choices. A scoring is performed for the four sentence pairs, the given sentence A and the possible continuation B. Here a vector is introduiced whose dot product with the [CLS] token representation C denotes the score for each of the four choices and a softmax layer is used to get the probabilty distribution. The model was fine-tuned for 3 epochs with a learning rate of 2e-5 and a batch size of 16.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/swag.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/swag.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Key points from the analysis/ablation studies section -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Two additional modes of pre-training were performed.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;No NSP&lt;/strong&gt; - The model is pre-trained with mask LM but not with the NSP task.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LTR and No NSP&lt;/strong&gt; - Instead of a masked LM, a standard left-to-right LM is used and the NSP task is again not performed.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;The LTR model performs worse than the MLM model on all tasks, with large drops on MRPC and SQuAD.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;An LTR model perofrms poorly on token predictions, and hence doesn&amp;rsquo;t perform well on SQuAD.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;For strengthening the LTR models, a randomly initialized BiLSTM model is added on the top. This improves the results on SQuAD but does not perform well on the GLUE tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Separately training LTR (left-to-right) and RTL (right-to-left) models and concatenating them for the token representations is an approach similar to ELMo. But the authors mention that this is twice as expensive as a single bidirectional model. Also, this is unintuitve for tasks like Question Answering since the RTL model would not be able to condition the answer on the question. Furthermore, it is less powerful than a deep bidirectional model, since it can use both left and right context at every layer.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;BERT&lt;sub&gt;BASE&lt;/sub&gt; contains 110M parameters and BERT&lt;sub&gt;LARGE&lt;/sub&gt; contains 340M parameters.&lt;/li&gt;
&lt;li&gt;Larger models lead to a strict accuracy improvement across all four datasets, even for MRPC (paraphrasing) which only has 3,600 labeled training examples.&lt;/li&gt;
&lt;li&gt;BERT claims to be the first model to demonstrate convincingly
that scaling to extreme model sizes also leads to large improvements on very small scale tasks, provided that the model has been sufficiently pre-trained.&lt;/li&gt;
&lt;li&gt;When the model is fine-tuned directly on the downstream task and uses only a very small number of randomly initialized additional parameters,the task specific models can benefit from the larger, more expressive pre-trained representations even when downstream task data is very small.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;The feature based model, in which fixed features are obtainned from the model, has some advantages. Firstly, not all tasks can be modeled using a Transformer encoder and require task-specific model architecture to be added.&lt;/li&gt;
&lt;li&gt;Secondly, pre-computing the expensive representations and using them for multiple experiments with cheaper models is a computational benefit.&lt;/li&gt;
&lt;li&gt;The authors compare the feature-based approach for the BERT inference and the normal BERT for the NER task. In the inference part of the feature-based aapproach the activations from one or more layers are taken &lt;em&gt;without&lt;/em&gt; any fine-tuning of the BERT paramaetrs for the NER task. These contextual embeddings are used as input to a randomly initialized a two-layer 768-dimensional BiLSTM before the classification layer.&lt;/li&gt;
&lt;li&gt;Although this does not perform better than the the fine-tuned approach, but the best performing method which used the concatenation of the last four hidden layers&amp;rsquo; representaion of the pre-trained Transformer as the token represnetation is only 0.3 F1 behind the fine-tuning approach. So, the authors conclude that BERT is effective for both fine-tunign and feature-based approaches.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation3.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-09_pretraining_deep_bidirectional_transformers_bert/images/ablation3.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/BERT.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is all for now!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summary #3 - Improving Language Understanding by Generative Pre-Training</title>
      <link>https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/</link>
      <pubDate>Sun, 02 May 2021 13:42:14 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: Improving Language Understanding by Generative Pre-Training&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;https://bit.ly/3xITvGP&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Blog&lt;/strong&gt;: &lt;a href=&#34;https://openai.com/blog/language-unsupervised/&#34;&gt;https://openai.com/blog/language-unsupervised/&lt;/a&gt; &lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever&lt;br&gt;
&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/openai/finetune-transformer-lm&#34;&gt;https://bit.ly/3gUFrUX&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;The paper proposes a semi-supervised technique that shows better performance on a wide variety of tasks like textual entailment, question answering, semantic similarity text classification by using a single task-agnostic model. The model can overcome the constraints of the small amount of annotated data for these specific tasks by performing an unsupervised generative-pretraining of a language model on a large diverse text corpus followed by supervised discriminative fine-tuning on each specific task. The pretraining model remains the same for all the tasks. Only a small, task-aware input adaptation is required when performing the fine-tuning. The model significantly improved the state-of-the-art (at the time) in 9 of the 12 tasks studied.&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;Most deep learning models require a substantial amount of data, which makes them difficult to train for tasks in which there is a dearth of good quality annotated data. Historically, pre-trained word embeddings have been used for such cases but the word-level information in itself is sometimes not enough for many of the complex tasks.&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;
&lt;p&gt;The goal of the model is to learn a universal representation that transfers with little adaptation to a wide range of tasks. The paper assumes access to a large corpus of unlabeled text and several datasets with manually annotated training examples (the target tasks). The unlabeled corpus and the annotated datasets need not be in the same domain.&lt;/p&gt;
&lt;p&gt;A two-stage training procedure is used. First, a language modelling (LM) objective is used on the unlabeled data to learn the initial parameters of the model. Next, these parameters are adapted to a target task using the corresponding supervised objective.&lt;/p&gt;
&lt;p&gt;A Transformer (specifically a Transfomer decoder) is used as the underlying architecture. Transformers work better than LSTMs (shown in the results as well) because they can capture long-term dependencies well which results in robust transfer performance across diverse tasks.  Furthermore, during the transfer, as mentioned above, task-specific input adaptations are used which process the structured text input as a single contiguous sequence of tokens. This is something very interesting and will be shown in the subsequent sections.&lt;/p&gt;
&lt;h3 id=&#34;unsupervised-pre-training&#34;&gt;Unsupervised pre-training&lt;/h3&gt;
&lt;p&gt;A standard forward LM objective is used to maximise the likelihood -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/unsupervised-lm.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/unsupervised-lm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here, , &lt;em&gt;U&lt;/em&gt; is the corpus of tokens {&lt;em&gt;u&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;,&amp;hellip; &lt;em&gt;u&lt;/em&gt;&lt;sub&gt;n&lt;/sub&gt;}, &lt;em&gt;k&lt;/em&gt; is the context window size and the conditional probability &lt;em&gt;P&lt;/em&gt;  is modeled using a network with parameters Θ. SGD is used to learn the parameters. The model uses a multi-layer Transformer decoder. The multi-head self-attention is applied over the input context tokens. This is followed by position-wise feedforward layers to produce an output probability distribution over the target tokens.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/probcalc.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/probcalc.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here &lt;em&gt;U&lt;/em&gt; is (u&lt;sub&gt;-k&lt;/sub&gt;,&amp;hellip;, u&lt;sub&gt;-1&lt;/sub&gt;) which is the context vector of tokens, &lt;em&gt;n&lt;/em&gt; is the number of layers, &lt;em&gt;W&lt;/em&gt;&lt;sub&gt;e&lt;/sub&gt; is the token embedding matrix and &lt;em&gt;W&lt;/em&gt;&lt;sub&gt;p&lt;/sub&gt; is the position embedding matrix.&lt;/p&gt;
&lt;h3 id=&#34;supervised-fine-tuning&#34;&gt;Supervised fine-tuning&lt;/h3&gt;
&lt;p&gt;After the training of the model with optimization &lt;em&gt;L&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;, the parameters are now adapted to the supervised target task. The labelled dataset is denoted by &lt;em&gt;C&lt;/em&gt;, where each instance is a sequence of input tokens, &lt;em&gt;x&lt;/em&gt;&lt;sup&gt;1&lt;/sup&gt;,&amp;hellip;,&lt;em&gt;x&lt;/em&gt;&lt;sup&gt;m&lt;/sup&gt;, along with a label &lt;em&gt;y&lt;/em&gt;. The inputs are passed through the pre-trained model to obtain the final transformer block&amp;rsquo;s activation &lt;em&gt;h&lt;/em&gt;&lt;sub&gt;l&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt;, which is then fed into an added linear output layer with parameters &lt;em&gt;W&lt;/em&gt;&lt;sub&gt;y&lt;/sub&gt; to predict &lt;em&gt;y&lt;/em&gt;.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/fintune1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/fintune1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The objective to be maximized is as follows&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/fintune2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/fintune2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Using an LM objective as an auxiliary objective to the finetuning helped to improve the generalization of the supervised model and make it converge faster.&lt;/p&gt;
&lt;p&gt;The overall objective can be written as -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/objective-fin.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/objective-fin.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;task-specific-input-transformations&#34;&gt;Task-specific input transformations&lt;/h3&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/input-transform.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/input-transform.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Since the pretrained model is trained on a contiguous sequence of texts, to handle the inputs of the various tasks, certain input transformations are needed as shown above. These transformations help to avoid making extensive changes to the architecture across tasks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Textual Entailment&lt;/strong&gt; - The premise (&lt;em&gt;p&lt;/em&gt;) and the hypothesis (&lt;em&gt;h&lt;/em&gt;) sequences are concatenated with a delimiter token in between.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Similarity&lt;/strong&gt; - Since there is no inherent ordering of the two sequences being compared, the input sequence is modified to contain both possible sentence orderings (with a delimiter in between). Each of these concatenated sequences is processed independently to produce two sequence representations &lt;em&gt;h&lt;/em&gt;&lt;sub&gt;l&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; which are then element-wise added before feeding to the linear output layer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Question Answering&lt;/strong&gt; - This one is interesting. For a given context document &lt;em&gt;z&lt;/em&gt;, question &lt;em&gt;q&lt;/em&gt; and a set of possible answers {&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;k&lt;/sub&gt;}. The document and question are concatenated with each of the possible answers, with a delimiter token in between [&lt;em&gt;z&lt;/em&gt;; &lt;em&gt;q&lt;/em&gt;;$;&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;k&lt;/sub&gt;]. Each of these sequences is processed independently by the model and then normalized by a softmax layer to produce an output distribution over possible answers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model specifications for the experimental setup are shown below -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/setup.PNG&#34; data-caption=&#34;Experimental Setup&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/setup.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Experimental Setup
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;The datasets that were used are listed below -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/datasets.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/datasets.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Natural Language Inference&lt;/strong&gt; - This task is challenging due to the presence of a wide variety of phenomena like lexical entailment, coreference, and lexical and syntactic ambiguity. The model performs better than the state-of-the-art in 4 (MNLI, QNLI, SNLI, SciTail) out of 5 datasets.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/nli.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/nli.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Question Answering and Commonsense Reasoning&lt;/strong&gt; - The RACE dataset (passages with associated questions from middle and high school exams) and Story Cloze dataset (selecting correct ending to multi-sentence stories from two options) were used. The model outperformed the baseline on both these datasets.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/qa.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/qa.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semantic Similarity&lt;/strong&gt; - The challenges in this task are recognizing rephrasing, negation, and handling ambiguity. The model performs better on 2 (QQP and STS-B) of the 3 datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt; - The model performs better on both Corpus of Linguistic Accepttability (CoLA) dataset and is at par with the state-of-the-art results on the Stanford Sentiment Treebank dataset.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/classification.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/classification.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Key points from the analysis section -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More the number of layers that are transferred from the pretrained model to the supervised target task, the better is the performance on the target tasks.&lt;/li&gt;
&lt;li&gt;To understand whether the unsupervised pre-training is effective or not, zero-shot testing was also performed i.e., using the pre-trained model directly without any finetuning. The model performance is stable and steadily increases over training suggesting that the generative pre-training supports the learning of a wide variety of task-relevant functionality. LSTMs exhibit higher variance in their zero-shot performance.
The testing and input transformations for using the pretrained model directly are explained below -













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/zeroshot.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/zeroshot.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/trend.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/trend.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;From the ablation studies, the authors show that the auxiliary LM objective helps on the NLI tasks and QQP (Quora Question Pairs data).&lt;/li&gt;
&lt;li&gt;Overall, larger datasets benefit from the auxiliary objective more than the smaller datasets.&lt;/li&gt;
&lt;li&gt;In general, the Transformer architecture performs better than a 2048 unit single layer LSTM model (if the Transformer in the pretraining model is replaced by an LSTM) on all datasets except the MRPC (Microsoft Paraphrase Corpus for semantic similarity) dataset.&lt;/li&gt;
&lt;li&gt;On comparing this model with the same transformer architecture trained in a supervised manner, it is observed that the model with pre-training performs better. This consistent for all the tasks mentioned in the paper, suggesting that pre-training helps to capture important linguistic information which is not captured when training with a supervised approach alone.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT1.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is all for now!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summary #2 - Deep contextualized word representations</title>
      <link>https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/</link>
      <pubDate>Sun, 25 Apr 2021 15:13:13 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: Deep contextualized word representations&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/1802.05365&#34;&gt;https://arxiv.org/abs/1802.05365&lt;/a&gt; &lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Matthew E. Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, Luke Zettlemoyer&lt;br&gt;
&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/allenai/allennlp/blob/main/allennlp/modules/elmo.py&#34;&gt;https://bit.ly/3xpHNAI&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; - Since this is a relatively old paper, all the performance comparisons and state-of-the-art claims mentioned below should only be considered for the models at the time the paper was published.&lt;/p&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;The paper proposes a new type of deep contextualized word representation that helps to effectively capture the syntactic and semantic characteristics of the word along with the linguistic context of the word. It can help differentiate the same word being used in different contexts with different meanings. The representations (embeddings) are learned from the internal states of a deep bidirectional language model (biLM). The embeddings, when used with the existing models, significantly improved the state of the art in six NLP problems - Question Answering, Natural Language Inference, Semantic Role Labeling, Coreference Resolution, Named Entity Recognition and Sentiment Analysis.&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;The existing word representations commonly in use were Word2Vec and GloVe. However, there was a need to capture even richer word representations. The paper states that the two main requirements of a good representation should be that they should be able to capture the complex characteristics of the word use and at the same time capture polysemy as well. This is the idea behind using ELMo (Embeddings from Language Models) representations.&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;
&lt;p&gt;As a high-level overview, it can be said that the ELMo representations are a function of the entire input sequence. A two-layer biLM model with character-level convolutions is trained on a text corpus. The ELMo word representations are computed as a linear function of the internal network states of the biLM. The biLM is pretrained on a large scale and the ELMo representations can be incorporated into several deep learning-based NLP architectures.&lt;/p&gt;
&lt;h3 id=&#34;bilm-bidirectional-language-model&#34;&gt;biLM (Bidirectional Language Model)&lt;/h3&gt;
&lt;p&gt;A forward language model computes the probability of the sequence by modelling the probability of a token t&lt;sub&gt;k&lt;/sub&gt; given the history (t&lt;sub&gt;1&lt;/sub&gt;, &amp;hellip;, t&lt;sub&gt;k-1&lt;/sub&gt;). Similarly, a backward language model predicts the previous token given the nature context i.e., it performs the same function but in reverse order.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/forwardlm.PNG&#34; data-caption=&#34;Forward LM probability modelling&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/forwardlm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Forward LM probability modelling
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/backwardlm.PNG&#34; data-caption=&#34;Backward LM probability modelling&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/backwardlm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Backward LM probability modelling
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In a forward LM, a context-independent token representation x&lt;sub&gt;k&lt;/sub&gt;&lt;sup&gt;LM&lt;/sup&gt; is obtained from a character-level CNN and then passed through &lt;em&gt;L&lt;/em&gt; layers of LSTMs. At each position &lt;em&gt;k&lt;/em&gt;, the LSTM layer outputs a context-dependent representation h&lt;sub&gt;&lt;i&gt;k,j&lt;/i&gt;&lt;/sub&gt;&lt;sup&gt;LM&lt;/sup&gt;, where &lt;em&gt;j&lt;/em&gt; = 1, &amp;hellip;, &lt;em&gt;L&lt;/em&gt;. the top layer of the LSTM output is used to predict the next token t&lt;sub&gt;k+1&lt;/sub&gt; with a Softmax layer. The same procedure is applied to the backward LM as well.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/bilm.PNG&#34; data-caption=&#34;biLM probability modelling&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/bilm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    biLM probability modelling
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;A biLM combines both the forward and backward LM. The above formulation jointly optimizes the log-likelihood of the forward and backward directions.&lt;/p&gt;
&lt;p&gt;The formulation ties both the token representation Θ&lt;sub&gt;x&lt;sub&gt; and the Softmax layer Θ&lt;sub&gt;s&lt;/sub&gt; Separate paremeters are maintained for the forward and backward LSTMs.&lt;/p&gt;
&lt;p&gt;Next, we look at getting the word representations using ELMo.&lt;/p&gt;
&lt;h3 id=&#34;elmo&#34;&gt;ELMo&lt;/h3&gt;
&lt;p&gt;ELMo is a task-specific combination of the intermediate layer representations of the biLM model. If we have &lt;em&gt;L&lt;/em&gt; LSTM layers, then for each token t&lt;sub&gt;k&lt;/sub&gt; we have 2L + 1 representations.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/elmorepr.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/elmorepr.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Now to get one single vector for each token, all the representations in &lt;em&gt;R&lt;/em&gt; are merged to one. Usually, task-specific weighting is performed.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/elmoeq.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/elmoeq.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The &lt;i&gt;s&lt;/i&gt;&lt;sup&gt;&lt;i&gt;task&lt;/i&gt;&lt;/sup&gt; are softmax normalized weights and the scale parameter γ&lt;sup&gt;&lt;i&gt;task&lt;/i&gt;&lt;/sup&gt; allows the task model to scale the entire ELMo vector. In some cases, applying LayerNorm to each biLM layer before weighting also helped.&lt;/p&gt;
&lt;h3 id=&#34;using-elmo-for-supervised-nlp-tasks&#34;&gt;Using ELMo for supervised NLP tasks&lt;/h3&gt;
&lt;p&gt;We start with a pretrained biLM model, The biLM is run to record the layer representations for each word. When using any supervised deep learning MLP model have a common architecture for the lowest layers. They usually use a context-independent token representation x&lt;sub&gt;k&lt;/sub&gt; for each token position using pre-trained embeddings and optionally also using character-based representations. Then, in the higher layers, the model forms context-sensitive representations using RNNs, CNNs or whatever, as per the task and the model.
For using ELMo, we can start in the same manner. We obtain the embeddings from the freezed weights of the biLM. Now instead of passing just x&lt;sub&gt;k&lt;/sub&gt; to the above layers, we will pass &lt;/br&gt; [x&lt;sub&gt;k&lt;/sub&gt;; &lt;strong&gt;ELMo&lt;/strong&gt;&lt;sub&gt;k&lt;/sub&gt;&lt;sup&gt;task&lt;/sup&gt; ] into the task model layers. For some tasks like SNLI (Natural language Inference) and SQuAD (Question-Answering), it was also seen that including ELMo at the output of the task model by introducing another set of output specific linear weights and replacing h&lt;sub&gt;k&lt;/sub&gt; with [h&lt;sub&gt;k&lt;/sub&gt;; &lt;strong&gt;ELMo&lt;/strong&gt;&lt;sub&gt;k&lt;/sub&gt;&lt;sup&gt;task&lt;/sup&gt; ] led to an improvement.&lt;/p&gt;
&lt;p&gt;Additionally, in some cases, regularizing the ELMo weights with λ||&lt;strong&gt;w&lt;/strong&gt;||&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; helped introduce an inductive bias on the ELMo weights to make it stay close to the average of all biLM layers.&lt;/p&gt;
&lt;h3 id=&#34;pre-trained-bidirectional-language-model-architecture&#34;&gt;Pre-trained bidirectional language model architecture&lt;/h3&gt;
&lt;p&gt;The pre-trained biLM used in the paper is similar to the architecture in &lt;a href=&#34;https://arxiv.org/abs/1602.02410&#34;&gt;Józefowicz et al.&lt;/a&gt;. It is modified to support joint training of both directions and a residual connection is added between the LSTM layers. The size of the embeddings and layers were from what was in the &lt;code&gt;CNN-BIG-LSTM&lt;/code&gt; architecture in &lt;a href=&#34;https://arxiv.org/abs/1602.02410&#34;&gt;Józefowicz et al.&lt;/a&gt;. The final model has &lt;em&gt;L&lt;/em&gt;=2 biLSTM layers with 4096 units and 512-dimensional embeddings and a residual connection from the first to the second layer. The context insensitive type representation uses 2048 character n-gram convolutional filters followed by two highway layers and a linear projection down to a 512 representation. As a result, the biLM provides three layers of representations for each input token, including those outside the training set due to the purely
character input.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/results.PNG&#34; data-caption=&#34;Results comparison of the baseline models with the ones used along with ELMo&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/results.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Results comparison of the baseline models with the ones used along with ELMo
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The details of the baseline models are given in the paper. In all the tasks, the use of the ELMo representations led to improvement in the state-of-the-art results.&lt;/p&gt;
&lt;p&gt;Key points from the analysis section -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Regularization parameter λ is important. λ=1 means that we are effectively reducing the weighting function to a simple average over the layers, while smaller values like λ=0.001 allows the layer weights to vary.&lt;/li&gt;
&lt;li&gt;The fact that we take the representations from all the layers gives a better performance as compared to just taking the topmost layer. Taking just the last layer is still better than the baseline.&lt;/li&gt;
&lt;li&gt;A small λ is preferred in most cases with ELMo, although for NER, a task with a smaller training set, the results are insensitive to λ.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/layercomp.PNG&#34; data-caption=&#34;Baseline vs ELMo last layer vs All the layers&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/layercomp.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Baseline vs ELMo last layer vs All the layers
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;Including ELMo at both the input and output layers for SNLI and SQuAD improves over just the input layer. This is because SNLI and SQuAD use an attention layer after the biRNN and using ELMo at the output layer would allow the model to attend directly to the internal representations of the biLM. But for SRL (and coreference resolution) performance is highest when it is included at just the input layer. Probably because the task-specific context representations are more important than those from the biLM.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/layerloc.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/layerloc.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;The higher-level LSTM states of the biLM capture context-dependent aspects of word meaning (e.g., they can be used without modification to perform well on supervised word sense disambiguation tasks) while lower-level states model aspects of syntax (e.g., they can be used to do part-of-speech tagging).&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/contextcapture.PNG&#34; data-caption=&#34;biLM captures the context of the word &amp;lsquo;play&amp;rsquo; effectively from the source sentences&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/contextcapture.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    biLM captures the context of the word &amp;lsquo;play&amp;rsquo; effectively from the source sentences
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Different layers in the biLM represent different types of information and explains why including all biLM layers are important for the highest performance in downstream
tasks.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Using ELMo with a model also improves the sample efficiency. The model now requires a fewer number of epochs (parameter updates) and less amount of training data as well. For eg., the baseline SRL model requires 486 epochs to reach the maximum F1 score. The model with the ELMo representations only requires 10 epochs to exceed the baseline. In addition, ELMo-enhanced models use smaller training sets more efficiently than models without ELMo. Again, if we consider the SRL case, the ELMo model with 1% of the training set has about the same F1 as the baseline model with 10% of the training set.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/sampleeff.PNG&#34; data-caption=&#34;biLM captures the context of the word &amp;lsquo;play&amp;rsquo; effectively from the source sentences&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-25_deep_contextualized_word_representations_elmo/images/sampleeff.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    biLM captures the context of the word &amp;lsquo;play&amp;rsquo; effectively from the source sentences
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/ELMo.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is all for now!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summary #1 - Attention Is All You Need</title>
      <link>https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/</link>
      <pubDate>Sun, 18 Apr 2021 16:57:49 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: Attention Is All You Need&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://bit.ly/3aklLFY&#34;&gt;https://bit.ly/3aklLFY&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin&lt;br&gt;
&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/tensorflow/tensor2tensor&#34;&gt;https://github.com/tensorflow/tensor2tensor&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;Proposes Transformers, a new simple architecture for sequence transduction that uses only an attention mechanism and does not use any kind of recurrence or convolution. This model achieves SOTA (at the time) on the WMT 2014 English-to-French translation task with a score of 41.0 BLEU. Also beats the existing best results on the WMT 2014 English-to-German translation task with a score of 28.4 BLEU. The training cost is also much less than the best models chosen in the paper (at the time).&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;Existing recurrent models like RNNs, LSTMs or GRUs work sequentially. They align the positions to steps in computation time. They generate a sequence of hidden states as a function of the previous hidden state and the input for the current position. But sequential computation has constraints. They are not easily parallelizable which is required when the sequence lengths become large. The Transformer model eschews recurrence and allows for more parallelization and requires less training time to achieve SOTA in the machine translation task.&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/arch.PNG&#34; data-caption=&#34;Detailed Transformer Architecture&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/arch.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Detailed Transformer Architecture
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The model is auto-regressive, it consumes the previously generated symbols as additional input when generating the next.&lt;/p&gt;
&lt;h3 id=&#34;encoder&#34;&gt;Encoder&lt;/h3&gt;
&lt;p&gt;The figure above shows just one layer of the encoder on the left. There are &lt;code&gt;N=6&lt;/code&gt; such layers. Each layer has two sub-layers - a multi-head self-attention layer and a position-wise fully connected feed-forward network. &lt;a href=&#34;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf&#34;&gt;Residual connections&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1607.06450&#34;&gt;layer normalization&lt;/a&gt; is used for each sub-layer.&lt;/p&gt;
&lt;h3 id=&#34;decoder&#34;&gt;Decoder&lt;/h3&gt;
&lt;p&gt;This also has &lt;code&gt;N=6&lt;/code&gt; stacked layers. The architecture diagram shows one layer of the decoder on the right. Each layer has three sub-layers. Two of them are the same as the encoder. The third layer performs multi-head attention over the output of the encoder stack. This is modified to prevent positions from attending to subsequent positions. Additionally, the output embeddings are also offset by one position. These features ensure that the predictions for a position depend only on the known outputs for positions before it.&lt;/p&gt;
&lt;h3 id=&#34;attention&#34;&gt;Attention&lt;/h3&gt;
&lt;p&gt;The paper uses a modified dot product attention, and it is called &amp;ldquo;Scaled Dot Product Attention&amp;rdquo;. Given queries and keys of dimension d&lt;sub&gt;k&lt;/sub&gt; and values of dimension d&lt;sub&gt;v&lt;/sub&gt;, the attention matrix is calculated as shown below.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/attention.PNG&#34; data-caption=&#34;Attention Matrix Calculation&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/attention.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Attention Matrix Calculation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Since, for large values of d&lt;sub&gt;k&lt;/sub&gt; the dot product grows large in magnitude, it pushes the softmax function into regions where it has extremely small gradients. The scaling of 1/sqrt(d&lt;sub&gt;k&lt;/sub&gt;) is done to avoid the problem of vanishing gradients.&lt;/p&gt;
&lt;p&gt;Multi-Head attention allows computing this attention in parallel. This helps to focus on different positions. Secondly, it also helps to attend to information from different subspaces due to the more number of attention heads.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/multihead-attention.PNG&#34; data-caption=&#34;Multihead Attention Calculation&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/multihead-attention.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Multihead Attention Calculation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The paper uses &lt;code&gt;h=8&lt;/code&gt; parallel attention layers or heads. The reduced dimension of each head compensates for the more number of heads and hence the computational cost remains the same as with single-head attention with full dimensionality.&lt;/p&gt;
&lt;p&gt;Applications of multi-head attention in the paper are given below -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/application-attention.PNG&#34; data-caption=&#34;Application of multi-head attention in the model&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/application-attention.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Application of multi-head attention in the model
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/multihead-attention-fig.PNG&#34; data-caption=&#34;Pictorial representaion of Multi-head attention&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/multihead-attention-fig.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Pictorial representaion of Multi-head attention
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;position-wise-feed-forward-networks&#34;&gt;Position-wise Feed-Forward Networks&lt;/h3&gt;
&lt;p&gt;The FFN sub-layer shown in the encoder and decoder architecture is a 2-hidden layer FC FNN with a ReLU activation in between.&lt;/p&gt;
&lt;h3 id=&#34;positional-encodings&#34;&gt;Positional Encodings&lt;/h3&gt;
&lt;p&gt;Positional encodings are injected (added) to the input embeddings at the bottom of the encoder and decoder stack to add some information about the relative order of the tokens in the sequence. The positional encodings have the same dimension as the input embeddings so that they can be added.
For position &lt;em&gt;pos&lt;/em&gt; and dimension &lt;em&gt;i&lt;/em&gt; the paper uses the following positional embeddings -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/positional.PNG&#34; data-caption=&#34;Positional Encoding calculation&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/positional.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Positional Encoding calculation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This choice allows the model to easily learn by the relative positions. The learned positional embeddings also perform about the same as the sinusoidal version. The sinusoidal version may allow the model to extrapolate to sequence lengths longer than the ones encountered in training.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/experiments.PNG&#34; data-caption=&#34;Experimental results when varying parameters&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/experiments.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Experimental results when varying parameters
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;Form (A), it can be seen that single-head attention is slightly worse than the best setting. The quality also drops off with too many heads.&lt;/li&gt;
&lt;li&gt;(B) shows that reducing the attention key size &lt;i&gt;d&lt;sub&gt;k&lt;/sub&gt;&lt;/i&gt; hurts model quality.&lt;/li&gt;
&lt;li&gt;In (C) and (D), it is visible that bigger models are better and dropout helps in avoiding overfitting.&lt;/li&gt;
&lt;li&gt;(E) shows that sinusoidal positional encoding when replaced with learned positional embeddings also does not lead to a loss in quality&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the base models, the authors used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. The big models were averaged over the last 20 checkpoints. Beam search with a beam size of 4 and length penalty α = 0.6. The maximum output length during inference is set to input length +50, but if it is possible, the model terminates early.&lt;/p&gt;
&lt;p&gt;The performance comparison with the other models is shown below -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/results.PNG&#34; data-caption=&#34;Model performance&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/results.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Model performance
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/Attention%20Is%20All%20You%20Need.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is all for now!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Deep Learning in the Browser - Exploring TF.js, WebDNN and ONNX.js</title>
      <link>https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/</link>
      <pubDate>Mon, 25 Jan 2021 12:53:13 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/</guid>
      <description>&lt;p&gt;After my &lt;a href=&#34;https://shreyansh26.github.io/post/2020-11-30_fast_api_docker_ml_deploy&#34;&gt;last post&lt;/a&gt; on deploying Machine Learning and Deep Learning models using FastAPI and Docker, I wanted to explore a bit more on deploying deep learning models. My last post discussed a server-side method for deploying the model. This post will discuss client side frameworks and techniques to deploy those models such that they work directly on the client side.&lt;/p&gt;
&lt;p&gt;In this tutorial I will be giving an overview of three frameworks, &lt;a href=&#34;https://www.tensorflow.org/js&#34;&gt;Tensorflow.js&lt;/a&gt;, &lt;a href=&#34;https://mil-tokyo.github.io/webdnn/&#34;&gt;WebDNN&lt;/a&gt; and &lt;a href=&#34;https://microsoft.github.io/onnxjs-demo/#/&#34;&gt;ONNX.js&lt;/a&gt;. I will be a deploying a simple pretrained image classification model (ResNet or Mobilenet) on the three frameworks and also tell you the comparsion between them. In this tutorial, I haven&amp;rsquo;t deployed custom models of my own but I will be explaining how you can do it and the difficulties you could encounter.&lt;/p&gt;
&lt;p&gt;The goal of this blog post is to introduce the three frameworks and how you can use them for deploying your models as well. Personally, I had not heard of WebDNN and ONNX.js before diving into this project, so I believe it can help some others like me to get familiar with these frameworks.&lt;/p&gt;
&lt;h2 id=&#34;tensorflowjs&#34;&gt;Tensorflow.js&lt;/h2&gt;
&lt;p&gt;I found Tensorflow.js to be the easiest to use. It already has a large collection of some &lt;a href=&#34;https://github.com/tensorflow/tfjs-models&#34;&gt;pretrained models&lt;/a&gt;. With Tensorflow.js, we don&amp;rsquo;t have a pretrained Resnet model because it is not exactly a lightweight model that can be deployed on a device with low compute power. So, I used &lt;a href=&#34;https://arxiv.org/abs/1704.04861&#34;&gt;Mobilenet&lt;/a&gt; (which is trained on the Imagenet dataset). Mobilenet was available in the Tensorflow.js pretrained models repository so I decided to use that directly.&lt;/p&gt;
&lt;p&gt;Now, on to the fun part, actually using the model and making a webapp. For the webapp portion, I am using &lt;a href=&#34;https://expressjs.com/&#34;&gt;Express&lt;/a&gt;, a web framework for Node.js. I have tried to keep the code structure and the webapp visually similar for all the three frameworks.&lt;/p&gt;
&lt;p&gt;Loading the model is as simple as -&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/dfedd9a445841a8bb963af9526a9f21c.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Now after loading the model, we call the &lt;code&gt;imgSet&lt;/code&gt; function which bascially loads the image from the path we specify and loads it onto a canvas. Details of this can be seen in the code which I will post at the end.&lt;/p&gt;
&lt;p&gt;Although the Mobilenet model in Tensoflow.js doesn&amp;rsquo;t require a fixed size of the image, but for uniformity in all other frameworks (WebDNN, ONNX.js), I decided to resize the images to 224x224 size. The main code for running the model is shown below -&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/3b1bc92aa52a13cabae2f426b36c2576.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The final webapp looks something like this -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/tfapp.gif&#34; data-caption=&#34;Image loading and prediction&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/tfapp.gif&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Image loading and prediction
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The model works well. It knows it is some kind of a water related animal, and given the Imagenet classes it has been trained on, it gies the closest result possible.&lt;/p&gt;
&lt;p&gt;The first prediction takes time (196ms) because the model is loaded and run for the first time. After that, the predictions take very little time (~80ms) mainly because the model is cached and predictions can be served faster.&lt;/p&gt;
&lt;p&gt;The average time taken by different backends (over 20 predictions) is also shown below -&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Backend&lt;/th&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;cpu&lt;/td&gt;
&lt;td&gt;2100ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;wasm&lt;/td&gt;
&lt;td&gt;82ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;webgl&lt;/td&gt;
&lt;td&gt;70ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;If one wants to convert their own models to a Tensorflow.js compatible version, it is very easy to convert the model as well as load it into your web application. One can refer to &lt;a href=&#34;https://github.com/tensorflow/tfjs/tree/master/tfjs-converter&#34;&gt;tfjs-converter&lt;/a&gt; and the documentation given &lt;a href=&#34;https://www.tensorflow.org/js/guide/conversion&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The code for this section is present &lt;a href=&#34;https://github.com/shreyansh26/DeepLearning-in-the-Browser/tree/main/TF&#34;&gt;on my Github&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;webdnn&#34;&gt;WebDNN&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://mil-tokyo.github.io/webdnn/&#34;&gt;WebDNN&lt;/a&gt; was developed by the Machine Intellignece Laboratory at the University of Tokyo. From its website,&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;WebDNN optimizes the trained DNN model to compress the model data and accelerate the execution, and executes it with novel JavaScript API such as WebAssembly and WebGPU to achieve zero-overhead execution. WebDNN supports 4 execution backend implementations: WebMetal, WebGL, WebAssembly, and fallback pure javascript implementation. By using these backends, WebDNN works all major browsers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;More details are available on the website, but the image below accurately depicts the steps involved in this procedure.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/webdnn-arch.PNG&#34; data-caption=&#34;WebDNN model conversion flow&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/webdnn-arch.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    WebDNN model conversion flow
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;WebDNN can be used to deploy tarined DNN models trained using popular DL frameworks like Tensorflow, Keras, PyTorch, Chainer, Kaffe. One disadvantage I found of using WebDNN is that the current model conversion module (as of writing the post) does not allow conversion using Tensorflow 2 and also does not support the latest versions of Keras (let alone &lt;code&gt;tensorflow.keras&lt;/code&gt;).&lt;/p&gt;
&lt;p&gt;I used a pretrained ResNet50 model (trained on Imagnet dataset) for this. I am sharing the &lt;a href=&#34;https://colab.research.google.com/drive/1pFdbZc5_Dd78twKshl-MH8T_EuVrH0Nw?usp=sharing&#34;&gt;following Colab notebook&lt;/a&gt; which contains the code to convert the ResNet50 Keras model.&lt;/p&gt;
&lt;p&gt;On to the web app coding part! The first thing the webapp does is to load the model.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/a527987583919e53b237a7d1a312f3a8.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;Next, we write the code to run the model on the image input.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/a7a9eb637f31e9dee0a2b39822ebc4b7.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The final webapp looks something like this -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/webdnn.gif&#34; data-caption=&#34;WebDNN predictions&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/webdnn.gif&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    WebDNN predictions
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The model does a very good job of identifying it is a bus. The top two predictions relate to it.&lt;/p&gt;
&lt;p&gt;Again, the first run takes a long time (~242ms) but the subsequent runs take quite less (~63ms average). Now one must note that ResNet50 is a relatively heavier model as compared to Mobilenet, but WebDNN manages to load it much faster than or at par with Mobilenet as we saw in the case with Tensorflow.js. Also, in the COlab notebook, we can see that for the same image, the ResNet50 model around 645ms to run the model. We easily see a ~10x improvement on converting the model to WebDNN.&lt;/p&gt;
&lt;p&gt;The average time taken by different backends (over 20 predictions) is also shown below -&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Backend&lt;/th&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;cpu&lt;/td&gt;
&lt;td&gt;10000ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;webgl&lt;/td&gt;
&lt;td&gt;60ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;WebDNN is quite optimised to run on futuristic hardware. The time it takes on a normal fallback vanilla-JS model version running on the CPU is around 10 seconds. But on WebGL, it takes much much less. I didn&amp;rsquo;t have access to a WebMetal backend, which they claim is the fastest. I would like to know if anyone runs it on WebGPU (WebMetal) and the average time the model took to run on it.&lt;/p&gt;
&lt;p&gt;The code for this section is present &lt;a href=&#34;https://github.com/shreyansh26/DeepLearning-in-the-Browser/tree/main/WebDNN&#34;&gt;on my Github&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;onnx&#34;&gt;ONNX&lt;/h2&gt;
&lt;p&gt;&lt;a href=&#34;https://onnx.ai/&#34;&gt;Open Neural Network Exchange (ONNX)&lt;/a&gt; is an open source format for AI models, both deep learning and traditional ML.&lt;/p&gt;
&lt;p&gt;From their website -&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;ONNX defines a common set of operators - the building blocks of machine learning and deep learning models - and a common file format to enable AI developers to use models with a variety of frameworks, tools, runtimes, and compilers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/microsoft/onnxjs&#34;&gt;ONNX.js&lt;/a&gt; is an open source Javascript library by Microsoft for running ONNX models on browsers and on Node.js. Like Tensorflow.js and WebDNN, it also has support for WebGL and CPU. From theit Github&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;With ONNX.js, web developers can score pre-trained ONNX models directly on browsers with various benefits of reducing server-client communication and protecting user privacy, as well as offering install-free and cross-platform in-browser ML experience.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;With ONNX.js, I used a pretrained &lt;a href=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/files/resnet50_8.onnx&#34;&gt;ResNet50 model&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Loading the model is similar -&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/07658e9b0b4dc759fb4f081cd9ea7b78.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;The ONNX examples on their repository gives some nice code snippets to show basic image preprocessing. I have used it directly in my code.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/1a2f6059395c60485e4d721c7afd761b.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;After that, the following code snippet loads the preprocessed image to an input tensor and then runs the model on it and then prints the predictions.&lt;/p&gt;
&lt;script type=&#34;application/javascript&#34; src=&#34;https://gist.github.com/shreyansh26/7e4366058eac9a8c1f05a82b569ff91a.js&#34;&gt;&lt;/script&gt;

&lt;p&gt;A demo of the webapp using ONNX.js is shown below.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/onnx1.gif&#34; data-caption=&#34;ONNX.js predictions&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/onnx1.gif&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    ONNX.js predictions
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/onnx2.gif&#34; data-caption=&#34;ONNX.js predictions&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-01-25_deep_learning_in_the_browser/images/onnx2.gif&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    ONNX.js predictions
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The Resnet model does an awesome job with the airline image and classifies it correctly. It also performs decently on the bus image giving the top prediction as &lt;em&gt;minibus&lt;/em&gt;. However, the goal of this post is not to judge how well the model works, but the technique of deploying the models and receiving predictions from them.&lt;/p&gt;
&lt;p&gt;I used the WebGL model for testing. It takes an average of 70ms to serve the predictions. The CPU version takes a VERY long time ~15000ms (15 seconds).&lt;/p&gt;
&lt;p&gt;The average time taken by different backends (over 20 predictions) is also shown below. I had some trouble with the WASM version so I didn&amp;rsquo;t include them in the results.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Backend&lt;/th&gt;
&lt;th&gt;Time&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;cpu&lt;/td&gt;
&lt;td&gt;15000ms&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;webgl&lt;/td&gt;
&lt;td&gt;71ms&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The best part about ONNX is that it is an open standard and allows easy conversion of models made in different frameworks to a &lt;code&gt;.onnx&lt;/code&gt; model. I would suggest going through &lt;a href=&#34;https://github.com/onnx/tutorials&#34;&gt;this tutorial&lt;/a&gt; for this.&lt;/p&gt;
&lt;p&gt;The code for this section is present &lt;a href=&#34;https://github.com/shreyansh26/DeepLearning-in-the-Browser/tree/main/ONNX&#34;&gt;on my Github&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id=&#34;the-end&#34;&gt;The End&lt;/h2&gt;
&lt;p&gt;That is all for now. I hope that this tutorial will help the reader get an idea of these frameworks for client-side model deployment and one can also use my code as a boilerplate for setting up webapps of your own for deploying ML models using these frameworks.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Privacy-preserving Deep Learning for Medical Image Classification</title>
      <link>https://shreyansh26.github.io/project/privacy-ml/</link>
      <pubDate>Mon, 25 Nov 2019 17:53:33 +0530</pubDate>
      <guid>https://shreyansh26.github.io/project/privacy-ml/</guid>
      <description>&lt;p&gt;Privacy Preserving Deep Learning for Medical Image Classification to detect chest pneumonia in chest X-ray images.&lt;/p&gt;
&lt;p&gt;Uses TF-Encrypted to implement Secure Multiparty Computation (SMPC) and Differential Privacy. SMPC helps to provide secure predictions and Differential Privacy is used to enhance privacy.&lt;/p&gt;
&lt;p&gt;The base model is a VGG16 model which is made secure and privacy-preserving.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multilingual Surface Realization for NLG</title>
      <link>https://shreyansh26.github.io/project/msr-nlg/</link>
      <pubDate>Mon, 23 Jul 2018 16:52:28 +0530</pubDate>
      <guid>https://shreyansh26.github.io/project/msr-nlg/</guid>
      <description>&lt;p&gt;A shared task organized at ACL 2018 (Association for Computational Linguistics, Melbourne, Australia). The task aims to determining the word order and inflecting words from given unordered Universal Dependencies (UD) structures from which word order information has been removed and the tokens have been lemmatized.
Worked on techniques like Language Modelling and Neural Machine Translation methods to solve the problem of reinflection and correct word order generation.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
