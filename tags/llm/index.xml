<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>llm | Shreyansh Singh</title>
    <link>https://shreyansh26.github.io/tags/llm/</link>
      <atom:link href="https://shreyansh26.github.io/tags/llm/index.xml" rel="self" type="application/rss+xml" />
    <description>llm</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Shreyansh Singh 2024</copyright><lastBuildDate>Sun, 18 Feb 2024 19:01:35 +0530</lastBuildDate>
    <image>
      <url>https://shreyansh26.github.io/img/Shreyansh.jpg</url>
      <title>llm</title>
      <link>https://shreyansh26.github.io/tags/llm/</link>
    </image>
    
    <item>
      <title>Paper Summary #10 - Gemini 1.5 Pro</title>
      <link>https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/</link>
      <pubDate>Sun, 18 Feb 2024 19:01:35 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Technical Paper&lt;/strong&gt;: &lt;a href=&#34;https://storage.googleapis.com/deepmind-media/gemini/gemini_v1_5_report.pdf&#34;&gt;Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Blog&lt;/strong&gt;: &lt;a href=&#34;https://blog.google/technology/ai/google-gemini-next-generation-model-february-2024&#34;&gt;Our next-generation model: Gemini 1.5&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;These are just short notes / excerpts from the technical paper for quick lookup. Actual implementational details are anyways missing in the technical paper.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;Gemini 1.5 Pro is a sparse MoE Transformer-based model (seems like a trend these days, after GPT-4&amp;rsquo;s rumors and Mixtral).&lt;/p&gt;
&lt;p&gt;It supports upto 10M context multimodal context length which is about a day of 22 hours of audio recordings, more than ten times the entirety of the 1440 page book &amp;ldquo;War and Peace&amp;rdquo;, the entire Flax codebase (41,070 lines of code), or three hours of video at 1 frame-per-second.&lt;/p&gt;
&lt;p&gt;Near-perfect retrieval across complete context length. In all modalities, i.e., text, video and audio, it achieves 100% accurate needle-in-a-haystack recall for 530k tokens, 99.7% accurate recall for 1M tokens and 99.2% for 10M contexts.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/niah1.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/niah1.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/text_niah.png&#34; data-caption=&#34;Text Haystack. This figure compares Gemini 1.5 Pro with GPT-4 Turbo for the text needle-in-a-haystack task. Green cells indicate the model successfully retrieved the secret number, gray cells indicate API errors, and red cells indicate that the model response did not contain the secret number. The top row shows results for Gemini 1.5 Pro, from 1k to 1M tokens (top left), and from 1M to 10M tokens (top right). The bottom row shows results on GPT-4 Turbo up to the maximum supported context length of 128k tokens. The results are color-coded to indicate: green for successful retrievals and red for unsuccessful ones.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/text_niah.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Text Haystack. This figure compares Gemini 1.5 Pro with GPT-4 Turbo for the text needle-in-a-haystack task. Green cells indicate the model successfully retrieved the secret number, gray cells indicate API errors, and red cells indicate that the model response did not contain the secret number. The top row shows results for Gemini 1.5 Pro, from 1k to 1M tokens (top left), and from 1M to 10M tokens (top right). The bottom row shows results on GPT-4 Turbo up to the maximum supported context length of 128k tokens. The results are color-coded to indicate: green for successful retrievals and red for unsuccessful ones.
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/video_niah.png&#34; data-caption=&#34;Video Haystack. This figure compares Gemini 1.5 Pro with GPT-4V for the video needle-in-a-haystack task, where the models are given video clips of different lengths up to three hours of video and are asked to retrieve a secret word embedded as text at different points within the clip. All video clips are sampled at one frame-per-second (1 fps). The first pair of 10 × 50 haystack plots on the left compare Gemini 1.5 Pro with GPT-4V on the first hour of the documentary. The x-axis represents the video duration which ranges from 1.2 minutes to 1 hour, and the y-axis represents the depth, namely the relative offset of the needle (e.g., the top left cell represents providing the model with the first 1.2 minutes and randomly sampling a frame in the first ten percent of that trimmed video where the needle is inserted). A green cell indicates that the model successfully retrieved the needle, whereas a gray cell indicates an API error. Whereas the GPT-4V API supports video lengths only up to around the first 3 minutes of the full hour, Gemini 1.5 Pro successfully retrieves the secret word inserted at all depth percentages up to an hour of video, as shown by the all-green plot. Finally, the 10 × 10 grid on the right shows Gemini 1.5 Pro’s perfect retrieval capabilities across three full hours of video, constructed by concatenating two copies of the documentary back-to-back.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/video_niah.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Video Haystack. This figure compares Gemini 1.5 Pro with GPT-4V for the video needle-in-a-haystack task, where the models are given video clips of different lengths up to three hours of video and are asked to retrieve a secret word embedded as text at different points within the clip. All video clips are sampled at one frame-per-second (1 fps). The first pair of 10 × 50 haystack plots on the left compare Gemini 1.5 Pro with GPT-4V on the first hour of the documentary. The x-axis represents the video duration which ranges from 1.2 minutes to 1 hour, and the y-axis represents the depth, namely the relative offset of the needle (e.g., the top left cell represents providing the model with the first 1.2 minutes and randomly sampling a frame in the first ten percent of that trimmed video where the needle is inserted). A green cell indicates that the model successfully retrieved the needle, whereas a gray cell indicates an API error. Whereas the GPT-4V API supports video lengths only up to around the first 3 minutes of the full hour, Gemini 1.5 Pro successfully retrieves the secret word inserted at all depth percentages up to an hour of video, as shown by the all-green plot. Finally, the 10 × 10 grid on the right shows Gemini 1.5 Pro’s perfect retrieval capabilities across three full hours of video, constructed by concatenating two copies of the documentary back-to-back.
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/audio_niah.png&#34; data-caption=&#34;Audio Haystack. This figure presents the audio version of the needle-in-a-haystack experiment comparing Gemini 1.5 Pro and a combination of Whisper and GPT-4 Turbo. In this setting, the needle is a short segment of audio that is inserted within a very large audio segment (of up to 22 hours) containing concatenated audio clips. The task is to retrieve the &amp;lsquo;secret keyword&amp;rsquo; which is revealed in the needle. Red indicates that the model did not identify the keyword, whereas green indicates that the model identified the keyword correctly&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/audio_niah.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Audio Haystack. This figure presents the audio version of the needle-in-a-haystack experiment comparing Gemini 1.5 Pro and a combination of Whisper and GPT-4 Turbo. In this setting, the needle is a short segment of audio that is inserted within a very large audio segment (of up to 22 hours) containing concatenated audio clips. The task is to retrieve the &amp;lsquo;secret keyword&amp;rsquo; which is revealed in the needle. Red indicates that the model did not identify the keyword, whereas green indicates that the model identified the keyword correctly
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;However, with about 100 needles (with the model requiring to retrieve them all), the performance drops to 70% recall up to 128K tokens, and &amp;gt;60% recall up to 1M tokens. GPT-4 is 50% recall at 128k tokens (its maximum context).&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/multiple_niah.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/multiple_niah.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Gemini 1.5 Pro beats RAG-based systems for all modalities. Although this may not be cost efficient right now unless there are multiple queries and the long context can be prefix-cached.&lt;/p&gt;
&lt;p&gt;Excellent use of context for learning new skills. It was able to learn to translate from/to English to/from Kalamang, an extremely low-resource language (spoken by less than 200 people in the world), with just a single set of linguistic documentation, a dictionary, and ≈ 400 parallel sentences. That&amp;rsquo;s it!&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/kalamang.png&#34; data-caption=&#34;Given a reference grammar book and a bilingual wordlist (dictionary), Gemini 1.5 Pro is able to translate from English to Kalamang with similar quality to a human who learned from the same materials.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/kalamang.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Given a reference grammar book and a bilingual wordlist (dictionary), Gemini 1.5 Pro is able to translate from English to Kalamang with similar quality to a human who learned from the same materials.
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/kalamang_results.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/kalamang_results.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Gemini 1.5 Pro is exceeds Gemini 1.0 Ultra on text. However the latter is still better on vision and audio tasks, but the compute-efficiency and speed of shipping does suggest some ideas of distillation coming into the picture.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/results_comparison.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/results_comparison.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Figures 4 and 5 in the Technical report are super cool. In Figure 4, Gemini 1.5 is able to extract the exact scene with the page number from a textbook given a hand-drawn sketch as the prompt.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/fig4_paper.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/fig4_paper.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;In Figure 5 they show that it is able to do the same in videos as well. The model returns a detailed description and timestamp of the scene.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/fig5_paper.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/fig5_paper.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The cumulative average negative log-likelihood (NLL) as a function of token position in long documents follows a power-law quite accurately up to 1M tokens for long-documents and about 2M tokens for code but deviates at the 10M scale.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/pplx.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2024-02-18_gemini_pro_google/images/pplx.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Though it &lt;a href=&#34;https://youtu.be/wa0MT8OwHuk&#34;&gt;seems like the inference speeds are quite slow&lt;/a&gt; for long contexts at the moment, but the capabilities are absolutely insane!&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;An exciting future ahead!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
