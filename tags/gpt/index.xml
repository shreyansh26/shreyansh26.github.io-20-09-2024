<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>gpt | Shreyansh Singh</title>
    <link>https://shreyansh26.github.io/tags/gpt/</link>
      <atom:link href="https://shreyansh26.github.io/tags/gpt/index.xml" rel="self" type="application/rss+xml" />
    <description>gpt</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Shreyansh Singh 2021</copyright><lastBuildDate>Sun, 23 May 2021 16:44:32 +0530</lastBuildDate>
    <image>
      <url>https://shreyansh26.github.io/img/Shreyansh.jpg</url>
      <title>gpt</title>
      <link>https://shreyansh26.github.io/tags/gpt/</link>
    </image>
    
    <item>
      <title>Paper Summary #6 - Language Models are Unsupervised Multitask Learners</title>
      <link>https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/</link>
      <pubDate>Sun, 23 May 2021 16:44:32 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: Language Models are Unsupervised Multitask Learners&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf&#34;&gt;https://bit.ly/3vgaVJc&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever  &lt;br&gt;
&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/openai/gpt-2&#34;&gt;https://github.com/openai/gpt-2&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I also made an annotated version of the paper which you can find &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;The paper demonstrates that language models begin to learn NLP tasks like question answering, machine translation, reading comprehension and summarization without any explicit supervision. The results shown are obtained after training the model on a new dataset of millions of web pages called WebText. GPT-2 is a 1.5 billion parameter model that achieves SOTA on 7 out of 8 LM tasks in a zero-shot setting. The paper proves that it is possible to build NLP systems that learn to perform tasks from the naturally occurring demonstrations of the tasks in text.&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;The main motivation arises from the fact that current systems are narrow experts rather than competent generalists. There should be a shift to more general systems which can perform many tasks without the need to manually create and label a training dataset for each one. There have been examples of erratic behaviour of captioning models, reading comprehension systems and image classifiers due to the large variety of possible inputs which can&amp;rsquo;t be modeled using supervised approaches. There is a lack of generalization in current systems.&lt;/p&gt;
&lt;p&gt;Additionally, multitask NLP systems are still in a very early stage mostly due to the fact that it would require a large amount of very specific labeled data for the model to learn from. Although some models use a combination of unsupervised pretraining followed by supervised fine-tuning. GPT-2 wants to do away with any supervised training and show how language models perform in a zero-shot setting on a wide range of tasks.&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;
&lt;p&gt;Language modeling is usually framed as a unsupervised distribution estimation. It is modeled as a joint probability over the symbols. Due to the sequential order of natural text, this can be written as a product of the conditional probabilities.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/lm.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/lm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Instead of a simple conditional probability distribution for a single task &lt;em&gt;p(output&lt;/em&gt;|&lt;em&gt;input)&lt;/em&gt;. To make a system that can perform multiple tasks, the distribution should be conditioned on the task as well i.e., &lt;em&gt;p(output&lt;/em&gt;|&lt;em&gt;input, task)&lt;/em&gt;. This is usually implemented at the architecture level for example, by using task-specific encoders and decoders. It can also be performed by the language directly.  A translation task can be represented as &lt;tt&gt;(translate to french, english text, french text). &lt;/tt&gt; A reading comprehension can be written as &lt;tt&gt; (answer the question, document, question, answer) &lt;/tt&gt;.&lt;/p&gt;
&lt;p&gt;In principle, language modeling will be able to learn these tasks without any supervision of which symbols are to be predicted. Since the global minimum of the unsupervised objective is also the global minimum of the supervised objective (which is based on a subset of the sequence), hence the model can focus only on optimizing for the unsupervised objective. For this, very large models are required, however the learning is slower as compared to the explicitly supervised approaches.&lt;/p&gt;
&lt;p&gt;The authors believe that a large enough language model will begin to learn the tasks embedded within the natural language itself and won&amp;rsquo;t require any additional supervision. For example, given enough text, the model will learn what question answering is, without having to train on question-answering data specifically.&lt;/p&gt;
&lt;h3 id=&#34;dataset&#34;&gt;Dataset&lt;/h3&gt;
&lt;p&gt;A new dataset (self-curated), WebText was used by the authors to train the model. The dataset contained page contents of all the scraped outbound links from Reddit, from posts that received at least 3 karma. They performed HTML cleaning, de-duplication. Also, Wikipedia pages were removed as the test datasets of many of the downstream tasks had information from Wikipedia.&lt;/p&gt;
&lt;h3 id=&#34;input-representation&#34;&gt;Input Representation&lt;/h3&gt;
&lt;p&gt;BytePair encoding, which effectively interpolates between word level inputs for frequent symbol sequences and character level inputs for infrequent symbol sequences was used for tokenizing the corpus. The encoding in the paper was not performed on bytes but Unicode points. This increases the base vocabulary from 256 (in byte mode) to 130,000 (with Unicode).&lt;/p&gt;
&lt;p&gt;The BPE encoding allowed the authors to combine the benefits of word-level LMs with the generality of byte-level approaches. Also, since now the model can assign a probability to any Unicode string, so the LM will be able to be evaluated on any dataset regardless of the pre-processing, tokenization or vocabulary size.&lt;/p&gt;
&lt;hr&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;hr&gt;
&lt;h3 id=&#34;model&#34;&gt;Model&lt;/h3&gt;
&lt;p&gt;The model is a Transformer decoder architecture, very similar to GPT-1. You can find details &lt;a href=&#34;http://localhost:1313/post/2021-05-02_language_understanding_generative_pretraining/#task-specific-input-transformations&#34;&gt;here&lt;/a&gt;. Some modifications include&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Moving the layer norm to the input of each sub-block and adding an additional layer norm after the final self-attention block.&lt;/li&gt;
&lt;li&gt;A modified initialization which accounts for the accumulation on the residual path with model depth is also used.&lt;/li&gt;
&lt;li&gt;The weights of the residual layers are scaled by 1/sqrt(&lt;em&gt;N&lt;/em&gt;) where &lt;em&gt;N&lt;/em&gt; is the number of residual layers.&lt;/li&gt;
&lt;li&gt;The vocabulary is expanded to 50,257.&lt;/li&gt;
&lt;li&gt;The context size from 512 to 1024 tokens and a larger batch size of 512 is used.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;h3 id=&#34;language-modeling&#34;&gt;Language Modeling&lt;/h3&gt;
&lt;p&gt;This is the primary task the model was trained for. In this category, the model is evaluated on its perplexity score. Some invertible de-tokenizers had to be used on the test set as not all types of text are seen during training for example, standardized text, having tokenization artifacts like shuffled sentences and &amp;lt;UNK&amp;gt; string. A de-tokenizer that removes those artifacts improves the score by 2.5 - 5 perplexity points.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Large improvements were seen on small datasets like Penn Treebank and WikiText-2.&lt;/li&gt;
&lt;li&gt;Large improvements were also seen on LAMBADA and Children&amp;rsquo;s Book Test where long-term dependency had to be measured.&lt;/li&gt;
&lt;li&gt;It failed to perform better than existing approaches for the One Billion Word Benchmark, probably because of it being the largest dataset and having the most destructive pre-processing - the sentence level shuffling which removes all range structure. this makes it difficult for GPT-2 to perform well on it.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;childrens-book-test-cbt&#34;&gt;Children’s Book Test (CBT)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;CBT task was to predict which of 10 possible choices for an omitted word is correct.&lt;/li&gt;
&lt;li&gt;Data overlap analysis showed one of the CBT test set books, The Jungle Book by Rudyard Kipling, is in WebText, so the authors report results on the validation set which has no significant overlap.&lt;/li&gt;
&lt;li&gt;GPT-2 achieved SOTA results on both prediction of common nouns (CBT-CN) and the prediction of named entities (CBT-NE).&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;lambada&#34;&gt;LAMBADA&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;In LAMBADA, the task is to predict the final word of sentences which require at least 50 tokens of context for a human to successfully predict.&lt;/li&gt;
&lt;li&gt;GPT-2 improved the perplexity from 99.8 (existing SOTA) to 8.6 and the accuracy from 19% to 52.66%.&lt;/li&gt;
&lt;li&gt;Choosing a stopping filter was difficult as many times GPT-2 predicted valid continuations of the sentence but not valid final words. A stop-word filter for this helped improve the accuracy a bit.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/reslm.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/reslm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;winograd-schema-challenge&#34;&gt;Winograd Schema Challenge&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The Winograd Schema challenge aims to measure the capability of a system to perform commonsense reasoning by measuring its ability to resolve ambiguities in the text.&lt;/li&gt;
&lt;li&gt;GPT-2 improves the state of the art accuracy by 7%, achieving 70.70%.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/reswino.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/reswino.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;reading-comprehension&#34;&gt;Reading Comprehension&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The CoQA dataset consists of documents from different domains with natural dialogues in the form of questions and answers. The task tests the reading comprehension capabilities and also the ability to answer questions based on conversation history.&lt;/li&gt;
&lt;li&gt;GPT-2 was evaluated on this task by conditioning on the document, the conversation history and the final token.&lt;/li&gt;
&lt;li&gt;This matched or exceeded the results from 3 of 4 baselines. Also, these models were trained on the 127,000+ question-answer pairs of the training data, which GPT-2 didn&amp;rsquo;t look at.&lt;/li&gt;
&lt;li&gt;GPT-2 didn&amp;rsquo;t perform as well as the BERT SOTA but the score is still impressive since it is a completely unsupervised model.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;summarization&#34;&gt;Summarization&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;GPT-2 didn&amp;rsquo;t perform well on the summarization task on the CNN and Daily Mail dataset.&lt;/li&gt;
&lt;li&gt;It just barely outperforms selecting 3 random sentences from the article.&lt;/li&gt;
&lt;li&gt;Removing the hint reduced the score by 6.4 points indicating that task-specific behaviour was being invoked by natural language.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/ressumm.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/ressumm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;translation&#34;&gt;Translation&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The model was conditioned in the following manner - &lt;tt&gt; english sentence = french sentence &lt;/tt&gt; and then after the prompt of &lt;tt&gt; english sentence = &lt;/tt&gt;, the greedy decoding and the first generated sentence was used as the translation.&lt;/li&gt;
&lt;li&gt;However, the performance of the model was very poor, even worse than a word-by-word substitution of the words with their translation.&lt;/li&gt;
&lt;li&gt;However, the French to English task was a bit better, surpassing some unsupervised methods but very far from the best unsupervised method.&lt;/li&gt;
&lt;li&gt;The result is interesting as almost all non-English text was removed from WebText during preprocessing. And the best unsupervised method used 500x more French data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;question-answering&#34;&gt;Question Answering&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;The context of the language model is seeded with example question-answer pairs which helps the model infer the short answer style of the dataset.&lt;/li&gt;
&lt;li&gt;However the performance is very poor and the model only answers 4.1% of the questions correctly.&lt;/li&gt;
&lt;li&gt;It was seen that the smaller models could answer around 1% of the questions, indicating that a larger model size helped.&lt;/li&gt;
&lt;li&gt;GPT-2 has an accuracy of 63.1% on the 1% of questions it is most confident in.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resqa.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resqa.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;generalization-vs-memorization&#34;&gt;Generalization vs Memorization&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;In general, it was seen that all the major datasets have some amount of overlap between the train and the test datasets.&lt;/li&gt;
&lt;li&gt;Even CIFAR-10 has a 3.3% overlap of train and test images.&lt;/li&gt;
&lt;li&gt;To test this, Bloom filters containing 8-grams of WebText training set tokens were created.&lt;/li&gt;
&lt;li&gt;These Bloom filters helped to calculate, given a dataset, the percentage of 8-grams from that dataset that are also found in the WebText training set.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resoverlap.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resoverlap.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;The overlap between the datasets&amp;rsquo; train and test set was also very high in some.&lt;/li&gt;
&lt;li&gt;And on analysis, it was seen that removing these overlaps from the train set, resulted in a slight drop in performance across tasks.&lt;/li&gt;
&lt;li&gt;The authors suggest fuzzy string matching or n-gram overlap based de-duplication as important sanity checks when splitting NLP datasets to create the train and test set.&lt;/li&gt;
&lt;li&gt;The performance on both the training and test sets of WebText are similar and improve together as model size is increased&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resown.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-23_language_models_unsupervised_multitask_learners_gpt2/images/resown.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The authors conclude by saying that since finetuning had helped GPT, the same could be tried for GPT-2 as well for benchmarks such as decaNLP and GLUE. Also, it may be helpful because as BERT pointed out, the inefficiency of unidirectional representations can not absolutely be eliminated by more training data and model size, as could be seen in tasks like summarization, translation and question answering.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT2.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is all for now!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summary #3 - Improving Language Understanding by Generative Pre-Training</title>
      <link>https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/</link>
      <pubDate>Sun, 02 May 2021 13:42:14 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: Improving Language Understanding by Generative Pre-Training&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf&#34;&gt;https://bit.ly/3xITvGP&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Blog&lt;/strong&gt;: &lt;a href=&#34;https://openai.com/blog/language-unsupervised/&#34;&gt;https://openai.com/blog/language-unsupervised/&lt;/a&gt; &lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever&lt;br&gt;
&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/openai/finetune-transformer-lm&#34;&gt;https://bit.ly/3gUFrUX&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;The paper proposes a semi-supervised technique that shows better performance on a wide variety of tasks like textual entailment, question answering, semantic similarity text classification by using a single task-agnostic model. The model can overcome the constraints of the small amount of annotated data for these specific tasks by performing an unsupervised generative-pretraining of a language model on a large diverse text corpus followed by supervised discriminative fine-tuning on each specific task. The pretraining model remains the same for all the tasks. Only a small, task-aware input adaptation is required when performing the fine-tuning. The model significantly improved the state-of-the-art (at the time) in 9 of the 12 tasks studied.&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;Most deep learning models require a substantial amount of data, which makes them difficult to train for tasks in which there is a dearth of good quality annotated data. Historically, pre-trained word embeddings have been used for such cases but the word-level information in itself is sometimes not enough for many of the complex tasks.&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;
&lt;p&gt;The goal of the model is to learn a universal representation that transfers with little adaptation to a wide range of tasks. The paper assumes access to a large corpus of unlabeled text and several datasets with manually annotated training examples (the target tasks). The unlabeled corpus and the annotated datasets need not be in the same domain.&lt;/p&gt;
&lt;p&gt;A two-stage training procedure is used. First, a language modelling (LM) objective is used on the unlabeled data to learn the initial parameters of the model. Next, these parameters are adapted to a target task using the corresponding supervised objective.&lt;/p&gt;
&lt;p&gt;A Transformer (specifically a Transfomer decoder) is used as the underlying architecture. Transformers work better than LSTMs (shown in the results as well) because they can capture long-term dependencies well which results in robust transfer performance across diverse tasks.  Furthermore, during the transfer, as mentioned above, task-specific input adaptations are used which process the structured text input as a single contiguous sequence of tokens. This is something very interesting and will be shown in the subsequent sections.&lt;/p&gt;
&lt;h3 id=&#34;unsupervised-pre-training&#34;&gt;Unsupervised pre-training&lt;/h3&gt;
&lt;p&gt;A standard forward LM objective is used to maximise the likelihood -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/unsupervised-lm.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/unsupervised-lm.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here, , &lt;em&gt;U&lt;/em&gt; is the corpus of tokens {&lt;em&gt;u&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;,&amp;hellip; &lt;em&gt;u&lt;/em&gt;&lt;sub&gt;n&lt;/sub&gt;}, &lt;em&gt;k&lt;/em&gt; is the context window size and the conditional probability &lt;em&gt;P&lt;/em&gt;  is modeled using a network with parameters Θ. SGD is used to learn the parameters. The model uses a multi-layer Transformer decoder. The multi-head self-attention is applied over the input context tokens. This is followed by position-wise feedforward layers to produce an output probability distribution over the target tokens.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/probcalc.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/probcalc.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here &lt;em&gt;U&lt;/em&gt; is (u&lt;sub&gt;-k&lt;/sub&gt;,&amp;hellip;, u&lt;sub&gt;-1&lt;/sub&gt;) which is the context vector of tokens, &lt;em&gt;n&lt;/em&gt; is the number of layers, &lt;em&gt;W&lt;/em&gt;&lt;sub&gt;e&lt;/sub&gt; is the token embedding matrix and &lt;em&gt;W&lt;/em&gt;&lt;sub&gt;p&lt;/sub&gt; is the position embedding matrix.&lt;/p&gt;
&lt;h3 id=&#34;supervised-fine-tuning&#34;&gt;Supervised fine-tuning&lt;/h3&gt;
&lt;p&gt;After the training of the model with optimization &lt;em&gt;L&lt;/em&gt;&lt;sub&gt;1&lt;/sub&gt;, the parameters are now adapted to the supervised target task. The labelled dataset is denoted by &lt;em&gt;C&lt;/em&gt;, where each instance is a sequence of input tokens, &lt;em&gt;x&lt;/em&gt;&lt;sup&gt;1&lt;/sup&gt;,&amp;hellip;,&lt;em&gt;x&lt;/em&gt;&lt;sup&gt;m&lt;/sup&gt;, along with a label &lt;em&gt;y&lt;/em&gt;. The inputs are passed through the pre-trained model to obtain the final transformer block&amp;rsquo;s activation &lt;em&gt;h&lt;/em&gt;&lt;sub&gt;l&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt;, which is then fed into an added linear output layer with parameters &lt;em&gt;W&lt;/em&gt;&lt;sub&gt;y&lt;/sub&gt; to predict &lt;em&gt;y&lt;/em&gt;.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/fintune1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/fintune1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The objective to be maximized is as follows&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/fintune2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/fintune2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Using an LM objective as an auxiliary objective to the finetuning helped to improve the generalization of the supervised model and make it converge faster.&lt;/p&gt;
&lt;p&gt;The overall objective can be written as -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/objective-fin.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/objective-fin.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;task-specific-input-transformations&#34;&gt;Task-specific input transformations&lt;/h3&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/input-transform.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/input-transform.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Since the pretrained model is trained on a contiguous sequence of texts, to handle the inputs of the various tasks, certain input transformations are needed as shown above. These transformations help to avoid making extensive changes to the architecture across tasks.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Textual Entailment&lt;/strong&gt; - The premise (&lt;em&gt;p&lt;/em&gt;) and the hypothesis (&lt;em&gt;h&lt;/em&gt;) sequences are concatenated with a delimiter token in between.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Similarity&lt;/strong&gt; - Since there is no inherent ordering of the two sequences being compared, the input sequence is modified to contain both possible sentence orderings (with a delimiter in between). Each of these concatenated sequences is processed independently to produce two sequence representations &lt;em&gt;h&lt;/em&gt;&lt;sub&gt;l&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt; which are then element-wise added before feeding to the linear output layer.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Question Answering&lt;/strong&gt; - This one is interesting. For a given context document &lt;em&gt;z&lt;/em&gt;, question &lt;em&gt;q&lt;/em&gt; and a set of possible answers {&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;k&lt;/sub&gt;}. The document and question are concatenated with each of the possible answers, with a delimiter token in between [&lt;em&gt;z&lt;/em&gt;; &lt;em&gt;q&lt;/em&gt;;$;&lt;em&gt;a&lt;/em&gt;&lt;sub&gt;k&lt;/sub&gt;]. Each of these sequences is processed independently by the model and then normalized by a softmax layer to produce an output distribution over possible answers.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The model specifications for the experimental setup are shown below -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/setup.PNG&#34; data-caption=&#34;Experimental Setup&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/setup.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Experimental Setup
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;The datasets that were used are listed below -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/datasets.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/datasets.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Natural Language Inference&lt;/strong&gt; - This task is challenging due to the presence of a wide variety of phenomena like lexical entailment, coreference, and lexical and syntactic ambiguity. The model performs better than the state-of-the-art in 4 (MNLI, QNLI, SNLI, SciTail) out of 5 datasets.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/nli.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/nli.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Question Answering and Commonsense Reasoning&lt;/strong&gt; - The RACE dataset (passages with associated questions from middle and high school exams) and Story Cloze dataset (selecting correct ending to multi-sentence stories from two options) were used. The model outperformed the baseline on both these datasets.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/qa.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/qa.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Semantic Similarity&lt;/strong&gt; - The challenges in this task are recognizing rephrasing, negation, and handling ambiguity. The model performs better on 2 (QQP and STS-B) of the 3 datasets.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Classification&lt;/strong&gt; - The model performs better on both Corpus of Linguistic Accepttability (CoLA) dataset and is at par with the state-of-the-art results on the Stanford Sentiment Treebank dataset.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/classification.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/classification.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Key points from the analysis section -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;More the number of layers that are transferred from the pretrained model to the supervised target task, the better is the performance on the target tasks.&lt;/li&gt;
&lt;li&gt;To understand whether the unsupervised pre-training is effective or not, zero-shot testing was also performed i.e., using the pre-trained model directly without any finetuning. The model performance is stable and steadily increases over training suggesting that the generative pre-training supports the learning of a wide variety of task-relevant functionality. LSTMs exhibit higher variance in their zero-shot performance.
The testing and input transformations for using the pretrained model directly are explained below -













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/zeroshot.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/zeroshot.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;
&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/trend.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-05-02_language_understanding_generative_pretraining/images/trend.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;From the ablation studies, the authors show that the auxiliary LM objective helps on the NLI tasks and QQP (Quora Question Pairs data).&lt;/li&gt;
&lt;li&gt;Overall, larger datasets benefit from the auxiliary objective more than the smaller datasets.&lt;/li&gt;
&lt;li&gt;In general, the Transformer architecture performs better than a 2048 unit single layer LSTM model (if the Transformer in the pretraining model is replaced by an LSTM) on all datasets except the MRPC (Microsoft Paraphrase Corpus for semantic similarity) dataset.&lt;/li&gt;
&lt;li&gt;On comparing this model with the same transformer architecture trained in a supervised manner, it is observed that the model with pre-training performs better. This consistent for all the tasks mentioned in the paper, suggesting that pre-training helps to capture important linguistic information which is not captured when training with a supervised approach alone.&lt;/li&gt;
&lt;/ul&gt;
&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/GPT1.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is all for now!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
