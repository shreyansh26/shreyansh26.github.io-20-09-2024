<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>transformers | Shreyansh Singh</title>
    <link>https://shreyansh26.github.io/tags/transformers/</link>
      <atom:link href="https://shreyansh26.github.io/tags/transformers/index.xml" rel="self" type="application/rss+xml" />
    <description>transformers</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Â© Shreyansh Singh 2023</copyright><lastBuildDate>Mon, 10 Oct 2022 14:57:33 +0530</lastBuildDate>
    <image>
      <url>https://shreyansh26.github.io/img/Shreyansh.jpg</url>
      <title>transformers</title>
      <link>https://shreyansh26.github.io/tags/transformers/</link>
    </image>
    
    <item>
      <title>Paper Summary #7 - Efficient Transformers: A Survey</title>
      <link>https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/</link>
      <pubDate>Mon, 10 Oct 2022 14:57:33 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: Efficient Transformers: A Survey&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://arxiv.org/abs/2009.06732&#34;&gt;https://arxiv.org/abs/2009.06732&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Yi Tay, Mostafa Dehghani, Dara Bahri, Donald Metzler&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I wanted to summarize this paper for a long time now because of the immense amount of information in this paper. Thanks to the &lt;a href=&#34;https://cohere.for.ai/&#34;&gt;Cohere For AI&lt;/a&gt; community for having a session on this paper which made me revisit this.&lt;/p&gt;
&lt;h1 id=&#34;what&#34;&gt;What?&lt;/h1&gt;
&lt;p&gt;This is a survey paper on the various memory-efficiency based improvements on the original Transformers architecture by Vaswani et al. But wait, for those unaware, how is the Transformers architecture inefficient?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The attention operation has a quadratic complexity over the sequence length L, also sometimes represented using N (since each token attends to other set of tokens in the sequence)&lt;/li&gt;
&lt;li&gt;The Attention operation of Q*K&lt;sup&gt;T&lt;/sup&gt; uses N&lt;sup&gt;2&lt;/sup&gt; time and memory. Here (in no-batching case) Q, K, V (query, key and value matrices) have dimensions &lt;i&gt;N x d &lt;/i&gt; where d is the dimension of query, key and value vectors.&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/attention.PNG&#34; data-caption=&#34;Attention calculation&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/attention.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Attention calculation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h1 id=&#34;glossary&#34;&gt;Glossary&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#low-rank-methods&#34;&gt;Low-rank Methods&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#linformer---httpsarxivorgabs200604768httpsarxivorgabs200604768&#34;&gt;Linformer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#performer---httpsarxivorgabs200914794httpsarxivorgabs200914794&#34;&gt;Perfomer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#learnable-patterns&#34;&gt;Learnable Patterns based methods&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#clustered-attention---httpsarxivorgabs200704825httpsarxivorgabs200704825--httpsclustered-transformersgithubiobloghttpsclustered-transformersgithubioblog&#34;&gt;Clustered Attention&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#reformer---httpsarxivorgabs200104451httpsarxivorgabs200104451&#34;&gt;Reformer&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#memory-based&#34;&gt;Memory-based methods&lt;/a&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#big-bird-httpsarxivorgabs200714062httpsarxivorgabs200714062--httpshuggingfacecoblogbig-birdhttpshuggingfacecoblogbig-bird&#34;&gt;Big Bird&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#complexity-summary-of-various-models&#34;&gt;Complexity summary of various models&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;memory-efficient-transformers&#34;&gt;Memory-Efficient Transformers&lt;/h1&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/arch-vaswani.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/arch-vaswani.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/summary.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/summary.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;low-rank-methods&#34;&gt;Low-Rank methods&lt;/h2&gt;
&lt;h3 id=&#34;linformer---httpsarxivorgabs200604768httpsarxivorgabs200604768&#34;&gt;Linformer - &lt;a href=&#34;https://arxiv.org/abs/2006.04768&#34;&gt;https://arxiv.org/abs/2006.04768&lt;/a&gt;&lt;/h3&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/linformer.png&#34; data-caption=&#34;Left and bottom-right show architecture and example of our proposed multihead linear self-attention. Top right shows inference time vs. sequence length for various Linformer models.&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/linformer.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Left and bottom-right show architecture and example of our proposed multihead linear self-attention. Top right shows inference time vs. sequence length for various Linformer models.
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;In Linformer, the original Key and Value matrices are projected from &lt;i&gt;(N x d)&lt;/i&gt; to a reduced &lt;i&gt;(k x d)&lt;/i&gt;.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/linformer-dets.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/linformer-dets.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The above operations only require &lt;code&gt;O(n*k)&lt;/code&gt; time and space complexity. Thus, if we can choose a very small projected dimension k, such that k &amp;lt; &amp;lt; N, then we can significantly reduce the memory and space consumption.&lt;/p&gt;
&lt;h3 id=&#34;performer---httpsarxivorgabs200914794httpsarxivorgabs200914794&#34;&gt;Performer - &lt;a href=&#34;https://arxiv.org/abs/2009.14794&#34;&gt;https://arxiv.org/abs/2009.14794&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;The goal in the Performer paper was to reduce the complexity of attention calculation (Q * K&lt;sup&gt;T&lt;/sup&gt;) * V of O(L&lt;sup&gt;2&lt;/sup&gt; * d) to O (L * d&lt;sup&gt;2&lt;/sup&gt;) by transforming the order of operations and using a kernel operation to approximate the softmax operation so that the order of operations can be changed.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/performer.png&#34; data-caption=&#34;An overview from the paper&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/performer.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    An overview from the paper
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;From, a &lt;a href=&#34;https://chiaracampagnola.io/2020/10/29/from-transformers-to-performers/&#34;&gt;great blog on the Performer paper&lt;/a&gt; -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/performer-dets.png&#34; data-caption=&#34;Change of operation order&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/performer-dets.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Change of operation order
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/performer-dets2.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/performer-dets2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/performer-dets3.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/performer-dets3.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;learnable-patterns&#34;&gt;Learnable Patterns&lt;/h2&gt;
&lt;h3 id=&#34;clustered-attention---httpsarxivorgabs200704825httpsarxivorgabs200704825--httpsclustered-transformersgithubiobloghttpsclustered-transformersgithubioblog&#34;&gt;Clustered Attention - &lt;a href=&#34;https://arxiv.org/abs/2007.04825&#34;&gt;https://arxiv.org/abs/2007.04825&lt;/a&gt; + &lt;a href=&#34;https://clustered-transformers.github.io/blog/&#34;&gt;https://clustered-transformers.github.io/blog/&lt;/a&gt;&lt;/h3&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/clusteredatt.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/clusteredatt.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;First cluster the queries into  non-overlapping clusters.&lt;/li&gt;
&lt;li&gt;Attention weights A&lt;sup&gt;c&lt;/sup&gt; are computed using the centroids instead of computing them for every query&lt;/li&gt;
&lt;li&gt;Use clustered attention weights A&lt;sup&gt;c&lt;/sup&gt; to compute new Values V&lt;sup&gt;c&lt;/sup&gt;&lt;/li&gt;
&lt;li&gt;Use the same attention weights and new values for queries that belong to same cluster.&lt;/li&gt;
&lt;li&gt;Computational complexity becomes &lt;code&gt;O(N * C * max (D&lt;sub&gt;k&lt;/sub&gt; * D&lt;sub&gt;v&lt;/sub&gt;))&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;They also propose an Improved Clustered Attention in their blog. The complexity comaprisons are here -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/clusteredatt-dets.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/clusteredatt-dets.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h3 id=&#34;reformer---httpsarxivorgabs200104451httpsarxivorgabs200104451&#34;&gt;Reformer - &lt;a href=&#34;https://arxiv.org/abs/2001.04451&#34;&gt;https://arxiv.org/abs/2001.04451&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;Uses the concept of Locality sensitive hashing (LSH) attention, where the goal is to not store the entire Q * K&lt;sup&gt;T&lt;/sup&gt; matrix but only the softmax(Q * K&lt;sup&gt;T&lt;/sup&gt;), which is dominated by the largest elements in a typically sparse matrix. For each query q we only need to pay attention to the keys k that are closest to q. For example, if K is of length 64K, for each q we could only consider a small subset of the 32 or 64 closest keys. So the attention mechanism finds the nearest neighbor keys of a query but in an inefficient manner.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/reformer-lsh.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/reformer-lsh.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;Calcluate LSH hashes of Queries and Keys (Q and K)&lt;/li&gt;
&lt;li&gt;Make chunks and compute attention only for vectors in the same bucket&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The paper also introduces the concept of Reversible residual networks (RevNets). In the residual connections in Transformers, one needs to store the activations in each layer in memory in order to calculate gradients during backpropagation. RevNets are composed of a series of reversible blocks. In RevNet, each layerâs activations can be reconstructed exactly from the subsequent layerâs activations, which enables us to perform backpropagation without storing the activations in memory.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/reformer-revnet.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/reformer-revnet.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Reformer applies the RevNet idea to the Transformer by combining the attention and feed-forward layers inside the RevNet block. Now F becomes an attention layer and G becomes the feed-forward layer:&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/reformer-revnet2.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/reformer-revnet2.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The reversible residual layers allows storing activations only once during the training process instead of N times.&lt;/p&gt;
&lt;p&gt;The memory complexity of Reformer is &lt;code&gt;O(N log N)&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;memory-based&#34;&gt;Memory-based&lt;/h2&gt;
&lt;h3 id=&#34;big-bird-httpsarxivorgabs200714062httpsarxivorgabs200714062--httpshuggingfacecoblogbig-birdhttpshuggingfacecoblogbig-bird&#34;&gt;Big Bird &lt;a href=&#34;https://arxiv.org/abs/2007.14062&#34;&gt;https://arxiv.org/abs/2007.14062&lt;/a&gt; + &lt;a href=&#34;https://huggingface.co/blog/big-bird&#34;&gt;https://huggingface.co/blog/big-bird&lt;/a&gt;&lt;/h3&gt;
&lt;p&gt;BigBird relies on block sparse attention and can handle sequences up to a length of 4096 at a much lower computational cost compared to BERT. It has achieved SOTA on various tasks involving very long sequences such as long documents summarization, question-answering with long contexts.&lt;/p&gt;
&lt;p&gt;BigBird proposes three ways of allowing long-term attention dependencies while staying computationally efficient -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Global attention&lt;/strong&gt; - Introduce some tokens which will attend to every token and which are attended by every token. The authors call this the &amp;lsquo;internal transformer construction (ITC)&amp;rsquo; in which a subset of indices is selected as global tokens. This can be interpreted as a model-memory-based approach.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sliding attention&lt;/strong&gt; - Tokens close to each other, attend together. In BigBird, each query attends to w/2 tokens to the left and w/2 tokens to the right. This corresponds to a fixed pattern (FP) approach.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Random attention&lt;/strong&gt; - Select some tokens randomly which will transfer information by transferring to other tokens which in turn can transfer to other tokens. This may reduce the cost of information travel from one token to other. Each query attends to r random keys. This pattern is fixed&lt;/li&gt;
&lt;/ul&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/bigbird-graph.gif&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/bigbird-graph.gif&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;BigBird block sparse attention is a combination of sliding, global &amp;amp; random connections (total 10 connections) as shown in gif above. While a graph of normal attention (bottom) will have all 15 connections (note: total 6 nodes are present). One can simply think of normal attention as all the tokens attending globally.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/bigbird-full.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/bigbird-full.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The attention calculation in BigBird is slightly complex and I would refer to the &lt;a href=&#34;https://huggingface.co/blog/big-bird#bigbird-block-sparse-attention&#34;&gt;Huggingface blog&lt;/a&gt; for it -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/bigbird-attention-gif.gif&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/bigbird-attention-gif.gif&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;&lt;i&gt;blue -&amp;gt; global blocks, red -&amp;gt; random blocks, orange -&amp;gt; sliding blocks&lt;/i&gt;&lt;/p&gt;
&lt;p&gt;The memory complexity of the self-attention is linear, i.e., &lt;code&gt;O(n)&lt;/code&gt;. The BigBird model does not introduce new parameters beyond the Transformer model.&lt;/p&gt;
&lt;h2 id=&#34;complexity-summary-of-various-models&#34;&gt;Complexity Summary of various models&lt;/h2&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/complexity-summary.png&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2022-10-10_efficient_transformers_survey/images/complexity-summary.png&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;hr&gt;
&lt;p&gt;There are many more papers discussed in the survey. I will add their summaries here as I go through them.&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt;Â &lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Paper Summary #1 - Attention Is All You Need</title>
      <link>https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/</link>
      <pubDate>Sun, 18 Apr 2021 16:57:49 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/</guid>
      <description>&lt;p&gt;&lt;strong&gt;Paper&lt;/strong&gt;: Attention Is All You Need&lt;br&gt;
&lt;strong&gt;Link&lt;/strong&gt;: &lt;a href=&#34;https://bit.ly/3aklLFY&#34;&gt;https://bit.ly/3aklLFY&lt;/a&gt;&lt;br&gt;
&lt;strong&gt;Authors&lt;/strong&gt;: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin&lt;br&gt;
&lt;strong&gt;Code&lt;/strong&gt;: &lt;a href=&#34;https://github.com/tensorflow/tensor2tensor&#34;&gt;https://github.com/tensorflow/tensor2tensor&lt;/a&gt;&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id=&#34;what&#34;&gt;What?&lt;/h2&gt;
&lt;p&gt;Proposes Transformers, a new simple architecture for sequence transduction that uses only an attention mechanism and does not use any kind of recurrence or convolution. This model achieves SOTA (at the time) on the WMT 2014 English-to-French translation task with a score of 41.0 BLEU. Also beats the existing best results on the WMT 2014 English-to-German translation task with a score of 28.4 BLEU. The training cost is also much less than the best models chosen in the paper (at the time).&lt;/p&gt;
&lt;h2 id=&#34;why&#34;&gt;Why?&lt;/h2&gt;
&lt;p&gt;Existing recurrent models like RNNs, LSTMs or GRUs work sequentially. They align the positions to steps in computation time. They generate a sequence of hidden states as a function of the previous hidden state and the input for the current position. But sequential computation has constraints. They are not easily parallelizable which is required when the sequence lengths become large. The Transformer model eschews recurrence and allows for more parallelization and requires less training time to achieve SOTA in the machine translation task.&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How?&lt;/h2&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/arch.PNG&#34; data-caption=&#34;Detailed Transformer Architecture&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/arch.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Detailed Transformer Architecture
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The model is auto-regressive, it consumes the previously generated symbols as additional input when generating the next.&lt;/p&gt;
&lt;h3 id=&#34;encoder&#34;&gt;Encoder&lt;/h3&gt;
&lt;p&gt;The figure above shows just one layer of the encoder on the left. There are &lt;code&gt;N=6&lt;/code&gt; such layers. Each layer has two sub-layers - a multi-head self-attention layer and a position-wise fully connected feed-forward network. &lt;a href=&#34;https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf&#34;&gt;Residual connections&lt;/a&gt; and &lt;a href=&#34;https://arxiv.org/abs/1607.06450&#34;&gt;layer normalization&lt;/a&gt; is used for each sub-layer.&lt;/p&gt;
&lt;h3 id=&#34;decoder&#34;&gt;Decoder&lt;/h3&gt;
&lt;p&gt;This also has &lt;code&gt;N=6&lt;/code&gt; stacked layers. The architecture diagram shows one layer of the decoder on the right. Each layer has three sub-layers. Two of them are the same as the encoder. The third layer performs multi-head attention over the output of the encoder stack. This is modified to prevent positions from attending to subsequent positions. Additionally, the output embeddings are also offset by one position. These features ensure that the predictions for a position depend only on the known outputs for positions before it.&lt;/p&gt;
&lt;h3 id=&#34;attention&#34;&gt;Attention&lt;/h3&gt;
&lt;p&gt;The paper uses a modified dot product attention, and it is called &amp;ldquo;Scaled Dot Product Attention&amp;rdquo;. Given queries and keys of dimension d&lt;sub&gt;k&lt;/sub&gt; and values of dimension d&lt;sub&gt;v&lt;/sub&gt;, the attention matrix is calculated as shown below.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/attention.PNG&#34; data-caption=&#34;Attention Matrix Calculation&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/attention.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Attention Matrix Calculation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;Since, for large values of d&lt;sub&gt;k&lt;/sub&gt; the dot product grows large in magnitude, it pushes the softmax function into regions where it has extremely small gradients. The scaling of 1/sqrt(d&lt;sub&gt;k&lt;/sub&gt;) is done to avoid the problem of vanishing gradients.&lt;/p&gt;
&lt;p&gt;Multi-Head attention allows computing this attention in parallel. This helps to focus on different positions. Secondly, it also helps to attend to information from different subspaces due to the more number of attention heads.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/multihead-attention.PNG&#34; data-caption=&#34;Multihead Attention Calculation&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/multihead-attention.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Multihead Attention Calculation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;The paper uses &lt;code&gt;h=8&lt;/code&gt; parallel attention layers or heads. The reduced dimension of each head compensates for the more number of heads and hence the computational cost remains the same as with single-head attention with full dimensionality.&lt;/p&gt;
&lt;p&gt;Applications of multi-head attention in the paper are given below -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/application-attention.PNG&#34; data-caption=&#34;Application of multi-head attention in the model&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/application-attention.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Application of multi-head attention in the model
  &lt;/figcaption&gt;


&lt;/figure&gt;














&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/multihead-attention-fig.PNG&#34; data-caption=&#34;Pictorial representaion of Multi-head attention&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/multihead-attention-fig.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Pictorial representaion of Multi-head attention
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;h3 id=&#34;position-wise-feed-forward-networks&#34;&gt;Position-wise Feed-Forward Networks&lt;/h3&gt;
&lt;p&gt;The FFN sub-layer shown in the encoder and decoder architecture is a 2-hidden layer FC FNN with a ReLU activation in between.&lt;/p&gt;
&lt;h3 id=&#34;positional-encodings&#34;&gt;Positional Encodings&lt;/h3&gt;
&lt;p&gt;Positional encodings are injected (added) to the input embeddings at the bottom of the encoder and decoder stack to add some information about the relative order of the tokens in the sequence. The positional encodings have the same dimension as the input embeddings so that they can be added.
For position &lt;em&gt;pos&lt;/em&gt; and dimension &lt;em&gt;i&lt;/em&gt; the paper uses the following positional embeddings -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/positional.PNG&#34; data-caption=&#34;Positional Encoding calculation&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/positional.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Positional Encoding calculation
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;p&gt;This choice allows the model to easily learn by the relative positions. The learned positional embeddings also perform about the same as the sinusoidal version. The sinusoidal version may allow the model to extrapolate to sequence lengths longer than the ones encountered in training.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/experiments.PNG&#34; data-caption=&#34;Experimental results when varying parameters&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/experiments.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Experimental results when varying parameters
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;ul&gt;
&lt;li&gt;Form (A), it can be seen that single-head attention is slightly worse than the best setting. The quality also drops off with too many heads.&lt;/li&gt;
&lt;li&gt;(B) shows that reducing the attention key size &lt;i&gt;d&lt;sub&gt;k&lt;/sub&gt;&lt;/i&gt; hurts model quality.&lt;/li&gt;
&lt;li&gt;In (C) and (D), it is visible that bigger models are better and dropout helps in avoiding overfitting.&lt;/li&gt;
&lt;li&gt;(E) shows that sinusoidal positional encoding when replaced with learned positional embeddings also does not lead to a loss in quality&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For the base models, the authors used a single model obtained by averaging the last 5 checkpoints, which were written at 10-minute intervals. The big models were averaged over the last 20 checkpoints. Beam search with a beam size of 4 and length penalty Î± = 0.6. The maximum output length during inference is set to input length +50, but if it is possible, the model terminates early.&lt;/p&gt;
&lt;p&gt;The performance comparison with the other models is shown below -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/results.PNG&#34; data-caption=&#34;Model performance&#34;&gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-04-18_attention_is_all_you_need/images/results.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;


  
  
  &lt;figcaption&gt;
    Model performance
  &lt;/figcaption&gt;


&lt;/figure&gt;

&lt;hr&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/Attention%20Is%20All%20You%20Need.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This is all for now!&lt;/p&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt;Â &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
