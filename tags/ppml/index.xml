<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ppml | Shreyansh Singh</title>
    <link>https://shreyansh26.github.io/tags/ppml/</link>
      <atom:link href="https://shreyansh26.github.io/tags/ppml/index.xml" rel="self" type="application/rss+xml" />
    <description>ppml</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Shreyansh Singh 2021</copyright><lastBuildDate>Sat, 18 Dec 2021 00:16:23 +0530</lastBuildDate>
    <image>
      <url>https://shreyansh26.github.io/img/Shreyansh.jpg</url>
      <title>ppml</title>
      <link>https://shreyansh26.github.io/tags/ppml/</link>
    </image>
    
    <item>
      <title>PPML Series #2 - Federated Optimization Algorithms - FedSGD and FedAvg</title>
      <link>https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/</link>
      <pubDate>Sat, 18 Dec 2021 00:16:23 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/</guid>
      <description>&lt;p&gt;In my last post, I covered a high-level overview of Federated Learning, its applications, advantages &amp;amp; challenges.&lt;/p&gt;
&lt;p&gt;We also went through a high-level overview of how Federated Optimization algorithms work. But from a mathematical sense, how is Federated Learning training actually performed? That&amp;rsquo;s what we will be looking at in this post.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;There was a &lt;a href=&#34;https://arxiv.org/abs/1602.05629&#34;&gt;paper&lt;/a&gt;, Communication-Efficient Learning of Deep Networks from Decentralized Data by Google (3637 citations!!!), in which the authors had proposed a federated optimization algorithm called FedAvg and compared it with a naive baseline, FedSGD.&lt;/p&gt;
&lt;h2 id=&#34;fedsgd&#34;&gt;FedSGD&lt;/h2&gt;
&lt;p&gt;Stochastic Gradient Descent (SGD) had shown great results in deep learning. So, as a baseline, the researchers decided to base the Federated Learning training algorithm on SGD as well. SGD can be applied naively to the federated optimization problem, where a single batch gradient calculation (say on a randomly selected client) is done per round of communication.&lt;/p&gt;
&lt;p&gt;The paper showed that this approach is computationally efficient, but requires very large numbers of rounds of training to produce good models.&lt;/p&gt;
&lt;p&gt;Before we get into the maths, I&amp;rsquo;ll define a few terms -&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/Latex-FL.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/Latex-FL.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The baseline algorithm, was called FedSGD, short for Federated SGD.&lt;/p&gt;
&lt;p&gt;For FedSGD, the parameter &lt;em&gt;C&lt;/em&gt; (explained above) which controls the global batch size is set to 1. This corresponds to a full-batch (non-stochastic) gradient descent. For the current global model &lt;i&gt;w&lt;sup&gt;t&lt;/sup&gt;&lt;/i&gt;, the average gradient on its global model is calculated for each client &lt;em&gt;k&lt;/em&gt;.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedSGD-updates.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedSGD-updates.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The central server then aggregates these gradients and applies the update.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedSGD-updates2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedSGD-updates2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;h2 id=&#34;fedavg&#34;&gt;FedAvg&lt;/h2&gt;
&lt;p&gt;We saw FedSGD. Now let&amp;rsquo;s make a small change to the update step above.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedAvg-updates.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedAvg-updates.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;What this does is that now each client locally takes one step of gradient descent
on the current model using its local data, and the server then takes a weighted average of the resulting models.&lt;/p&gt;
&lt;p&gt;This way we can add more computation to each client by iterating the local update multiple times before doing the averaging step. This small modification results in the FederatedAveraging (FedAvg) algorithm.&lt;/p&gt;
&lt;p&gt;But why make this change? The answer is in my last post -&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;In practice, major speedups are obtained when computation on each client is improved, once a minimum level of parallelism over clients is achieved.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The amount of computation is controlled by three parameters -&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;C&lt;/strong&gt; - Fraction of clients participating in that round
&lt;strong&gt;E&lt;/strong&gt; - No. of training passes each client makes over its local dataset each round
&lt;strong&gt;B&lt;/strong&gt; - Local minibatch size used for client updates&lt;/p&gt;
&lt;p&gt;The pseudocode for the FedAvg algorithm is shown below.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedAvg-algo.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/FedAvg-algo.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;B = ꝏ (used in experiments) implies full local dataset is treated as the minibatch. So, setting B = ꝏ and E = 1 makes this the FedSGD algorithm.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Okay, now let&amp;rsquo;s look at some experimental results, although I would also suggest looking up the results from the original paper as well.
One experiment showed the number of rounds required to attain a target accuracy, in two tasks - MNIST and a character modelling task.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/res1.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/res1.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;Here, IID and non-IID here refer to the datasets that were artificially generated by the authors to represent two kinds of distributions - IID, in which there is in fact an IID distribution among the clients. And non-IID in which the data is not IID among the clients. For example, for the MNIST dataset the authors studied two ways of partitioning the MNIST data over clients: IID, where the data is shuffled, and then partitioned into 100 clients each receiving 600 examples, and Non-IID, where we first sort the data by digit label, divide it into 200 shards of size 300, and assign each of 100 clients 2 shards. For the language modeling task, the dataset was built from &lt;em&gt;The Complete Works of William Shakespeare&lt;/em&gt;. From the paper -&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;We construct a client dataset for each speaking role in each play with at least two lines. This produced a dataset with 1146 clients. For each client, we split the data into a set of training lines (the first 80% of lines for the role), and test lines (the last 20%, rounded up to at least one line). The resulting dataset has 3,564,579 characters in the training set, and 870,014 characters in the test set. This data is substantially unbalanced, with many roles having only a few lines, and a few with a large number of lines. Further, observe the test set is not a random sample of lines, but is temporally separated by the chronology of each play. Using an identical train/test split, we also form a balanced and IID version of the dataset, also with 1146 clients.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;For the MNIST dataset, a CNN with two 5x5 convolution layers (the first with 32 channels, the second with 64, each followed with 2x2 max pooling), a fully connected layer with 512 units and ReLu activation, and a final softmax output layer was used. And for the language modeling task, a stacked character-level LSTM language model, which after reading each character in a line, predicts the next character. The model takes a series of characters as input and embeds each of these into a learned 8 dimensional space. The embedded characters are then processed through 2 LSTM layers, each with 256 nodes. Finally the output of the second LSTM layer is sent to a softmax output layer with one node per character. The full model has 866,578 parameters, and we trained using an unroll length of 80 characters.&lt;/p&gt;
&lt;p&gt;From the results in the paper, it could be seen that in both the IID and non-IID settings, keeping a small mini-batch size and higher number of training passes on each client per round resulted in the model converging faster. For all model classes, FedAvg converges to a higher level of test accuracy than the baseline FedSGD models. For the CNN, the B = ꝏ; E = 1 FedSGD model reaches 99.22% accuracy in 1200 rounds, while the B = 10 ;E = 20 FedAvg model reaches an accuracy of 99.44% in 300 rounds.&lt;/p&gt;













&lt;figure&gt;


  &lt;a data-fancybox=&#34;&#34; href=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/res2.PNG&#34; &gt;
&lt;img src=&#34;https://shreyansh26.github.io/post/2021-12-18_federated_optimization_fedavg/images/res2.PNG&#34; alt=&#34;&#34; &gt;&lt;/a&gt;



&lt;/figure&gt;

&lt;p&gt;The authors also hypothesise that in addition to lowering communication costs, model averaging produces a regularization benefit similar to that achieved by dropout.&lt;/p&gt;
&lt;p&gt;All in all, the experiments demonstrated that the FedAvg algorithm was robust to unbalanced and non-IID distributions, and also reduced the number of rounds of communication required for training, by orders of magnitude.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I wrote a Twitter thread on this topic as well - do give it a like/follow me if you liked the article.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;My last thread covered a high-level overview of Federated Learning, its applications, advantages &amp;amp; challenges.&lt;br&gt;&lt;br&gt;But from a mathematical sense, how is Federated Learning training actually performed? That&amp;#39;s what we will be looking at in this thread 🧵&lt;a href=&#34;https://t.co/anyvEluWoq&#34;&gt;https://t.co/anyvEluWoq&lt;/a&gt;&lt;/p&gt;&amp;mdash; Shreyansh Singh (@shreyansh_26) &lt;a href=&#34;https://twitter.com/shreyansh_26/status/1463454860460785670?ref_src=twsrc%5Etfw&#34;&gt;November 24, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;That&amp;rsquo;s the end for now!&lt;/p&gt;
&lt;p&gt;This post finishes my summary on the basics of Federated Learning and is also a concise version of the very famous paper &amp;ldquo;Communication-Efficient Learning of Deep Networks from Decentralized Data&amp;rdquo; by Google.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;I have also released an annotated version of the paper. If you are interested, you can find it &lt;a href=&#34;https://github.com/shreyansh26/Annotated-ML-Papers/blob/main/PPML/Federated%20Learning/Communication-Efficient%20Learning%20of%20Deep%20Networks%20from%20Decentralized%20Data.pdf&#34;&gt;here&lt;/a&gt;.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;If the post helps you or you have any questions, do let me know!&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>PPML Series #1 - An introduction to Federated Learning</title>
      <link>https://shreyansh26.github.io/post/2021-12-11_intro_to_federated_learning/</link>
      <pubDate>Sat, 11 Dec 2021 16:17:16 +0530</pubDate>
      <guid>https://shreyansh26.github.io/post/2021-12-11_intro_to_federated_learning/</guid>
      <description>&lt;hr&gt;
&lt;h2 id=&#34;motivation&#34;&gt;Motivation&lt;/h2&gt;
&lt;p&gt;Privacy-preserving Machine Learning had always been exciting for me. Since my B.Tech. thesis involving PPML (SMPC + Computer Vision), I didn&amp;rsquo;t get a chance to work on it after that. So, after about 2 years, I have started to read about it again, and sharing it with the community.&lt;/p&gt;
&lt;p&gt;Federated Learning is a domain that I had somewhat eluded during my thesis. I had some idea about the topic but didn&amp;rsquo;t get into it much. So, I decided to start with FL this time. There is a ton of literature out there and is a field of active interest right now.&lt;/p&gt;
&lt;h2 id=&#34;introduction&#34;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Modern mobile devices have abundance of data, majorly textual data, image data. Applying machine learning to these can definitely help improve user experience. For example - your mobile keyboard uses language models can improve speech recognition and text entry, your photos apps (Google Photos, say) has image models that can automatically select good photos.&lt;/p&gt;
&lt;p&gt;However, we have two problems here. Firstly, if we consider millions of devices, then this data is large in quantity. Secondly, this dataset may have personal pictures, textual information written by the device owners,among other things.  Hence, this data is privacy sensitive in most cases. This creates problems to store this data in a database. It can be both infeasible as well as cause privacy violations.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Federated Learning&lt;/em&gt; is a decentralised machine learning approach which allows to leave the training data on the individual devices and learns a shared model by aggregating locally computed gradient updates. Federated Learning can significantly reduce privacy and security risks by limiting the attack surface to only the device, rather than the device and the cloud. Obviously, some level of trust on the server coordinating the training is still required.&lt;/p&gt;
&lt;h2 id=&#34;why-fl&#34;&gt;Why FL?&lt;/h2&gt;
&lt;p&gt;Federated Learning usually helps in three contexts -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Training on real-world data from mobile devices provides advantage over training on proxy data stored in data centres. The distributions from which these examples are drawn are likely to differ substantially from easily available proxy datasets: the use of language in chat and text messages is generally much different than standard language corpora, e.g., chat messages are not like Wikipedia articles. Images taken through the camera are also not like Flickr images.&lt;/li&gt;
&lt;li&gt;The  data is privacy sensitive or large in size (compared to the size of the model), so it is preferable not to log it to the data centre purely for the purpose of model training.&lt;/li&gt;
&lt;li&gt;Sometimes, labels for tasks can naturally be obtained from user interaction. Entered text is self-labeled for learning a language model, and photo labels can be defined by natural user interaction with their photo app (which photos are deleted, shared, or viewed).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Applications of FL could include image classification, predicting which images will be viewed multiple times in the future, language modelling, next word/phrase prediction.&lt;/p&gt;
&lt;h3 id=&#34;how-does-fl-provide-privacy-up-to-a-certain-extent&#34;&gt;How does FL provide privacy (up to a certain extent)?&lt;/h3&gt;
&lt;p&gt;Handling even anonymized data can lead to privacy concerns. What better way than to use the data itself to train the models but at the same time, not risk its privacy. In contrast, the information transmitted for federated learning is the minimal update necessary to improve a particular model. They will generally contain much less information about the raw data. Further, the source of the updates is not needed by the aggregation algorithm, so updates can be transmitted without identifying meta-data over a mix network such as Tor or trusted third-parties.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;h2 id=&#34;federated-optimization&#34;&gt;Federated Optimization&lt;/h2&gt;
&lt;p&gt;The optimization problem implicit in federated learning as federated optimization, drawing a connection (and contrast) to distributed optimization.&lt;/p&gt;
&lt;h3 id=&#34;how-is-fl-different-from-any-other-distributed-optimization-problem&#34;&gt;How is FL different from any other distributed optimization problem?&lt;/h3&gt;
&lt;p&gt;In federated optimization, there are a few key properties that differentiate from a typical distributed optimization problem.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Non-IID&lt;/strong&gt; - Training data will vary from user to user and will not have properties that are similar to the population.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Unbalanced&lt;/strong&gt; - Some users use a device more and generate more data, some less.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Massively distributed&lt;/strong&gt; - Number of users are much more than the average number of examples per client.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Limited communication&lt;/strong&gt; - Devices are frequently offline and are on slow or expensive connections.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;how-to-perform-federated-optimization&#34;&gt;How to perform Federated Optimization?&lt;/h3&gt;
&lt;p&gt;In this blog post, I won&amp;rsquo;t go into the mathematical details regarding the optimization techniques used in Federated Learning. However, we will be discussing the high-level overview of how training is performed. The steps are as follows -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is a fixed set of &lt;em&gt;K&lt;/em&gt; clients, each with a fixed local dataset.&lt;/li&gt;
&lt;li&gt;At the beginning of each round, a random fraction &lt;em&gt;C&lt;/em&gt; of clients is selected, and the server sends the current global algorithm state to each of these clients (e.g., the current model parameters).&lt;/li&gt;
&lt;li&gt;Only a fraction of clients is selected for efficiency, as experiments show diminishing returns for adding more clients beyond a certain point.&lt;/li&gt;
&lt;li&gt;Each selected client then performs local computation based on the global state and its local dataset, and sends an update to the server.&lt;/li&gt;
&lt;li&gt;The server then applies these updates to its global state, and the process repeats.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;challenges&#34;&gt;Challenges&lt;/h2&gt;
&lt;p&gt;In general, for ML tasks, and in data centres, the costs of compute is what is the most important - GPUs are used to lower the computation cost.
In Federated Learning, the communication costs somewhat dominate.&lt;/p&gt;
&lt;p&gt;But what do communication costs mean here?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Upload bandwidths in mobiles (globally) is limited to 1 MB/s or less.&lt;/li&gt;
&lt;li&gt;Clients volunteer only if the charged, plugged-in and on free/unmetered WiFi connections.&lt;/li&gt;
&lt;li&gt;Each client participates in only a small number of rounds per day.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In Federated Learning, computation is not much of an issue because the dataset size on each device will be relatively much less and modern smartphones now have processors fast enough to do those computations locally. So, the goal becomes to use additional computation in order to decrease the number of rounds of communication needed to train a model.&lt;/p&gt;
&lt;p&gt;So, the goal becomes to use additional computation in order to lower the number of rounds of communication needed to train the model.&lt;/p&gt;
&lt;p&gt;There are two approaches which can be adopted for this -&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Increased parallelism&lt;/em&gt; -  More clients are used which work independently between each communication round.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Increased computation on each client&lt;/em&gt; - Rather than performing a simple computation like gradient calculation, each client performs a more complex calculation between each communication round.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In practice, major speedups are obtained when computation on each client is improved, once a minimum level of parallelism over clients is achieved.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;I wrote a Twitter thread on this topic as well - do give it a like/follow me if you liked the article.&lt;/p&gt;
&lt;blockquote class=&#34;twitter-tweet&#34;&gt;&lt;p lang=&#34;en&#34; dir=&#34;ltr&#34;&gt;I recently started reading about Privacy-preserving ML, as this has been a topic that has always interested me.&lt;br&gt;I hope to share my learnings here on Twitter.&lt;br&gt;&lt;br&gt;I started with Federated Learning and here&amp;#39;s a detailed thread that will give you a high-level idea of FL🧵&lt;/p&gt;&amp;mdash; Shreyansh Singh (@shreyansh_26) &lt;a href=&#34;https://twitter.com/shreyansh_26/status/1462262151209381888?ref_src=twsrc%5Etfw&#34;&gt;November 21, 2021&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&#34;https://platform.twitter.com/widgets.js&#34; charset=&#34;utf-8&#34;&gt;&lt;/script&gt;

&lt;p&gt;This is all for now. Thanks for reading!&lt;/p&gt;
&lt;p&gt;In my next post, I&amp;rsquo;ll share a mathematical explanation as to how optimization (learning) is done in a Federated Learning setting. I will also explain some experimental results that have been published.&lt;/p&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script type=&#34;text/javascript&#34; src=&#34;//downloads.mailchimp.com/js/signup-forms/popup/unique-methods/embed.js&#34; data-dojo-config=&#34;usePlainJson: true, isDebug: false&#34;&gt;&lt;/script&gt;
&lt;!-- &lt;button style=&#34;background-color: #70ab17; color: #1770AB&#34; id=&#34;openpopup&#34;&gt;Subscribe to my posts!&lt;/button&gt; --&gt;
&lt;div class=&#34;button_cont&#34; align=&#34;center&#34;&gt;&lt;button id=&#34;openpopup&#34; class=&#34;example_a&#34;&gt;Subscribe to my posts!&lt;/button&gt;&lt;/div&gt;
&lt;style&gt;
    .example_a {
        color: #fff !important;
        text-transform: uppercase;
        text-decoration: none;
        background: #3f51b5;
        padding: 20px;
        border-radius: 5px;
        cursor: pointer;
        display: inline-block;
        border: none;
        transition: all 0.4s ease 0s;
    }

    .example_a:hover {
        background: #434343;
        letter-spacing: 1px;
        -webkit-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        -moz-box-shadow: 0px 5px 40px -10px rgba(0,0,0,0.57);
        box-shadow: 5px 40px -10px rgba(0,0,0,0.57);
        transition: all 0.4s ease 0s;
    }
&lt;/style&gt;
&lt;script type=&#34;text/javascript&#34;&gt;

function showMailingPopUp() {
    window.dojoRequire([&#34;mojo/signup-forms/Loader&#34;], function(L) { L.start({&#34;baseUrl&#34;:&#34;mc.us4.list-manage.com&#34;,&#34;uuid&#34;:&#34;0b10ac14f50d7f4e7d11cf26a&#34;,&#34;lid&#34;:&#34;667a1bb3da&#34;,&#34;uniqueMethods&#34;:true}) })

    document.cookie = &#34;MCPopupClosed=;path=/;expires=Thu, 01 Jan 1970 00:00:00 UTC&#34;;
}

document.getElementById(&#34;openpopup&#34;).onclick = function() {showMailingPopUp()};

&lt;/script&gt;
&lt;p&gt; &lt;/p&gt;
&lt;script data-name=&#34;BMC-Widget&#34; data-cfasync=&#34;false&#34; src=&#34;https://cdnjs.buymeacoffee.com/1.0.0/widget.prod.min.js&#34; data-id=&#34;shreyanshsingh&#34; data-description=&#34;Support me on Buy me a coffee!&#34; data-message=&#34;&#34; data-color=&#34;#FF5F5F&#34; data-position=&#34;Right&#34; data-x_margin=&#34;18&#34; data-y_margin=&#34;18&#34;&gt;&lt;/script&gt;
&lt;p&gt;Follow me on &lt;a href=&#34;https://twitter.com/shreyansh_26&#34;&gt;Twitter&lt;/a&gt;, &lt;a href=&#34;https://github.com/shreyansh26&#34;&gt;Github&lt;/a&gt; or connect on &lt;a href=&#34;https://www.linkedin.com/in/shreyansh26/&#34;&gt;LinkedIn&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
